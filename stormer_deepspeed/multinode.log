/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled

Lmod is automatically replacing "nvhpc/23.9" with "gcc-native/12.3".


Lmod is automatically replacing "PrgEnv-nvhpc/8.5.0" with "PrgEnv-gnu/8.5.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.28

Running on 5 nodes
Total number of GPUs: 20
Connected to tcp://x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov:7919
Found executable /soft/applications/conda/2024-04-29/mconda3/bin/python
Launching application a8be9958-f401-4155-80fb-a516d4926689
Using PMI port 39867,39868
[2025-06-19 10:53:57,944] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:53:57,944] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:53:57,944] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:53:57,944] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:53:58,048] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:53:58,048] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:53:58,048] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:53:58,048] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:53:58,170] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:53:58,170] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:53:58,170] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:53:58,170] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:53:58,343] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:53:58,343] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:53:58,343] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:53:58,343] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:53:58,545] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:53:58,546] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:53:58,546] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:53:58,546] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:54:56,053] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:54:56,053] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:54:56,196] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:54:56,196] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:54:57,191] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:54:57,191] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:55:07,565] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:55:07,565] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:58:10,699] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:58:10,699] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:58:10,723] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:58:10,723] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:58:10,789] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:58:10,789] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:58:10,814] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:58:10,815] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:58:10,848] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:58:10,848] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:58:10,888] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:58:10,888] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:58:11,073] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:58:11,073] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:58:11,174] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:58:11,174] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:58:11,183] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:58:11,183] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:58:11,200] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:58:11,200] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:58:11,289] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:58:11,289] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:58:11,423] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:58:11,423] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:58:12,060] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:58:12,060] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:58:12,184] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:58:12,184] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:58:12,257] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:58:12,257] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 10:58:12,329] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 10:58:12,329] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=8, local_rank=0, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=16, local_rank=0, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-19 10:58:12,333] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=9, local_rank=1, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=17, local_rank=1, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=10, local_rank=2, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=11, local_rank=3, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=12, local_rank=0, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=18, local_rank=2, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=0, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=19, local_rank=3, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=1, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=2, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=3, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=13, local_rank=1, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=14, local_rank=2, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-19 10:58:12,333] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=15, local_rank=3, world_size=20, master_addr=10.140.57.88, master_port=29500
read_signalfd: got signal 15 (Terminated)
Forwarding signal 15
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 4 died from signal 15
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 12 died from signal 15
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 8 died from signal 15
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 0 died from signal 15
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 16 died from signal 15
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 14 died from signal 15
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 2 died from signal 15
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 6 died from signal 15
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 7 died from signal 15
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 10 died from signal 15
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 9 died from signal 15
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 13 died from signal 15
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 1 died from signal 15
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 18 died from signal 15
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 17 died from signal 15
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 15 died from signal 15
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 5 died from signal 15
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 3 died from signal 15
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 11 died from signal 15
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 19 died from signal 15
Application a8be9958 resources: utime=1029s stime=990s maxrss=35184940KB inblock=235394182 oublock=8 minflt=32765054 majflt=26775 nvcsw=9241014 nivcsw=284856
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled

Lmod is automatically replacing "nvhpc/23.9" with "gcc-native/12.3".


Lmod is automatically replacing "PrgEnv-nvhpc/8.5.0" with "PrgEnv-gnu/8.5.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.28

Running on 7 nodes
Total number of GPUs: 28
Connected to tcp://x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov:7919
Found executable /soft/applications/conda/2024-04-29/mconda3/bin/python
Launching application a5794432-bd56-4152-9dd2-5606d912f122
Using PMI port 43086,43087
[2025-06-19 10:59:41,818] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:41,818] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:41,829] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:41,833] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,077] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,077] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,077] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,077] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,083] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,083] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,083] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,083] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,090] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,090] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,090] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,090] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,287] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,287] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,287] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,287] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,327] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,327] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,327] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,327] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,361] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,361] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,361] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 10:59:42,361] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:00:36,855] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:00:36,855] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:00:36,973] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:00:36,973] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:00:37,187] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:00:37,187] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:00:49,162] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:00:49,162] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:52,289] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:52,289] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:52,431] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:52,431] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:52,579] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:52,580] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:52,797] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:52,797] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:52,804] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:52,804] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:52,883] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:52,884] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:52,977] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:52,979] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:52,998] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:52,998] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:53,019] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:53,019] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:53,035] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:53,035] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:53,059] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:53,059] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:53,124] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:53,124] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:53,168] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:53,168] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:53,238] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:53,238] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:53,239] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:53,239] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:53,371] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:53,371] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:53,436] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:53,436] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:53,641] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:53,641] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:53,666] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:53,666] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:54,097] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:54,097] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:54,104] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:54,104] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:54,209] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:54,209] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:54,626] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:54,626] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:03:54,652] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:03:54,652] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=20, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=24, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=25, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=8, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=26, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=12, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=13, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=27, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=9, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=16, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=10, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=17, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=11, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=18, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=14, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=21, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=15, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=19, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=22, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:03:54,681] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=23, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
Initialized deepspeed on global rank 0, local rank 0 with world size 28.
[2025-06-19 11:03:56,761] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2+5f631abc, git-hash=5f631abc, git-branch=HEAD
[2025-06-19 11:04:04,932] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-06-19 11:04:04,934] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-06-19 11:04:04,934] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-06-19 11:04:04,941] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-06-19 11:04:04,941] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-06-19 11:04:04,941] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-06-19 11:04:04,941] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-06-19 11:04:04,941] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2025-06-19 11:04:04,941] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2025-06-19 11:04:04,941] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-06-19 11:04:04,941] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-06-19 11:04:04,941] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2025-06-19 11:04:04,941] [INFO] [config.py:1000:print]   amp_params ................... False
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1520b8093b10>
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   dump_state ................... False
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   global_rank .................. 0
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2025-06-19 11:04:04,942] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 5e-05, 'weight_decay': 0.01}
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   pld_params ................... False
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   steps_per_print .............. 100000
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   train_batch_size ............. 3584
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   world_size ................... 28
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2025-06-19 11:04:04,943] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2025-06-19 11:04:04,943] [INFO] [config.py:986:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 128, 
    "train_batch_size": 3.584000e+03, 
    "steps_per_print": 1.000000e+05, 
    "gradient_accumulation_steps": 1, 
    "fp16": {
        "enabled": false
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 5e-05, 
            "weight_decay": 0.01
        }
    }, 
    "comms_logger": {
        "enabled": true, 
        "verbose": false
    }, 
    "zero_optimization": {
        "stage": 0
    }
}
Validating lr=5e-05, train epoch 0.:   0%|          | 0/91 [00:00<?, ?it/s]Validating lr=5e-05, train epoch 0.:   1%|          | 1/91 [00:03<04:36,  3.08s/it]Validating lr=5e-05, train epoch 0.:   2%|▏         | 2/91 [00:05<04:00,  2.70s/it]Validating lr=5e-05, train epoch 0.:   3%|▎         | 3/91 [00:08<03:50,  2.62s/it]Validating lr=5e-05, train epoch 0.:   4%|▍         | 4/91 [00:10<03:42,  2.56s/it]Validating lr=5e-05, train epoch 0.:   5%|▌         | 5/91 [00:13<03:38,  2.54s/it]Validating lr=5e-05, train epoch 0.:   7%|▋         | 6/91 [00:15<03:34,  2.53s/it]Validating lr=5e-05, train epoch 0.:   8%|▊         | 7/91 [00:18<03:31,  2.52s/it]Validating lr=5e-05, train epoch 0.:   9%|▉         | 8/91 [00:20<03:27,  2.51s/it]Validating lr=5e-05, train epoch 0.:  10%|▉         | 9/91 [00:22<03:24,  2.49s/it]Validating lr=5e-05, train epoch 0.:  11%|█         | 10/91 [00:25<03:21,  2.49s/it]Validating lr=5e-05, train epoch 0.:  12%|█▏        | 11/91 [00:27<03:18,  2.49s/it]Validating lr=5e-05, train epoch 0.:  13%|█▎        | 12/91 [00:30<03:16,  2.49s/it]Validating lr=5e-05, train epoch 0.:  14%|█▍        | 13/91 [00:32<03:14,  2.50s/it]Validating lr=5e-05, train epoch 0.:  15%|█▌        | 14/91 [00:35<03:12,  2.50s/it]Validating lr=5e-05, train epoch 0.:  16%|█▋        | 15/91 [00:37<03:10,  2.50s/it]Validating lr=5e-05, train epoch 0.:  18%|█▊        | 16/91 [00:40<03:06,  2.49s/it]Validating lr=5e-05, train epoch 0.:  19%|█▊        | 17/91 [00:42<03:03,  2.48s/it]Validating lr=5e-05, train epoch 0.:  20%|█▉        | 18/91 [00:45<03:01,  2.48s/it]Validating lr=5e-05, train epoch 0.:  21%|██        | 19/91 [00:47<02:58,  2.48s/it]Validating lr=5e-05, train epoch 0.:  22%|██▏       | 20/91 [00:50<02:56,  2.49s/it]Validating lr=5e-05, train epoch 0.:  23%|██▎       | 21/91 [00:52<02:53,  2.48s/it]Validating lr=5e-05, train epoch 0.:  24%|██▍       | 22/91 [00:55<02:51,  2.49s/it]Validating lr=5e-05, train epoch 0.:  25%|██▌       | 23/91 [00:57<02:49,  2.49s/it]Validating lr=5e-05, train epoch 0.:  26%|██▋       | 24/91 [01:00<02:46,  2.49s/it]Validating lr=5e-05, train epoch 0.:  27%|██▋       | 25/91 [01:02<02:43,  2.48s/it]Validating lr=5e-05, train epoch 0.:  29%|██▊       | 26/91 [01:05<02:41,  2.48s/it]Validating lr=5e-05, train epoch 0.:  30%|██▉       | 27/91 [01:07<02:38,  2.48s/it]Validating lr=5e-05, train epoch 0.:  31%|███       | 28/91 [01:10<02:36,  2.48s/it]Validating lr=5e-05, train epoch 0.:  32%|███▏      | 29/91 [01:12<02:33,  2.48s/it]Validating lr=5e-05, train epoch 0.:  33%|███▎      | 30/91 [01:15<02:32,  2.49s/it]Validating lr=5e-05, train epoch 0.:  34%|███▍      | 31/91 [01:17<02:29,  2.49s/it]Validating lr=5e-05, train epoch 0.:  35%|███▌      | 32/91 [01:20<02:26,  2.48s/it]Validating lr=5e-05, train epoch 0.:  36%|███▋      | 33/91 [01:22<02:24,  2.49s/it]Validating lr=5e-05, train epoch 0.:  37%|███▋      | 34/91 [01:25<02:22,  2.50s/it]Validating lr=5e-05, train epoch 0.:  38%|███▊      | 35/91 [01:27<02:19,  2.49s/it]Validating lr=5e-05, train epoch 0.:  40%|███▉      | 36/91 [01:30<02:17,  2.49s/it]Validating lr=5e-05, train epoch 0.:  41%|████      | 37/91 [01:32<02:15,  2.50s/it]Validating lr=5e-05, train epoch 0.:  42%|████▏     | 38/91 [01:35<02:12,  2.50s/it]Validating lr=5e-05, train epoch 0.:  43%|████▎     | 39/91 [01:37<02:09,  2.49s/it]Validating lr=5e-05, train epoch 0.:  44%|████▍     | 40/91 [01:40<02:07,  2.49s/it]Validating lr=5e-05, train epoch 0.:  45%|████▌     | 41/91 [01:42<02:04,  2.48s/it]Validating lr=5e-05, train epoch 0.:  46%|████▌     | 42/91 [01:45<02:00,  2.47s/it]Validating lr=5e-05, train epoch 0.:  47%|████▋     | 43/91 [01:47<01:58,  2.46s/it]Validating lr=5e-05, train epoch 0.:  48%|████▊     | 44/91 [01:49<01:55,  2.46s/it]Validating lr=5e-05, train epoch 0.:  49%|████▉     | 45/91 [01:52<01:53,  2.47s/it]Validating lr=5e-05, train epoch 0.:  51%|█████     | 46/91 [01:54<01:51,  2.47s/it]Validating lr=5e-05, train epoch 0.:  52%|█████▏    | 47/91 [01:57<01:48,  2.47s/it]Validating lr=5e-05, train epoch 0.:  53%|█████▎    | 48/91 [01:59<01:46,  2.47s/it]Validating lr=5e-05, train epoch 0.:  54%|█████▍    | 49/91 [02:02<01:43,  2.47s/it]Validating lr=5e-05, train epoch 0.:  55%|█████▍    | 50/91 [02:04<01:41,  2.48s/it]Validating lr=5e-05, train epoch 0.:  56%|█████▌    | 51/91 [02:07<01:39,  2.48s/it]Validating lr=5e-05, train epoch 0.:  57%|█████▋    | 52/91 [02:09<01:36,  2.47s/it]Validating lr=5e-05, train epoch 0.:  58%|█████▊    | 53/91 [02:12<01:34,  2.47s/it]Validating lr=5e-05, train epoch 0.:  59%|█████▉    | 54/91 [02:14<01:31,  2.46s/it]Validating lr=5e-05, train epoch 0.:  60%|██████    | 55/91 [02:17<01:28,  2.47s/it]Validating lr=5e-05, train epoch 0.:  62%|██████▏   | 56/91 [02:19<01:26,  2.48s/it]Validating lr=5e-05, train epoch 0.:  63%|██████▎   | 57/91 [02:22<01:23,  2.47s/it]Validating lr=5e-05, train epoch 0.:  64%|██████▎   | 58/91 [02:24<01:21,  2.46s/it]Validating lr=5e-05, train epoch 0.:  65%|██████▍   | 59/91 [02:27<01:19,  2.47s/it]Validating lr=5e-05, train epoch 0.:  66%|██████▌   | 60/91 [02:29<01:16,  2.48s/it]Validating lr=5e-05, train epoch 0.:  67%|██████▋   | 61/91 [02:31<01:14,  2.47s/it]Validating lr=5e-05, train epoch 0.:  68%|██████▊   | 62/91 [02:34<01:11,  2.47s/it]Validating lr=5e-05, train epoch 0.:  69%|██████▉   | 63/91 [02:36<01:08,  2.46s/it]Validating lr=5e-05, train epoch 0.:  70%|███████   | 64/91 [02:39<01:06,  2.47s/it]Validating lr=5e-05, train epoch 0.:  71%|███████▏  | 65/91 [02:41<01:03,  2.46s/it]Validating lr=5e-05, train epoch 0.:  73%|███████▎  | 66/91 [02:44<01:01,  2.46s/it]Validating lr=5e-05, train epoch 0.:  74%|███████▎  | 67/91 [02:46<00:59,  2.46s/it]Validating lr=5e-05, train epoch 0.:  75%|███████▍  | 68/91 [02:49<00:56,  2.47s/it]Validating lr=5e-05, train epoch 0.:  76%|███████▌  | 69/91 [02:51<00:54,  2.46s/it]Validating lr=5e-05, train epoch 0.:  77%|███████▋  | 70/91 [02:54<00:51,  2.47s/it]Validating lr=5e-05, train epoch 0.:  78%|███████▊  | 71/91 [02:56<00:49,  2.47s/it]Validating lr=5e-05, train epoch 0.:  79%|███████▉  | 72/91 [02:59<00:46,  2.46s/it]Validating lr=5e-05, train epoch 0.:  80%|████████  | 73/91 [03:01<00:44,  2.46s/it]Validating lr=5e-05, train epoch 0.:  81%|████████▏ | 74/91 [03:03<00:41,  2.46s/it]Validating lr=5e-05, train epoch 0.:  82%|████████▏ | 75/91 [03:06<00:39,  2.46s/it]Validating lr=5e-05, train epoch 0.:  84%|████████▎ | 76/91 [03:08<00:36,  2.46s/it]Validating lr=5e-05, train epoch 0.:  85%|████████▍ | 77/91 [03:11<00:34,  2.45s/it]Validating lr=5e-05, train epoch 0.:  86%|████████▌ | 78/91 [03:13<00:32,  2.46s/it]Validating lr=5e-05, train epoch 0.:  87%|████████▋ | 79/91 [03:16<00:29,  2.46s/it]Validating lr=5e-05, train epoch 0.:  88%|████████▊ | 80/91 [03:18<00:27,  2.46s/it]Validating lr=5e-05, train epoch 0.:  89%|████████▉ | 81/91 [03:21<00:24,  2.46s/it]Validating lr=5e-05, train epoch 0.:  90%|█████████ | 82/91 [03:23<00:22,  2.46s/it]Validating lr=5e-05, train epoch 0.:  91%|█████████ | 83/91 [03:26<00:19,  2.47s/it]Validating lr=5e-05, train epoch 0.:  92%|█████████▏| 84/91 [03:28<00:17,  2.47s/it]Validating lr=5e-05, train epoch 0.:  93%|█████████▎| 85/91 [03:31<00:14,  2.46s/it]Validating lr=5e-05, train epoch 0.:  95%|█████████▍| 86/91 [03:33<00:12,  2.46s/it]Validating lr=5e-05, train epoch 0.:  96%|█████████▌| 87/91 [03:35<00:09,  2.47s/it]Validating lr=5e-05, train epoch 0.:  97%|█████████▋| 88/91 [03:38<00:07,  2.47s/it]Validating lr=5e-05, train epoch 0.:  98%|█████████▊| 89/91 [03:40<00:04,  2.47s/it]Validating lr=5e-05, train epoch 0.:  99%|█████████▉| 90/91 [03:43<00:02,  2.47s/it]Validating lr=5e-05, train epoch 0.: 100%|██████████| 91/91 [03:45<00:00,  2.46s/it]Validating lr=5e-05, train epoch 0.: 100%|██████████| 91/91 [03:45<00:00,  2.48s/it]
Validating lr=5e-05, train epoch 1.:   0%|          | 0/91 [00:00<?, ?it/s]Validating lr=5e-05, train epoch 1.:   1%|          | 1/91 [00:02<03:41,  2.47s/it]Validating lr=5e-05, train epoch 1.:   2%|▏         | 2/91 [00:04<03:39,  2.47s/it]Validating lr=5e-05, train epoch 1.:   3%|▎         | 3/91 [00:07<03:37,  2.48s/it]Validating lr=5e-05, train epoch 1.:   4%|▍         | 4/91 [00:09<03:34,  2.46s/it]Validating lr=5e-05, train epoch 1.:   5%|▌         | 5/91 [00:12<03:31,  2.46s/it]Validating lr=5e-05, train epoch 1.:   7%|▋         | 6/91 [00:14<03:29,  2.46s/it]Validating lr=5e-05, train epoch 1.:   8%|▊         | 7/91 [00:17<03:27,  2.46s/it]Validating lr=5e-05, train epoch 1.:   9%|▉         | 8/91 [00:19<03:25,  2.48s/it]Validating lr=5e-05, train epoch 1.:  10%|▉         | 9/91 [00:22<03:23,  2.48s/it]Validating lr=5e-05, train epoch 1.:  11%|█         | 10/91 [00:24<03:21,  2.49s/it]Validating lr=5e-05, train epoch 1.:  12%|█▏        | 11/91 [00:27<03:18,  2.48s/it]Validating lr=5e-05, train epoch 1.:  13%|█▎        | 12/91 [00:29<03:15,  2.48s/it]Validating lr=5e-05, train epoch 1.:  14%|█▍        | 13/91 [00:32<03:12,  2.46s/it]Validating lr=5e-05, train epoch 1.:  15%|█▌        | 14/91 [00:34<03:10,  2.47s/it]Validating lr=5e-05, train epoch 1.:  16%|█▋        | 15/91 [00:37<03:07,  2.46s/it]Validating lr=5e-05, train epoch 1.:  18%|█▊        | 16/91 [00:39<03:04,  2.46s/it]Validating lr=5e-05, train epoch 1.:  19%|█▊        | 17/91 [00:41<03:01,  2.46s/it]Validating lr=5e-05, train epoch 1.:  20%|█▉        | 18/91 [00:44<02:59,  2.45s/it]Validating lr=5e-05, train epoch 1.:  21%|██        | 19/91 [00:46<02:55,  2.44s/it]Validating lr=5e-05, train epoch 1.:  22%|██▏       | 20/91 [00:49<02:54,  2.45s/it]Validating lr=5e-05, train epoch 1.:  23%|██▎       | 21/91 [00:51<02:51,  2.45s/it]Validating lr=5e-05, train epoch 1.:  24%|██▍       | 22/91 [00:54<02:49,  2.45s/it]Validating lr=5e-05, train epoch 1.:  25%|██▌       | 23/91 [00:56<02:47,  2.46s/it]Validating lr=5e-05, train epoch 1.:  26%|██▋       | 24/91 [00:59<02:44,  2.46s/it]Validating lr=5e-05, train epoch 1.:  27%|██▋       | 25/91 [01:01<02:42,  2.46s/it]Validating lr=5e-05, train epoch 1.:  29%|██▊       | 26/91 [01:04<02:39,  2.45s/it]Validating lr=5e-05, train epoch 1.:  30%|██▉       | 27/91 [01:06<02:37,  2.46s/it]Validating lr=5e-05, train epoch 1.:  31%|███       | 28/91 [01:08<02:34,  2.45s/it]Validating lr=5e-05, train epoch 1.:  32%|███▏      | 29/91 [01:11<02:31,  2.45s/it]Validating lr=5e-05, train epoch 1.:  33%|███▎      | 30/91 [01:13<02:29,  2.45s/it]Validating lr=5e-05, train epoch 1.:  34%|███▍      | 31/91 [01:16<02:27,  2.46s/it]Validating lr=5e-05, train epoch 1.:  35%|███▌      | 32/91 [01:18<02:25,  2.47s/it]Validating lr=5e-05, train epoch 1.:  36%|███▋      | 33/91 [01:21<02:23,  2.47s/it]Validating lr=5e-05, train epoch 1.:  37%|███▋      | 34/91 [01:23<02:20,  2.46s/it]Validating lr=5e-05, train epoch 1.:  38%|███▊      | 35/91 [01:26<02:17,  2.46s/it]Validating lr=5e-05, train epoch 1.:  40%|███▉      | 36/91 [01:28<02:14,  2.45s/it]Validating lr=5e-05, train epoch 1.:  41%|████      | 37/91 [01:31<02:11,  2.44s/it]Validating lr=5e-05, train epoch 1.:  42%|████▏     | 38/91 [01:33<02:09,  2.45s/it]Validating lr=5e-05, train epoch 1.:  43%|████▎     | 39/91 [01:35<02:07,  2.45s/it]Validating lr=5e-05, train epoch 1.:  44%|████▍     | 40/91 [01:38<02:05,  2.45s/it]Validating lr=5e-05, train epoch 1.:  45%|████▌     | 41/91 [01:40<02:03,  2.47s/it]Validating lr=5e-05, train epoch 1.:  46%|████▌     | 42/91 [01:43<02:00,  2.46s/it]Validating lr=5e-05, train epoch 1.:  47%|████▋     | 43/91 [01:45<01:57,  2.45s/it]Validating lr=5e-05, train epoch 1.:  48%|████▊     | 44/91 [01:48<01:55,  2.46s/it]Validating lr=5e-05, train epoch 1.:  49%|████▉     | 45/91 [01:50<01:53,  2.46s/it]Validating lr=5e-05, train epoch 1.:  51%|█████     | 46/91 [01:53<01:50,  2.45s/it]Validating lr=5e-05, train epoch 1.:  52%|█████▏    | 47/91 [01:55<01:47,  2.45s/it]Validating lr=5e-05, train epoch 1.:  53%|█████▎    | 48/91 [01:58<01:45,  2.46s/it]Validating lr=5e-05, train epoch 1.:  54%|█████▍    | 49/91 [02:00<01:43,  2.46s/it]Validating lr=5e-05, train epoch 1.:  55%|█████▍    | 50/91 [02:03<01:41,  2.47s/it]Validating lr=5e-05, train epoch 1.:  56%|█████▌    | 51/91 [02:05<01:38,  2.46s/it]Validating lr=5e-05, train epoch 1.:  57%|█████▋    | 52/91 [02:07<01:35,  2.46s/it]Validating lr=5e-05, train epoch 1.:  58%|█████▊    | 53/91 [02:10<01:33,  2.45s/it]Validating lr=5e-05, train epoch 1.:  59%|█████▉    | 54/91 [02:12<01:31,  2.46s/it]Validating lr=5e-05, train epoch 1.:  60%|██████    | 55/91 [02:15<01:28,  2.46s/it]Validating lr=5e-05, train epoch 1.:  62%|██████▏   | 56/91 [02:17<01:25,  2.46s/it]Validating lr=5e-05, train epoch 1.:  63%|██████▎   | 57/91 [02:20<01:23,  2.46s/it]Validating lr=5e-05, train epoch 1.:  64%|██████▎   | 58/91 [02:22<01:21,  2.46s/it]Validating lr=5e-05, train epoch 1.:  65%|██████▍   | 59/91 [02:25<01:18,  2.46s/it]Validating lr=5e-05, train epoch 1.:  66%|██████▌   | 60/91 [02:27<01:16,  2.47s/it]Validating lr=5e-05, train epoch 1.:  67%|██████▋   | 61/91 [02:30<01:14,  2.47s/it]Validating lr=5e-05, train epoch 1.:  68%|██████▊   | 62/91 [02:32<01:11,  2.47s/it]Validating lr=5e-05, train epoch 1.:  69%|██████▉   | 63/91 [02:35<01:09,  2.47s/it]Validating lr=5e-05, train epoch 1.:  70%|███████   | 64/91 [02:37<01:06,  2.46s/it]Validating lr=5e-05, train epoch 1.:  71%|███████▏  | 65/91 [02:39<01:03,  2.46s/it]Validating lr=5e-05, train epoch 1.:  73%|███████▎  | 66/91 [02:42<01:01,  2.46s/it]Validating lr=5e-05, train epoch 1.:  74%|███████▎  | 67/91 [02:44<00:58,  2.44s/it]Validating lr=5e-05, train epoch 1.:  75%|███████▍  | 68/91 [02:47<00:56,  2.45s/it]Validating lr=5e-05, train epoch 1.:  76%|███████▌  | 69/91 [02:49<00:54,  2.46s/it]Validating lr=5e-05, train epoch 1.:  77%|███████▋  | 70/91 [02:52<00:51,  2.45s/it]Validating lr=5e-05, train epoch 1.:  78%|███████▊  | 71/91 [02:54<00:49,  2.46s/it]Validating lr=5e-05, train epoch 1.:  79%|███████▉  | 72/91 [02:57<00:46,  2.46s/it]Validating lr=5e-05, train epoch 1.:  80%|████████  | 73/91 [02:59<00:44,  2.45s/it]Validating lr=5e-05, train epoch 1.:  81%|████████▏ | 74/91 [03:02<00:41,  2.45s/it]Validating lr=5e-05, train epoch 1.:  82%|████████▏ | 75/91 [03:04<00:39,  2.45s/it]Validating lr=5e-05, train epoch 1.:  84%|████████▎ | 76/91 [03:06<00:36,  2.44s/it]Validating lr=5e-05, train epoch 1.:  85%|████████▍ | 77/91 [03:09<00:34,  2.45s/it]Validating lr=5e-05, train epoch 1.:  86%|████████▌ | 78/91 [03:11<00:31,  2.46s/it]Validating lr=5e-05, train epoch 1.:  87%|████████▋ | 79/91 [03:14<00:29,  2.46s/it]Validating lr=5e-05, train epoch 1.:  88%|████████▊ | 80/91 [03:16<00:26,  2.45s/it]Validating lr=5e-05, train epoch 1.:  89%|████████▉ | 81/91 [03:19<00:24,  2.45s/it]Validating lr=5e-05, train epoch 1.:  90%|█████████ | 82/91 [03:21<00:22,  2.45s/it]Validating lr=5e-05, train epoch 1.:  91%|█████████ | 83/91 [03:24<00:19,  2.45s/it]Validating lr=5e-05, train epoch 1.:  92%|█████████▏| 84/91 [03:26<00:17,  2.46s/it]Validating lr=5e-05, train epoch 1.:  93%|█████████▎| 85/91 [03:29<00:14,  2.46s/it]Validating lr=5e-05, train epoch 1.:  95%|█████████▍| 86/91 [03:31<00:12,  2.46s/it]Validating lr=5e-05, train epoch 1.:  96%|█████████▌| 87/91 [03:33<00:09,  2.46s/it]Validating lr=5e-05, train epoch 1.:  97%|█████████▋| 88/91 [03:36<00:07,  2.46s/it]Validating lr=5e-05, train epoch 1.:  98%|█████████▊| 89/91 [03:38<00:04,  2.45s/it]Validating lr=5e-05, train epoch 1.:  99%|█████████▉| 90/91 [03:41<00:02,  2.44s/it]Validating lr=5e-05, train epoch 1.: 100%|██████████| 91/91 [03:43<00:00,  2.44s/it]Validating lr=5e-05, train epoch 1.: 100%|██████████| 91/91 [03:43<00:00,  2.46s/it]
Evaluating for lr=5e-05:   0%|          | 0/9 [00:00<?, ?it/s]Initialized deepspeed on global rank 4, local rank 0 with world size 28.
[rank4]: Traceback (most recent call last):
[rank4]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank4]:     loss_dict = evaluate(
[rank4]:                 ^^^^^^^^^
[rank4]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank4]:     loss_dict[loss_name] += dist.item() / world_size
[rank4]:                             ^^^^^^^^^
[rank4]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 7, local rank 3 with world size 28.
[rank7]: Traceback (most recent call last):
[rank7]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank7]:     loss_dict = evaluate(
[rank7]:                 ^^^^^^^^^
[rank7]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank7]:     loss_dict[loss_name] += dist.item() / world_size
[rank7]:                             ^^^^^^^^^
[rank7]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 1, local rank 1 with world size 28.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank1]:     loss_dict = evaluate(
[rank1]:                 ^^^^^^^^^
[rank1]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank1]:     loss_dict[loss_name] += dist.item() / world_size
[rank1]:                             ^^^^^^^^^
[rank1]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 21, local rank 1 with world size 28.
Initialized deepspeed on global rank 22, local rank 2 with world size 28.
[rank21]: Traceback (most recent call last):
[rank21]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank21]:     loss_dict = evaluate(
[rank21]:                 ^^^^^^^^^
[rank21]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank21]:     loss_dict[loss_name] += dist.item() / world_size
[rank21]:                             ^^^^^^^^^
[rank21]: AttributeError: module 'torch.distributed' has no attribute 'item'
[rank22]: Traceback (most recent call last):
[rank22]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank22]:     loss_dict = evaluate(
[rank22]:                 ^^^^^^^^^
[rank22]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank22]:     loss_dict[loss_name] += dist.item() / world_size
[rank22]:                             ^^^^^^^^^
[rank22]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 5, local rank 1 with world size 28.
[rank5]: Traceback (most recent call last):
[rank5]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank5]:     loss_dict = evaluate(
[rank5]:                 ^^^^^^^^^
[rank5]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank5]:     loss_dict[loss_name] += dist.item() / world_size
[rank5]:                             ^^^^^^^^^
[rank5]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 6, local rank 2 with world size 28.
[rank6]: Traceback (most recent call last):
[rank6]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank6]:     loss_dict = evaluate(
[rank6]:                 ^^^^^^^^^
[rank6]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank6]:     loss_dict[loss_name] += dist.item() / world_size
[rank6]:                             ^^^^^^^^^
[rank6]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 20, local rank 0 with world size 28.
[rank20]: Traceback (most recent call last):
[rank20]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank20]:     loss_dict = evaluate(
[rank20]:                 ^^^^^^^^^
[rank20]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank20]:     loss_dict[loss_name] += dist.item() / world_size
[rank20]:                             ^^^^^^^^^
[rank20]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 23, local rank 3 with world size 28.
[rank23]: Traceback (most recent call last):
[rank23]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank23]:     loss_dict = evaluate(
[rank23]:                 ^^^^^^^^^
[rank23]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank23]:     loss_dict[loss_name] += dist.item() / world_size
[rank23]:                             ^^^^^^^^^
[rank23]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 16, local rank 0 with world size 28.
[rank16]: Traceback (most recent call last):
[rank16]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank16]:     loss_dict = evaluate(
[rank16]:                 ^^^^^^^^^
[rank16]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank16]:     loss_dict[loss_name] += dist.item() / world_size
[rank16]:                             ^^^^^^^^^
[rank16]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 12, local rank 0 with world size 28.
Initialized deepspeed on global rank 25, local rank 1 with world size 28.
[rank12]: Traceback (most recent call last):
[rank12]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank12]:     loss_dict = evaluate(
[rank12]:                 ^^^^^^^^^
[rank12]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank12]:     loss_dict[loss_name] += dist.item() / world_size
[rank12]:                             ^^^^^^^^^
[rank12]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 18, local rank 2 with world size 28.
[rank25]: Traceback (most recent call last):
[rank25]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank25]:     loss_dict = evaluate(
[rank25]:                 ^^^^^^^^^
[rank25]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank25]:     loss_dict[loss_name] += dist.item() / world_size
[rank25]:                             ^^^^^^^^^
[rank25]: AttributeError: module 'torch.distributed' has no attribute 'item'
[rank18]: Traceback (most recent call last):
[rank18]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank18]:     loss_dict = evaluate(
[rank18]:                 ^^^^^^^^^
[rank18]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank18]:     loss_dict[loss_name] += dist.item() / world_size
[rank18]:                             ^^^^^^^^^
[rank18]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 27, local rank 3 with world size 28.
[rank27]: Traceback (most recent call last):
[rank27]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank27]:     loss_dict = evaluate(
[rank27]:                 ^^^^^^^^^
[rank27]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank27]:     loss_dict[loss_name] += dist.item() / world_size
[rank27]:                             ^^^^^^^^^
[rank27]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 19, local rank 3 with world size 28.
[rank19]: Traceback (most recent call last):
[rank19]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank19]:     loss_dict = evaluate(
[rank19]:                 ^^^^^^^^^
[rank19]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank19]:     loss_dict[loss_name] += dist.item() / world_size
[rank19]:                             ^^^^^^^^^
[rank19]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 15, local rank 3 with world size 28.
Initialized deepspeed on global rank 24, local rank 0 with world size 28.
Evaluating for lr=5e-05:   0%|          | 0/9 [00:01<?, ?it/s]
[rank24]: Traceback (most recent call last):
[rank24]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank24]:     loss_dict = evaluate(
[rank24]:                 ^^^^^^^^^
[rank24]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank24]:     loss_dict[loss_name] += dist.item() / world_size
[rank24]:                             ^^^^^^^^^
[rank24]: AttributeError: module 'torch.distributed' has no attribute 'item'
[rank15]: Traceback (most recent call last):
[rank15]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank15]:     loss_dict = evaluate(
[rank15]:                 ^^^^^^^^^
[rank15]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank15]:     loss_dict[loss_name] += dist.item() / world_size
[rank15]:                             ^^^^^^^^^
[rank15]: AttributeError: module 'torch.distributed' has no attribute 'item'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank0]:     loss_dict = evaluate(
[rank0]:                 ^^^^^^^^^
[rank0]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank0]:     loss_dict[loss_name] += dist.item() / world_size
[rank0]:                             ^^^^^^^^^
[rank0]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 26, local rank 2 with world size 28.
Initialized deepspeed on global rank 17, local rank 1 with world size 28.
[rank26]: Traceback (most recent call last):
[rank26]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank26]:     loss_dict = evaluate(
[rank26]:                 ^^^^^^^^^
[rank26]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank26]:     loss_dict[loss_name] += dist.item() / world_size
[rank26]:                             ^^^^^^^^^
[rank26]: AttributeError: module 'torch.distributed' has no attribute 'item'
[rank17]: Traceback (most recent call last):
[rank17]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank17]:     loss_dict = evaluate(
[rank17]:                 ^^^^^^^^^
[rank17]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank17]:     loss_dict[loss_name] += dist.item() / world_size
[rank17]:                             ^^^^^^^^^
[rank17]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 8, local rank 0 with world size 28.
[rank8]: Traceback (most recent call last):
[rank8]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank8]:     loss_dict = evaluate(
[rank8]:                 ^^^^^^^^^
[rank8]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank8]:     loss_dict[loss_name] += dist.item() / world_size
[rank8]:                             ^^^^^^^^^
[rank8]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 3, local rank 3 with world size 28.
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank3]:     loss_dict = evaluate(
[rank3]:                 ^^^^^^^^^
[rank3]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank3]:     loss_dict[loss_name] += dist.item() / world_size
[rank3]:                             ^^^^^^^^^
[rank3]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 2, local rank 2 with world size 28.
Initialized deepspeed on global rank 11, local rank 3 with world size 28.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank2]:     loss_dict = evaluate(
[rank2]:                 ^^^^^^^^^
[rank2]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank2]:     loss_dict[loss_name] += dist.item() / world_size
[rank2]:                             ^^^^^^^^^
[rank2]: AttributeError: module 'torch.distributed' has no attribute 'item'
[rank11]: Traceback (most recent call last):
[rank11]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank11]:     loss_dict = evaluate(
[rank11]:                 ^^^^^^^^^
[rank11]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank11]:     loss_dict[loss_name] += dist.item() / world_size
[rank11]:                             ^^^^^^^^^
[rank11]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 9, local rank 1 with world size 28.
[rank9]: Traceback (most recent call last):
[rank9]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank9]:     loss_dict = evaluate(
[rank9]:                 ^^^^^^^^^
[rank9]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank9]:     loss_dict[loss_name] += dist.item() / world_size
[rank9]:                             ^^^^^^^^^
[rank9]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 10, local rank 2 with world size 28.
[rank10]: Traceback (most recent call last):
[rank10]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank10]:     loss_dict = evaluate(
[rank10]:                 ^^^^^^^^^
[rank10]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank10]:     loss_dict[loss_name] += dist.item() / world_size
[rank10]:                             ^^^^^^^^^
[rank10]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 14, local rank 2 with world size 28.
[rank14]: Traceback (most recent call last):
[rank14]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank14]:     loss_dict = evaluate(
[rank14]:                 ^^^^^^^^^
[rank14]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank14]:     loss_dict[loss_name] += dist.item() / world_size
[rank14]:                             ^^^^^^^^^
[rank14]: AttributeError: module 'torch.distributed' has no attribute 'item'
Initialized deepspeed on global rank 13, local rank 1 with world size 28.
[rank13]: Traceback (most recent call last):
[rank13]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank13]:     loss_dict = evaluate(
[rank13]:                 ^^^^^^^^^
[rank13]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 145, in evaluate
[rank13]:     loss_dict[loss_name] += dist.item() / world_size
[rank13]:                             ^^^^^^^^^
[rank13]: AttributeError: module 'torch.distributed' has no attribute 'item'
x3004c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 4 exited with code 1
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 12 exited with code 1
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 20 exited with code 1
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 16 exited with code 1
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 24 exited with code 1
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 8 exited with code 1
x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 0 died from signal 15
x3004c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 6 exited with code 1
x3004c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 5 exited with code 1
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 14 exited with code 1
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 13 exited with code 1
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 21 exited with code 1
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 23 exited with code 1
x3004c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 7 exited with code 1
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 15 exited with code 1
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 27 exited with code 1
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 25 exited with code 1
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 17 exited with code 1
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 19 exited with code 1
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 22 exited with code 1
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 11 exited with code 1
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 9 exited with code 1
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 26 exited with code 1
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 18 exited with code 1
x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 1 died from signal 15
x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 2 died from signal 15
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 10 exited with code 1
x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 3 died from signal 15
Application a5794432 resources: utime=10440s stime=5008s maxrss=35185708KB inblock=352153108 oublock=536 minflt=447142693 majflt=43145 nvcsw=42179495 nivcsw=9839146
Training completed
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled

Lmod is automatically replacing "nvhpc/23.9" with "gcc-native/12.3".


Lmod is automatically replacing "PrgEnv-nvhpc/8.5.0" with "PrgEnv-gnu/8.5.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.28

declare -x APP2_STATE="23.12.0"
declare -x BASH_ENV="/usr/share/lmod/lmod/init/bash"
declare -x C3_RSH="ssh -oConnectTimeout=10 -oForwardX11=no"
declare -x CFLAGS="-I/soft/applications/conda/2024-04-29/mconda3/include"
declare -x COLORTERM="1"
declare -x COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin"
declare -x CONDA_DEFAULT_ENV="base"
declare -x CONDA_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/conda"
declare -x CONDA_PREFIX="/soft/applications/conda/2024-04-29/mconda3"
declare -x CONDA_PROMPT_MODIFIER="(2024-04-29/base) "
declare -x CONDA_PYTHON_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/python"
declare -x CONDA_SHLVL="1"
declare -x CPU="x86_64"
declare -x CRAYPAT_LD_LIBRARY_PATH="/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAYPAT_OPTS_EXECUTABLE="libexec64/opts"
declare -x CRAYPAT_ROOT="/opt/cray/pe/perftools/23.12.0"
declare -x CRAYPE_DIR="/opt/cray/pe/craype/2.7.30"
declare -x CRAYPE_NETWORK_TARGET="ofi"
declare -x CRAYPE_VERSION="2.7.30"
declare -x CRAY_CPU_TARGET="x86-milan"
declare -x CRAY_DSMML_BASEDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_DIR="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_PREFIX="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_ROOTDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_VER="0.2.2"
declare -x CRAY_DSMML_VERSION="0.2.2"
declare -x CRAY_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x CRAY_HDF5_PARALLEL_PREFIX="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x CRAY_HDF5_PARALLEL_VERSION="1.12.2.9"
declare -x CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:/opt/cray/pe/pmi/6.1.13/lib:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:/opt/cray/pe/mpich/8.1.28/gtl/lib:/opt/cray/pe/dsmml/0.2.2/dsmml/lib:/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAY_LMOD_COMPILER="gnu/12.0"
declare -x CRAY_LMOD_CPU="x86-milan/1.0"
declare -x CRAY_LMOD_MPI="cray-mpich/8.0"
declare -x CRAY_LMOD_NET="ofi/1.0"
declare -x CRAY_MPICH_BASEDIR="/opt/cray/pe/mpich/8.1.28/ofi"
declare -x CRAY_MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_PREFIX="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_ROOTDIR="/opt/cray/pe/mpich/8.1.28"
declare -x CRAY_MPICH_VER="8.1.28"
declare -x CRAY_MPICH_VERSION="8.1.28"
declare -x CRAY_PERFTOOLS_PREFIX="/opt/cray/pe/perftools/23.12.0"
declare -x CRAY_PERFTOOLS_VERSION="23.12.0"
declare -x CRAY_PMI_INCLUDE_OPTS="-I/opt/cray/pe/pmi/6.1.13/include"
declare -x CRAY_PMI_POST_LINK_OPTS="-L/opt/cray/pe/pmi/6.1.13/lib"
declare -x CRAY_PMI_PREFIX="/opt/cray/pe/pmi/6.1.13"
declare -x CRAY_PMI_VERSION="6.1.13"
declare -x CSHEDIT="emacs"
declare -x CUDA_HOME="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_TOOLKIT_BASE="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDNN_HOME="/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/"
declare -x ENVIRONMENT="BATCH"
declare -x ENV_NAME="conda/2024-04-29"
declare -x FROM_HEADER=""
declare -x GCC_PATH="/usr/bin"
declare -x GCC_PREFIX="/usr/lib64/gcc/x86_64-suse-linux/12"
declare -x GCC_VERSION="12.3"
declare -x GNU_VERSION="12.3"
declare -x GPG_TTY="not a tty"
declare -x GSETTINGS_SCHEMA_DIR="/soft/applications/conda/2024-04-29/mconda3/share/glib-2.0/schemas"
declare -x GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=""
declare -x G_BROKEN_FILENAMES="1"
declare -x G_FILENAME_ENCODING="@locale,UTF-8,ISO-8859-15,CP1252"
declare -x HDF5_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HDF5_ROOT="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HISTSIZE="1000"
declare -x HOME="/home/shourya01"
declare -x HOST="x3003c0s37b0n0"
declare -x HOSTNAME="x3003c0s37b0n0"
declare -x HOSTTYPE="x86_64"
declare -x HTTPS_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x HTTP_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x LANG="en_US.UTF-8"
declare -x LANGUAGE="en_US.UTF-8"
declare -x LDFLAGS="-L/soft/applications/conda/2024-04-29/mconda3/lib -Wl,--enable-new-dtags,-rpath,/soft/applications/conda/2024-04-29/mconda3/lib"
declare -x LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:/soft/perftools/darshan/darshan-3.4.4/lib:/opt/cray/pe/papi/7.0.1.2/lib64:/opt/cray/libfabric/1.15.2.0/lib64"
declare -x LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so"
declare -x LESS="-M -I -R"
declare -x LESSCLOSE="lessclose.sh %s %s"
declare -x LESSKEY="/etc/lesskey.bin"
declare -x LESSOPEN="lessopen.sh %s"
declare -x LESS_ADVANCED_PREPROCESSOR="no"
declare -x LMOD_CMD="/usr/share/lmod/lmod/libexec/lmod"
declare -x LMOD_DIR="/usr/share/lmod/lmod/libexec"
declare -x LMOD_FAMILY_COMPILER="gcc-native"
declare -x LMOD_FAMILY_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_CRAYPE="craype"
declare -x LMOD_FAMILY_CRAYPE_CPU="craype-x86-milan"
declare -x LMOD_FAMILY_CRAYPE_CPU_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_NETWORK="craype-network-ofi"
declare -x LMOD_FAMILY_CRAYPE_NETWORK_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_VERSION="2.7.30"
declare -x LMOD_FAMILY_GCC_COMPILER="gcc-native"
declare -x LMOD_FAMILY_GCC_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_HDF5="cray-hdf5-parallel"
declare -x LMOD_FAMILY_HDF5_VERSION="1.12.2.9"
declare -x LMOD_FAMILY_MPI="cray-mpich"
declare -x LMOD_FAMILY_MPI_VERSION="8.1.28"
declare -x LMOD_FAMILY_PRGENV="PrgEnv-gnu"
declare -x LMOD_FAMILY_PRGENV_VERSION="8.5.0"
declare -x LMOD_FAMILY_PYTHON="conda"
declare -x LMOD_FAMILY_PYTHON_VERSION="2024-04-29"
declare -x LMOD_PKG="/usr/share/lmod/lmod"
declare -x LMOD_ROOT="/usr/share/lmod"
declare -x LMOD_SETTARG_FULL_SUPPORT="no"
declare -x LMOD_SYSTEM_DEFAULT_MODULES="PrgEnv-nvhpc:craype-network-ofi:perftools-base:darshan:xalt"
declare -x LMOD_VERSION="8.7.34"
declare -x LMOD_sys="Linux"
declare -x LOADEDMODULES="libfabric/1.15.2.0:craype-network-ofi:perftools-base/23.12.0:darshan/3.4.4:xalt/3.0.2-202408282050:gcc-native/12.3:craype/2.7.30:cray-dsmml/0.2.2:cray-mpich/8.1.28:cray-pmi/6.1.13:cray-pals/1.3.4:cray-libpals/1.3.4:craype-x86-milan:PrgEnv-gnu/8.5.0:cray-hdf5-parallel/1.12.2.9:cudnn/9.1.0:conda/2024-04-29"
declare -x LOGNAME="shourya01"
declare -x MACHTYPE="x86_64-suse-linux"
declare -x MAIL="/var/spool/mail/shourya01"
declare -x MANPATH="/opt/cray/pals/1.3.4/man:/opt/cray/pe/pmi/6.1.13/man:/opt/cray/pe/mpich/8.1.28/ofi/man:/opt/cray/pe/mpich/8.1.28/man/mpich:/opt/cray/pe/dsmml/0.2.2/dsmml/man:/opt/cray/pe/craype/2.7.30/man:/opt/cray/pe/perftools/23.12.0/man:/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:/opt/cray/libfabric/1.15.2.0/share/man:/usr/share/lmod/lmod/share/man:/home/shourya01/.local/man:/usr/local/man:/usr/share/man:/usr/man:/opt/c3/man:/opt/pbs/share/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man"
declare -x MINICOM="-c on"
declare -x MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:/opt/cray/pe/lmod/modulefiles/mix_compilers:/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:/soft/modulefiles:/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:/usr/share/modulefiles/Linux:/usr/share/modulefiles/Core:/usr/share/lmod/lmod/modulefiles/Core:/usr/share/lmod/lmod/modulefiles:/opt/cray/pals/lmod/modulefiles/core:/opt/cray/modulefiles:/opt/cray/pe/lmod/modulefiles/core:/opt/cray/pe/lmod/modulefiles/craype-targets/default:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:/soft/xalt/modulefiles"
declare -x MODULEPATH_ROOT="/usr/share/modulefiles"
declare -x MODULESHOME="/usr/share/lmod/lmod"
declare -x MORE="-sl"
declare -x MPI4JAX_USE_CUDA_MPI="1"
declare -x MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x MPICH_GPU_SUPPORT_ENABLED="1"
declare -x NCCL_IB_DISABLE="1"
declare -x NCCL_SOCKET_IFNAME="hsn"
declare -x NCPUS="64"
declare -x OFFLOAD_INIT="on_start"
declare -x OLDPWD
declare -x OMP_NUM_THREADS="4"
declare -x OSCAR_HOME="/opt/oscar"
declare -x OSTYPE="linux"
declare -x PAGER="less"
declare -x PALS_TRANSFER="0"
declare -x PATH="/soft/applications/conda/2024-04-29/mconda3/bin:/soft/applications/conda/2024-04-29/mconda3/condabin:/soft/xalt/3.0.2-202408282050/bin:/soft/compilers/cudatoolkit/cuda-12.4.1/bin:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:/opt/cray/pe/hdf5/1.12.2.9/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/home/shourya01/.local/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/shourya01/bin:/opt/cray/pe/bin"
declare -x PAT_RT_PERFCTR_DISABLE_COMPONENTS="nvml,rocm_smi"
declare -x PBS_ACCOUNT="ParaLLMs"
declare -x PBS_ENVIRONMENT="PBS_BATCH"
declare -x PBS_JOBCOOKIE="74EEFA957425B8B6438F667D3C2F2121"
declare -x PBS_JOBDIR="/home/shourya01"
declare -x PBS_JOBID="5136126.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_JOBNAME="bash"
declare -x PBS_MOMPORT="15003"
declare -x PBS_NODEFILE="/var/spool/pbs/aux/5136126.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_NODENUM="0"
declare -x PBS_O_HOME="/home/shourya01"
declare -x PBS_O_HOST="polaris-login-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_O_INTERACTIVE_AUTH_METHOD="resvport"
declare -x PBS_O_LANG="en_US.UTF-8"
declare -x PBS_O_LOGNAME="shourya01"
declare -x PBS_O_MAIL="/var/spool/mail/shourya01"
declare -x PBS_O_PATH="/home/shourya01/.local/bin:/home/shourya01/.vscode-server/cli/servers/Stable-91fa95bccb027ece6a968589bb1d662fa9c8e170/server/bin/remote-cli:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/soft/xalt/3.0.2-202408282050/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/nvidia/23.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/extras/qd/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/shourya01/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/dbhome/db2cat/sqllib/bin:/dbhome/db2cat/sqllib/adm:/dbhome/db2cat/sqllib/misc:/dbhome/db2cat/sqllib/gskit/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/opt/cray/pe/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.vscode-server/extensions/ms-python.debugpy-2025.8.0/bundled/scripts/noConfigScripts"
declare -x PBS_O_QUEUE="debug-scaling"
declare -x PBS_O_SHELL="/bin/bash"
declare -x PBS_O_SYSTEM="Linux"
declare -x PBS_O_WORKDIR="/home/shourya01"
declare -x PBS_QUEUE="debug-scaling"
declare -x PBS_TASKNUM="1"
declare -x PELOCAL_PRGENV="true"
declare -x PERFTOOLS_VERSION="23.12.0"
declare -x PE_DSMML_MODULE_NAME="cray-dsmml"
declare -x PE_DSMML_PKGCONFIG_LIBS="dsmml"
declare -x PE_ENV="GNU"
declare -x PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel:mpichf90"
declare -x PE_GCC_EXTERNAL="native"
declare -x PE_GCC_LEVEL="12"
declare -x PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig"
declare -x PE_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x PE_HDF5_PARALLEL_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel"
declare -x PE_HDF5_PARALLEL_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel"
declare -x PE_MPICH_FIXED_PRGENV="GNU"
declare -x PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90"
declare -x PE_MPICH_GENCOMPILERS_GNU="12.3"
declare -x PE_MPICH_GTL_DIR_amd_gfx906="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx908="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx90a="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx940="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx942="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia70="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia80="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia90="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_ponteVecchio="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_LIBS_amd_gfx906="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx908="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx90a="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx940="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx942="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_nvidia70="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia80="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia90="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_ponteVecchio="-lmpi_gtl_ze"
declare -x PE_MPICH_MODULE_NAME="cray-mpich"
declare -x PE_MPICH_PKGCONFIG_LIBS="mpich"
declare -x PE_MPICH_PKGCONFIG_VARIABLES="PE_MPICH_GTL_DIR_@accelerator@:PE_MPICH_GTL_LIBS_@accelerator@"
declare -x PE_PALS_PKGCONFIG_LIBS="libpals"
declare -x PE_PERFTOOLS_MPICH_LIBDIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib"
declare -x PE_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel:mpich:dsmml:darshan-runtime"
declare -x PE_PKGCONFIG_PRODUCTS="PE_PALS:PE_PMI:PE_MPICH:PE_DSMML"
declare -x PE_PMI_PKGCONFIG_LIBS="cray-pmi"
declare -x PE_PRODUCT_LIST="CRAYPE_X86_MILAN"
declare -x PKGCONFIG_ENABLED="1"
declare -x PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:/opt/cray/pals/1.3.4/lib/pkgconfig:/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:/opt/cray/pe/craype/2.7.30/pkg-config:/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig"
declare -x PROFILEREAD="true"
declare -x PWD="/home/shourya01"
declare -x PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages"
declare -x PYTHONUSERBASE="/home/shourya01/.local/polaris/conda/2024-04-29"
declare -x QT_SYSTEM_DIR="/usr/share/desktop-data"
declare -x SHELL="/bin/bash"
declare -x SHLVL="2"
declare -x SLURM_MPI_TYPE="cray_shasta"
declare -x STARSHIP_SESSION_KEY="2352065614933223"
declare -x STARSHIP_SHELL="bash"
declare -x TMPDIR="/var/tmp/pbs.5136126.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x TRITON_DISABLE_AUTOTUNE="1"
declare -x TZ="Etc/UTC"
declare -x USER="shourya01"
declare -x USE_PCM_DB="2"
declare -x WINDOWMANAGER="xterm"
declare -x XALT_DIR="/soft/xalt/3.0.2-202408282050"
declare -x XALT_EXECUTABLE_TRACKING="yes"
declare -x XALT_SAMPLING="no"
declare -x XALT_SCALAR_AND_SPSR_SAMPLING="yes"
declare -x XCURSOR_THEME="DMZ"
declare -x XDG_CONFIG_DIRS="/etc/xdg"
declare -x XDG_DATA_DIRS="/usr/share"
declare -x XKEYSYMDB="/usr/X11R6/lib/X11/XKeysymDB"
declare -x XLA_FLAGS="--xla_gpu_force_compilation_parallelism=1 --xla_gpu_cuda_data_dir=/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x XLA_PYTHON_CLIENT_PREALLOCATE="false"
declare -x XML_CATALOG_FILES="file:///soft/applications/conda/2024-04-29/mconda3/etc/xml/catalog file:///etc/xml/catalog"
declare -x XNLSPATH="/usr/X11R6/lib/X11/nls"
declare -x _CE_CONDA=""
declare -x _CE_M=""
declare -x _LMFILES_="/opt/cray/modulefiles/libfabric/1.15.2.0:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-network-ofi.lua:/opt/cray/pe/lmod/modulefiles/core/perftools-base/23.12.0.lua:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles/darshan/3.4.4:/soft/xalt/modulefiles/xalt/3.0.2-202408282050:/opt/cray/pe/lmod/modulefiles/core/gcc-native/12.3.lua:/opt/cray/pe/lmod/modulefiles/core/craype/2.7.30.lua:/opt/cray/pe/lmod/modulefiles/core/cray-dsmml/0.2.2.lua:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0/cray-mpich/8.1.28.lua:/opt/cray/pe/lmod/modulefiles/core/cray-pmi/6.1.13.lua:/opt/cray/pals/lmod/modulefiles/core/cray-pals/1.3.4.lua:/opt/cray/pals/lmod/modulefiles/core/cray-libpals/1.3.4.lua:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-x86-milan.lua:/opt/cray/pe/lmod/modulefiles/core/PrgEnv-gnu/8.5.0.lua:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2.9.lua:/soft/modulefiles/cudnn/9.1.0.lua:/soft/modulefiles/conda/2024-04-29.lua"
declare -x _ModuleTable001_="X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpQcmdFbnYgPSAiUHJnRW52LWdudSIsCmNvbXBpbGVyID0gImdjYy1uYXRpdmUiLApjcmF5cGUgPSAiY3JheXBlIiwKY3JheXBlX2NwdSA9ICJjcmF5cGUteDg2LW1pbGFuIiwKY3JheXBlX25ldHdvcmsgPSAiY3JheXBlLW5ldHdvcmstb2ZpIiwKZ2NjX2NvbXBpbGVyID0gImdjYy1uYXRpdmUiLApoZGY1ID0gImNyYXktaGRmNS1wYXJhbGxlbCIsCm1waSA9ICJjcmF5LW1waWNoIiwKcHl0aG9uID0gImNvbmRhIiwKfSwKbVQgPSB7ClsiUHJnRW52LWdudSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUv"
declare -x _ModuleTable002_="bG1vZC9tb2R1bGVmaWxlcy9jb3JlL1ByZ0Vudi1nbnUvOC41LjAubHVhIiwKZnVsbE5hbWUgPSAiUHJnRW52LWdudS84LjUuMCIsCmxvYWRPcmRlciA9IDE0LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gIlByZ0Vudi1nbnUiLAp3ViA9ICJeMDAwMDAwMDguMDAwMDAwMDA1Lip6ZmluYWwiLAp9LApjb25kYSA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY29uZGEvMjAyNC0wNC0yOS5sdWEiLApmdWxsTmFtZSA9ICJjb25kYS8yMDI0LTA0LTI5IiwKbG9hZE9yZGVyID0gMTcsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY29uZGEiLAp3ViA9ICJeMDAw"
declare -x _ModuleTable003_="MDIwMjQuKnpmaW5hbC0uMDAwMDAwMDA0Lip6ZmluYWwtLjAwMDAwMDAyOS4qemZpbmFsIiwKfSwKWyJjcmF5LWRzbW1sIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1kc21tbC8wLjIuMi5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWRzbW1sLzAuMi4yIiwKbG9hZE9yZGVyID0gOCwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWRzbW1sIiwKd1YgPSAiXjAwMDAwMDAwLjAwMDAwMDAwMi4wMDAwMDAwMDIuKnpmaW5hbCIsCn0sClsiY3JheS1oZGY1LXBhcmFsbGVsIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9v"
declare -x _ModuleTable004_="ZmkvMS4wL2NyYXktbXBpY2gvOC4wL2NyYXktaGRmNS1wYXJhbGxlbC8xLjEyLjIuOS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLApsb2FkT3JkZXIgPSAxNSwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDEyLjAwMDAwMDAwMi4wMDAwMDAwMDkuKnpmaW5hbCIsCn0sClsiY3JheS1saWJwYWxzIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wYWxzL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5LWxpYnBhbHMvMS4zLjQubHVhIiwKZnVsbE5hbWUgPSAiY3JheS1s"
declare -x _ModuleTable005_="aWJwYWxzLzEuMy40IiwKbG9hZE9yZGVyID0gMTIsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1saWJwYWxzIiwKd1YgPSAiXjAwMDAwMDAxLjAwMDAwMDAwMy4wMDAwMDAwMDQuKnpmaW5hbCIsCn0sClsiY3JheS1tcGljaCJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMC9jcmF5LW1waWNoLzguMS4yOC5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LW1waWNoLzguMS4yOCIsCmxvYWRPcmRlciA9IDksCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1tcGljaCIsCndWID0gIl4w"
declare -x _ModuleTable006_="MDAwMDAwOC4wMDAwMDAwMDEuMDAwMDAwMDI4Lip6ZmluYWwiLAp9LApbImNyYXktcGFscyJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGFscy9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wYWxzLzEuMy40Lmx1YSIsCmZ1bGxOYW1lID0gImNyYXktcGFscy8xLjMuNCIsCmxvYWRPcmRlciA9IDExLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcGFscyIsCndWID0gIl4wMDAwMDAwMS4wMDAwMDAwMDMuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImNyYXktcG1pIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wbWkvNi4xLjEzLmx1YSIsCmZ1bGxOYW1lID0gImNy"
declare -x _ModuleTable007_="YXktcG1pLzYuMS4xMyIsCmxvYWRPcmRlciA9IDEwLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcG1pIiwKd1YgPSAiXjAwMDAwMDA2LjAwMDAwMDAwMS4wMDAwMDAwMTMuKnpmaW5hbCIsCn0sCmNyYXlwZSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5cGUvMi43LjMwLmx1YSIsCmZ1bGxOYW1lID0gImNyYXlwZS8yLjcuMzAiLApsb2FkT3JkZXIgPSA3LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZSIsCndWID0gIl4wMDAwMDAwMi4wMDAwMDAwMDcuMDAwMDAwMDMwLip6ZmluYWwiLAp9LApb"
declare -x _ModuleTable008_="ImNyYXlwZS1uZXR3b3JrLW9maSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS1uZXR3b3JrLW9maS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5cGUtbmV0d29yay1vZmkiLApsb2FkT3JkZXIgPSAyLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS1uZXR3b3JrLW9maSIsCndWID0gIk0uKnpmaW5hbCIsCn0sClsiY3JheXBlLXg4Ni1taWxhbiJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS14ODYtbWlsYW4ubHVhIiwKZnVsbE5hbWUgPSAiY3JheXBl"
declare -x _ModuleTable009_="LXg4Ni1taWxhbiIsCmxvYWRPcmRlciA9IDEzLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS14ODYtbWlsYW4iLAp3ViA9ICJNLip6ZmluYWwiLAp9LApjdWRubiA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY3Vkbm4vOS4xLjAubHVhIiwKZnVsbE5hbWUgPSAiY3Vkbm4vOS4xLjAiLApsb2FkT3JkZXIgPSAxNiwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjdWRubi85LjEuMCIsCndWID0gIjAwMDAwMDAwOS4wMDAwMDAwMDEuKnpmaW5hbCIsCn0sCmRhcnNoYW4gPSB7CmZuID0gIi9zb2Z0L3BlcmZ0b29scy9k"
declare -x _ModuleTable010_="YXJzaGFuL2RhcnNoYW4tMy40LjQvc2hhcmUvY3JheXBlLTIueC9tb2R1bGVmaWxlcy9kYXJzaGFuLzMuNC40IiwKZnVsbE5hbWUgPSAiZGFyc2hhbi8zLjQuNCIsCmxvYWRPcmRlciA9IDQsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiZGFyc2hhbiIsCndWID0gIjAwMDAwMDAwMy4wMDAwMDAwMDQuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImdjYy1uYXRpdmUiXSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9nY2MtbmF0aXZlLzEyLjMubHVhIiwKZnVsbE5hbWUgPSAiZ2NjLW5hdGl2ZS8xMi4zIiwKbG9hZE9yZGVyID0gNiwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0"
declare -x _ModuleTable011_="YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJnY2MtbmF0aXZlIiwKd1YgPSAiXjAwMDAwMDEyLjAwMDAwMDAwMy4qemZpbmFsIiwKfSwKbGliZmFicmljID0gewpmbiA9ICIvb3B0L2NyYXkvbW9kdWxlZmlsZXMvbGliZmFicmljLzEuMTUuMi4wIiwKZnVsbE5hbWUgPSAibGliZmFicmljLzEuMTUuMi4wIiwKbG9hZE9yZGVyID0gMSwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJsaWJmYWJyaWMiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDE1LjAwMDAwMDAwMi4qemZpbmFsIiwKfSwKWyJwZXJmdG9vbHMtYmFzZSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb3JlL3BlcmZ0b29s"
declare -x _ModuleTable012_="cy1iYXNlLzIzLjEyLjAubHVhIiwKZnVsbE5hbWUgPSAicGVyZnRvb2xzLWJhc2UvMjMuMTIuMCIsCmxvYWRPcmRlciA9IDMsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAicGVyZnRvb2xzLWJhc2UiLAp3ViA9ICJeMDAwMDAwMjMuMDAwMDAwMDEyLip6ZmluYWwiLAp9LAp4YWx0ID0gewpmbiA9ICIvc29mdC94YWx0L21vZHVsZWZpbGVzL3hhbHQvMy4wLjItMjAyNDA4MjgyMDUwIiwKZnVsbE5hbWUgPSAieGFsdC8zLjAuMi0yMDI0MDgyODIwNTAiLApsb2FkT3JkZXIgPSA1LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gInhhbHQiLAp3ViA9ICJeMDAwMDAw"
declare -x _ModuleTable013_="MDMuMDAwMDAwMDAwLjAwMDAwMDAwMi4qemZpbmFsLS4yMDI0MDgyODIwNTAuKnpmaW5hbCIsCn0sCn0sCm1wYXRoQSA9IHsKCiIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9oZGY1LXBhcmFsbGVsL2dudS8xMi4wL29maS8xLjAvY3JheS1tcGljaC84LjAvY3JheS1oZGY1LXBhcmFsbGVsLzEuMTIuMiIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3B1L3g4Ni1taWxhbi8xLjAiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9vZmkvMS4wL2NyYXktbXBpY2gvOC4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMCIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxl"
declare -x _ModuleTable014_="ZmlsZXMvbWl4X2NvbXBpbGVycyIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29tcGlsZXIvZ251LzEyLjAiLCAiL3NvZnQvbW9kdWxlZmlsZXMiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL3BlcmZ0b29scy8yMy4xMi4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9uZXQvb2ZpLzEuMCIsICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0xpbnV4IgosICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmUiLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZSIKLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BhbHMvbG1vZC9tb2R1bGVmaWxlcy9jb3JlIgosICIvb3B0L2Ny"
declare -x _ModuleTable015_="YXkvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZSIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdCIKLCAiL3NvZnQvcGVyZnRvb2xzL2RhcnNoYW4vZGFyc2hhbi0zLjQuNC9zaGFyZS9jcmF5cGUtMi54L21vZHVsZWZpbGVzIiwgIi9zb2Z0L3hhbHQvbW9kdWxlZmlsZXMiLAp9LApzeXN0ZW1CYXNlTVBBVEggPSAiL3Vzci9zaGFyZS9tb2R1bGVmaWxlcy9MaW51eDovdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmU6L3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZTovdXNyL3NoYXJlL2xtb2QvbG1vZC9tb2R1bGVmaWxlczovb3B0L2NyYXkvcGFscy9sbW9kL21vZHVs"
declare -x _ModuleTable016_="ZWZpbGVzL2NvcmU6L29wdC9jcmF5L21vZHVsZWZpbGVzOi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmU6L29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdDovc29mdC9wZXJmdG9vbHMvZGFyc2hhbi9kYXJzaGFuLTMuNC40L3NoYXJlL2NyYXlwZS0yLngvbW9kdWxlZmlsZXM6L3NvZnQveGFsdC9tb2R1bGVmaWxlcyIsCn0K"
declare -x _ModuleTable_Sz_="16"
declare -x __LMOD_Priority_PATH="/soft/xalt/3.0.2-202408282050/bin:-100"
declare -x __LMOD_REF_COUNT_COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin:1"
declare -x __LMOD_REF_COUNT_CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:1;/opt/cray/pe/pmi/6.1.13/lib:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:1;/opt/cray/pe/mpich/8.1.28/gtl/lib:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib:1;/opt/cray/pe/perftools/23.12.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:1;/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:1;/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:1;/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:1;/soft/perftools/darshan/darshan-3.4.4/lib:1;/opt/cray/pe/papi/7.0.1.2/lib64:1;/opt/cray/libfabric/1.15.2.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so:1"
declare -x __LMOD_REF_COUNT_MANPATH="/opt/cray/pals/1.3.4/man:2;/opt/cray/pe/pmi/6.1.13/man:1;/opt/cray/pe/mpich/8.1.28/ofi/man:1;/opt/cray/pe/mpich/8.1.28/man/mpich:1;/opt/cray/pe/dsmml/0.2.2/dsmml/man:1;/opt/cray/pe/craype/2.7.30/man:1;/opt/cray/pe/perftools/23.12.0/man:1;/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:1;/opt/cray/libfabric/1.15.2.0/share/man:1;/usr/share/lmod/lmod/share/man:1;/home/shourya01/.local/man:1;/usr/local/man:1;/usr/share/man:1;/usr/man:1;/opt/c3/man:1;/opt/pbs/share/man:1;/opt/clmgr/man:1;/opt/sgi/share/man:1;/opt/clmgr/share/man:1;/opt/clmgr/lib/cm-cli/man:1"
declare -x __LMOD_REF_COUNT_MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:1;/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:1;/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:1;/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:1;/opt/cray/pe/lmod/modulefiles/mix_compilers:1;/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:1;/soft/modulefiles:1;/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:1;/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:1;/usr/share/modulefiles/Linux:1;/usr/share/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles:1;/opt/cray/pals/lmod/modulefiles/core:1;/opt/cray/modulefiles:1;/opt/cray/pe/lmod/modulefiles/core:1;/opt/cray/pe/lmod/modulefiles/craype-targets/default:1;/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:1;/soft/xalt/modulefiles:1"
declare -x __LMOD_REF_COUNT_PATH="/soft/xalt/3.0.2-202408282050/bin:1;/soft/compilers/cudatoolkit/cuda-12.4.1/bin:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:1;/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:1;/opt/cray/pe/hdf5/1.12.2.9/bin:1;/opt/cray/pals/1.3.4/bin:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:1;/opt/cray/pe/mpich/8.1.28/bin:1;/opt/cray/pe/craype/2.7.30/bin:1;/home/shourya01/.local/bin:4;/soft/perftools/darshan/darshan-3.4.4/bin:1;/opt/cray/pe/perftools/23.12.0/bin:1;/opt/cray/pe/papi/7.0.1.2/bin:1;/opt/cray/libfabric/1.15.2.0/bin:1;/opt/clmgr/sbin:1;/opt/clmgr/bin:1;/opt/sgi/sbin:1;/opt/sgi/bin:1;/usr/local/bin:1;/usr/bin:1;/bin:2;/opt/c3/bin:1;/usr/lib/mit/bin:1;/usr/lib/mit/sbin:1;/opt/pbs/bin:1;/sbin:1;/home/shourya01/bin:1;/opt/cray/pe/bin:1"
declare -x __LMOD_REF_COUNT_PE_DSMML_PKGCONFIG_LIBS="dsmml:1"
declare -x __LMOD_REF_COUNT_PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:1;hdf5_fortran_parallel:1;mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FIXED_PRGENV="GNU:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_GENCOMPILERS_GNU="12.3:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_PKGCONFIG_LIBS="mpich:1"
declare -x __LMOD_REF_COUNT_PE_PALS_PKGCONFIG_LIBS="libpals:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_LIBS="hdf5_hl_parallel:1;hdf5_parallel:1;mpich:1;dsmml:1;darshan-runtime:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_PRODUCTS="PE_PALS:1;PE_PMI:1;PE_MPICH:1;PE_DSMML:1"
declare -x __LMOD_REF_COUNT_PE_PMI_PKGCONFIG_LIBS="cray-pmi:1"
declare -x __LMOD_REF_COUNT_PE_PRODUCT_LIST="CRAYPE_X86_MILAN:1;PERFTOOLS:1;CRAYPAT:1"
declare -x __LMOD_REF_COUNT_PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:1;/opt/cray/pals/1.3.4/lib/pkgconfig:1;/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:1;/opt/cray/pe/craype/2.7.30/pkg-config:1;/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:1;/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages:1"
declare -x ftp_proxy="http://proxy.alcf.anl.gov:3128"
declare -x http_proxy="http://proxy.alcf.anl.gov:3128"
declare -x https_proxy="http://proxy.alcf.anl.gov:3128"
declare -x no_proxy="admin,polaris-adminvm-01,localhost,*.cm.polaris.alcf.anl.gov,polaris-*,*.polaris.alcf.anl.gov,*.alcf.anl.gov"
Running on 7 nodes
Total number of GPUs: 28
Connected to tcp://x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov:7919
Found executable /soft/applications/conda/2024-04-29/mconda3/bin/python
Launching application 9239d3c7-cd2a-47d4-8963-0b42402d6774
Using PMI port 49788,49789
[2025-06-19 11:14:56,382] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,382] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,382] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,385] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,458] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,458] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,458] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,458] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,489] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,489] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,489] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,491] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,512] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,512] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,512] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,512] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,519] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,520] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,519] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,519] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,717] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,717] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,717] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,717] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,717] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,717] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,717] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:14:56,717] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:15:53,815] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:15:53,815] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:15:54,121] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:15:54,121] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:15:54,255] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:15:54,256] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:15:55,183] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:15:55,183] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:02,697] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:02,697] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:02,826] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:02,826] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:02,885] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:02,885] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:03,968] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:03,968] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:04,681] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:04,682] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:04,732] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:04,732] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:04,787] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:04,787] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:04,826] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:04,826] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:04,863] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:04,864] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:04,900] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:04,900] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:04,961] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:04,962] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:04,965] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:04,967] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:05,095] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:05,095] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:05,140] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:05,141] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:05,147] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:05,147] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:05,148] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:05,148] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:05,177] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:05,177] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:05,182] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:05,182] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:05,187] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:05,187] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:06,034] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:06,034] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:06,071] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:06,071] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:06,108] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:06,108] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:06,190] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:06,190] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:19:06,311] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:19:06,311] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=20, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=24, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=8, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=12, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=16, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=21, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=25, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=9, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=14, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=17, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=22, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=26, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=10, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=15, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=18, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=27, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=11, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=13, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=19, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=23, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:19:06,317] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
Initialized deepspeed on global rank 0, local rank 0 with world size 28.
[2025-06-19 11:19:08,578] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2+5f631abc, git-hash=5f631abc, git-branch=HEAD
[2025-06-19 11:19:21,368] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-06-19 11:19:21,369] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-06-19 11:19:21,369] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-06-19 11:19:21,384] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-06-19 11:19:21,384] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-06-19 11:19:21,384] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-06-19 11:19:21,384] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-06-19 11:19:21,384] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2025-06-19 11:19:21,385] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2025-06-19 11:19:21,385] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-06-19 11:19:21,385] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-06-19 11:19:21,385] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2025-06-19 11:19:21,385] [INFO] [config.py:1000:print]   amp_params ................... False
[2025-06-19 11:19:21,385] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-06-19 11:19:21,385] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2025-06-19 11:19:21,385] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2025-06-19 11:19:21,385] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2025-06-19 11:19:21,385] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2025-06-19 11:19:21,385] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2025-06-19 11:19:21,385] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x14d72c02dfd0>
[2025-06-19 11:19:21,385] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2025-06-19 11:19:21,385] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2025-06-19 11:19:21,385] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-06-19 11:19:21,385] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   dump_state ................... False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   global_rank .................. 0
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 5e-05, 'weight_decay': 0.01}
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   pld_params ................... False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2025-06-19 11:19:21,386] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2025-06-19 11:19:21,387] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2025-06-19 11:19:21,387] [INFO] [config.py:1000:print]   steps_per_print .............. 100000
[2025-06-19 11:19:21,387] [INFO] [config.py:1000:print]   train_batch_size ............. 3584
[2025-06-19 11:19:21,387] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128
[2025-06-19 11:19:21,387] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2025-06-19 11:19:21,387] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2025-06-19 11:19:21,387] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2025-06-19 11:19:21,387] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2025-06-19 11:19:21,387] [INFO] [config.py:1000:print]   world_size ................... 28
[2025-06-19 11:19:21,387] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2025-06-19 11:19:21,387] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-06-19 11:19:21,387] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2025-06-19 11:19:21,387] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2025-06-19 11:19:21,387] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2025-06-19 11:19:21,387] [INFO] [config.py:986:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 128, 
    "train_batch_size": 3.584000e+03, 
    "steps_per_print": 1.000000e+05, 
    "gradient_accumulation_steps": 1, 
    "fp16": {
        "enabled": false
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 5e-05, 
            "weight_decay": 0.01
        }
    }, 
    "comms_logger": {
        "enabled": true, 
        "verbose": false
    }, 
    "zero_optimization": {
        "stage": 0
    }
}
Validating lr=5e-05, train epoch 0.:   0%|          | 0/91 [00:00<?, ?it/s]Validating lr=5e-05, train epoch 0.:   1%|          | 1/91 [00:04<07:26,  4.96s/it]Validating lr=5e-05, train epoch 0.:   2%|▏         | 2/91 [00:09<06:44,  4.55s/it]Validating lr=5e-05, train epoch 0.:   3%|▎         | 3/91 [00:13<06:28,  4.42s/it]Validating lr=5e-05, train epoch 0.:   4%|▍         | 4/91 [00:17<06:19,  4.36s/it]Validating lr=5e-05, train epoch 0.:   5%|▌         | 5/91 [00:22<06:12,  4.33s/it]Validating lr=5e-05, train epoch 0.:   7%|▋         | 6/91 [00:26<06:04,  4.29s/it]Validating lr=5e-05, train epoch 0.:   8%|▊         | 7/91 [00:30<05:58,  4.27s/it]Validating lr=5e-05, train epoch 0.:   9%|▉         | 8/91 [00:34<05:54,  4.27s/it]Validating lr=5e-05, train epoch 0.:  10%|▉         | 9/91 [00:38<05:48,  4.25s/it]Validating lr=5e-05, train epoch 0.:  11%|█         | 10/91 [00:43<05:45,  4.27s/it]Validating lr=5e-05, train epoch 0.:  12%|█▏        | 11/91 [00:47<05:40,  4.26s/it]Validating lr=5e-05, train epoch 0.:  13%|█▎        | 12/91 [00:51<05:35,  4.25s/it]Validating lr=5e-05, train epoch 0.:  14%|█▍        | 13/91 [00:55<05:31,  4.25s/it]Validating lr=5e-05, train epoch 0.:  15%|█▌        | 14/91 [01:00<05:27,  4.25s/it]Validating lr=5e-05, train epoch 0.:  16%|█▋        | 15/91 [01:04<05:22,  4.25s/it]Validating lr=5e-05, train epoch 0.:  18%|█▊        | 16/91 [01:08<05:18,  4.24s/it]Validating lr=5e-05, train epoch 0.:  19%|█▊        | 17/91 [01:12<05:15,  4.26s/it]Validating lr=5e-05, train epoch 0.:  20%|█▉        | 18/91 [01:17<05:11,  4.27s/it]Validating lr=5e-05, train epoch 0.:  21%|██        | 19/91 [01:21<05:06,  4.25s/it]Validating lr=5e-05, train epoch 0.:  22%|██▏       | 20/91 [01:25<05:02,  4.25s/it]Validating lr=5e-05, train epoch 0.:  23%|██▎       | 21/91 [01:30<04:58,  4.27s/it]Validating lr=5e-05, train epoch 0.:  24%|██▍       | 22/91 [01:34<04:55,  4.28s/it]Validating lr=5e-05, train epoch 0.:  25%|██▌       | 23/91 [01:38<04:51,  4.28s/it]Validating lr=5e-05, train epoch 0.:  26%|██▋       | 24/91 [01:42<04:45,  4.26s/it]Validating lr=5e-05, train epoch 0.:  27%|██▋       | 25/91 [01:47<04:41,  4.26s/it]Validating lr=5e-05, train epoch 0.:  29%|██▊       | 26/91 [01:51<04:35,  4.25s/it]Validating lr=5e-05, train epoch 0.:  30%|██▉       | 27/91 [01:55<04:30,  4.22s/it]Validating lr=5e-05, train epoch 0.:  31%|███       | 28/91 [01:59<04:25,  4.21s/it]Validating lr=5e-05, train epoch 0.:  32%|███▏      | 29/91 [02:03<04:21,  4.22s/it]Validating lr=5e-05, train epoch 0.:  33%|███▎      | 30/91 [02:08<04:16,  4.21s/it]Validating lr=5e-05, train epoch 0.:  34%|███▍      | 31/91 [02:12<04:13,  4.22s/it]Validating lr=5e-05, train epoch 0.:  35%|███▌      | 32/91 [02:16<04:10,  4.24s/it]Validating lr=5e-05, train epoch 0.:  36%|███▋      | 33/91 [02:20<04:06,  4.25s/it]Validating lr=5e-05, train epoch 0.:  37%|███▋      | 34/91 [02:25<04:02,  4.25s/it]Validating lr=5e-05, train epoch 0.:  38%|███▊      | 35/91 [02:29<03:57,  4.24s/it]Validating lr=5e-05, train epoch 0.:  40%|███▉      | 36/91 [02:33<03:53,  4.25s/it]Validating lr=5e-05, train epoch 0.:  41%|████      | 37/91 [02:37<03:48,  4.24s/it]Validating lr=5e-05, train epoch 0.:  42%|████▏     | 38/91 [02:42<03:44,  4.23s/it]Validating lr=5e-05, train epoch 0.:  43%|████▎     | 39/91 [02:46<03:39,  4.22s/it]Validating lr=5e-05, train epoch 0.:  44%|████▍     | 40/91 [02:50<03:35,  4.22s/it]Validating lr=5e-05, train epoch 0.:  45%|████▌     | 41/91 [02:54<03:30,  4.21s/it]Validating lr=5e-05, train epoch 0.:  46%|████▌     | 42/91 [02:58<03:26,  4.21s/it]Validating lr=5e-05, train epoch 0.:  47%|████▋     | 43/91 [03:03<03:22,  4.21s/it]Validating lr=5e-05, train epoch 0.:  48%|████▊     | 44/91 [03:07<03:17,  4.21s/it]Validating lr=5e-05, train epoch 0.:  49%|████▉     | 45/91 [03:11<03:13,  4.21s/it]Validating lr=5e-05, train epoch 0.:  51%|█████     | 46/91 [03:15<03:09,  4.21s/it]Validating lr=5e-05, train epoch 0.:  52%|█████▏    | 47/91 [03:19<03:05,  4.21s/it]Validating lr=5e-05, train epoch 0.:  53%|█████▎    | 48/91 [03:24<03:01,  4.21s/it]Validating lr=5e-05, train epoch 0.:  54%|█████▍    | 49/91 [03:28<02:57,  4.24s/it]Validating lr=5e-05, train epoch 0.:  55%|█████▍    | 50/91 [03:32<02:52,  4.22s/it]Validating lr=5e-05, train epoch 0.:  56%|█████▌    | 51/91 [03:36<02:48,  4.20s/it]Validating lr=5e-05, train epoch 0.:  57%|█████▋    | 52/91 [03:41<02:43,  4.20s/it]Validating lr=5e-05, train epoch 0.:  58%|█████▊    | 53/91 [03:45<02:40,  4.22s/it]Validating lr=5e-05, train epoch 0.:  59%|█████▉    | 54/91 [03:49<02:36,  4.22s/it]Validating lr=5e-05, train epoch 0.:  60%|██████    | 55/91 [03:53<02:32,  4.23s/it]Validating lr=5e-05, train epoch 0.:  62%|██████▏   | 56/91 [03:57<02:27,  4.21s/it]Validating lr=5e-05, train epoch 0.:  63%|██████▎   | 57/91 [04:02<02:23,  4.21s/it]Validating lr=5e-05, train epoch 0.:  64%|██████▎   | 58/91 [04:06<02:18,  4.21s/it]Validating lr=5e-05, train epoch 0.:  65%|██████▍   | 59/91 [04:10<02:14,  4.21s/it]Validating lr=5e-05, train epoch 0.:  66%|██████▌   | 60/91 [04:14<02:10,  4.23s/it]Validating lr=5e-05, train epoch 0.:  67%|██████▋   | 61/91 [04:18<02:06,  4.21s/it]Validating lr=5e-05, train epoch 0.:  68%|██████▊   | 62/91 [04:23<02:02,  4.22s/it]Validating lr=5e-05, train epoch 0.:  69%|██████▉   | 63/91 [04:27<01:58,  4.22s/it]Validating lr=5e-05, train epoch 0.:  70%|███████   | 64/91 [04:31<01:53,  4.21s/it]Validating lr=5e-05, train epoch 0.:  71%|███████▏  | 65/91 [04:35<01:49,  4.21s/it]Validating lr=5e-05, train epoch 0.:  73%|███████▎  | 66/91 [04:40<01:45,  4.21s/it]Validating lr=5e-05, train epoch 0.:  74%|███████▎  | 67/91 [04:44<01:40,  4.20s/it]Validating lr=5e-05, train epoch 0.:  75%|███████▍  | 68/91 [04:48<01:36,  4.20s/it]Validating lr=5e-05, train epoch 0.:  76%|███████▌  | 69/91 [04:52<01:32,  4.21s/it]Validating lr=5e-05, train epoch 0.:  77%|███████▋  | 70/91 [04:56<01:28,  4.21s/it]Validating lr=5e-05, train epoch 0.:  78%|███████▊  | 71/91 [05:01<01:24,  4.20s/it]Validating lr=5e-05, train epoch 0.:  79%|███████▉  | 72/91 [05:05<01:19,  4.20s/it]Validating lr=5e-05, train epoch 0.:  80%|████████  | 73/91 [05:09<01:15,  4.21s/it]Validating lr=5e-05, train epoch 0.:  81%|████████▏ | 74/91 [05:13<01:11,  4.21s/it]Validating lr=5e-05, train epoch 0.:  82%|████████▏ | 75/91 [05:17<01:07,  4.20s/it]Validating lr=5e-05, train epoch 0.:  84%|████████▎ | 76/91 [05:22<01:03,  4.20s/it]Validating lr=5e-05, train epoch 0.:  85%|████████▍ | 77/91 [05:26<00:58,  4.20s/it]Validating lr=5e-05, train epoch 0.:  86%|████████▌ | 78/91 [05:30<00:54,  4.21s/it]Validating lr=5e-05, train epoch 0.:  87%|████████▋ | 79/91 [05:34<00:50,  4.21s/it]Validating lr=5e-05, train epoch 0.:  88%|████████▊ | 80/91 [05:38<00:46,  4.22s/it]Validating lr=5e-05, train epoch 0.:  89%|████████▉ | 81/91 [05:43<00:42,  4.21s/it]Validating lr=5e-05, train epoch 0.:  90%|█████████ | 82/91 [05:47<00:37,  4.22s/it]Validating lr=5e-05, train epoch 0.:  91%|█████████ | 83/91 [05:51<00:33,  4.21s/it]Validating lr=5e-05, train epoch 0.:  92%|█████████▏| 84/91 [05:55<00:29,  4.22s/it]Validating lr=5e-05, train epoch 0.:  93%|█████████▎| 85/91 [06:00<00:25,  4.24s/it]Validating lr=5e-05, train epoch 0.:  95%|█████████▍| 86/91 [06:04<00:21,  4.23s/it]Validating lr=5e-05, train epoch 0.:  96%|█████████▌| 87/91 [06:08<00:16,  4.23s/it]Validating lr=5e-05, train epoch 0.:  97%|█████████▋| 88/91 [06:12<00:12,  4.23s/it]Validating lr=5e-05, train epoch 0.:  98%|█████████▊| 89/91 [06:16<00:08,  4.22s/it]Validating lr=5e-05, train epoch 0.:  99%|█████████▉| 90/91 [06:21<00:04,  4.23s/it]Validating lr=5e-05, train epoch 0.: 100%|██████████| 91/91 [06:25<00:00,  4.25s/it]Validating lr=5e-05, train epoch 0.: 100%|██████████| 91/91 [06:25<00:00,  4.24s/it]
Validating lr=5e-05, train epoch 1.:   0%|          | 0/91 [00:00<?, ?it/s]Validating lr=5e-05, train epoch 1.:   1%|          | 1/91 [00:04<06:19,  4.22s/it]Validating lr=5e-05, train epoch 1.:   2%|▏         | 2/91 [00:08<06:14,  4.20s/it]Validating lr=5e-05, train epoch 1.:   3%|▎         | 3/91 [00:12<06:11,  4.23s/it]Validating lr=5e-05, train epoch 1.:   4%|▍         | 4/91 [00:16<06:07,  4.23s/it]Validating lr=5e-05, train epoch 1.:   5%|▌         | 5/91 [00:21<06:02,  4.21s/it]Validating lr=5e-05, train epoch 1.:   7%|▋         | 6/91 [00:25<05:57,  4.20s/it]Validating lr=5e-05, train epoch 1.:   8%|▊         | 7/91 [00:29<05:52,  4.20s/it]Validating lr=5e-05, train epoch 1.:   9%|▉         | 8/91 [00:33<05:49,  4.21s/it]Validating lr=5e-05, train epoch 1.:  10%|▉         | 9/91 [00:37<05:45,  4.21s/it]Validating lr=5e-05, train epoch 1.:  11%|█         | 10/91 [00:42<05:40,  4.20s/it]Validating lr=5e-05, train epoch 1.:  12%|█▏        | 11/91 [00:46<05:36,  4.21s/it]Validating lr=5e-05, train epoch 1.:  13%|█▎        | 12/91 [00:50<05:32,  4.20s/it]Validating lr=5e-05, train epoch 1.:  14%|█▍        | 13/91 [00:54<05:26,  4.19s/it]Validating lr=5e-05, train epoch 1.:  15%|█▌        | 14/91 [00:58<05:24,  4.21s/it]Validating lr=5e-05, train epoch 1.:  16%|█▋        | 15/91 [01:03<05:20,  4.22s/it]Validating lr=5e-05, train epoch 1.:  18%|█▊        | 16/91 [01:07<05:15,  4.21s/it]Validating lr=5e-05, train epoch 1.:  19%|█▊        | 17/91 [01:11<05:10,  4.20s/it]Validating lr=5e-05, train epoch 1.:  20%|█▉        | 18/91 [01:15<05:06,  4.20s/it]Validating lr=5e-05, train epoch 1.:  21%|██        | 19/91 [01:19<05:01,  4.19s/it]Validating lr=5e-05, train epoch 1.:  22%|██▏       | 20/91 [01:24<04:57,  4.19s/it]Validating lr=5e-05, train epoch 1.:  23%|██▎       | 21/91 [01:28<04:53,  4.20s/it]Validating lr=5e-05, train epoch 1.:  24%|██▍       | 22/91 [01:32<04:51,  4.22s/it]Validating lr=5e-05, train epoch 1.:  25%|██▌       | 23/91 [01:36<04:46,  4.22s/it]Validating lr=5e-05, train epoch 1.:  26%|██▋       | 24/91 [01:41<04:42,  4.22s/it]Validating lr=5e-05, train epoch 1.:  27%|██▋       | 25/91 [01:45<04:38,  4.21s/it]Validating lr=5e-05, train epoch 1.:  29%|██▊       | 26/91 [01:49<04:33,  4.21s/it]Validating lr=5e-05, train epoch 1.:  30%|██▉       | 27/91 [01:53<04:29,  4.21s/it]Validating lr=5e-05, train epoch 1.:  31%|███       | 28/91 [01:57<04:25,  4.22s/it]Validating lr=5e-05, train epoch 1.:  32%|███▏      | 29/91 [02:02<04:20,  4.21s/it]Validating lr=5e-05, train epoch 1.:  33%|███▎      | 30/91 [02:06<04:16,  4.20s/it]Validating lr=5e-05, train epoch 1.:  34%|███▍      | 31/91 [02:10<04:12,  4.20s/it]Validating lr=5e-05, train epoch 1.:  35%|███▌      | 32/91 [02:14<04:07,  4.20s/it]Validating lr=5e-05, train epoch 1.:  36%|███▋      | 33/91 [02:18<04:04,  4.21s/it]Validating lr=5e-05, train epoch 1.:  37%|███▋      | 34/91 [02:23<03:59,  4.21s/it]Validating lr=5e-05, train epoch 1.:  38%|███▊      | 35/91 [02:27<03:55,  4.21s/it]Validating lr=5e-05, train epoch 1.:  40%|███▉      | 36/91 [02:31<03:51,  4.21s/it]Validating lr=5e-05, train epoch 1.:  41%|████      | 37/91 [02:35<03:47,  4.21s/it]Validating lr=5e-05, train epoch 1.:  42%|████▏     | 38/91 [02:39<03:43,  4.22s/it]Validating lr=5e-05, train epoch 1.:  43%|████▎     | 39/91 [02:44<03:39,  4.22s/it]Validating lr=5e-05, train epoch 1.:  44%|████▍     | 40/91 [02:48<03:35,  4.22s/it]Validating lr=5e-05, train epoch 1.:  45%|████▌     | 41/91 [02:52<03:30,  4.20s/it]Validating lr=5e-05, train epoch 1.:  46%|████▌     | 42/91 [02:56<03:25,  4.20s/it]Validating lr=5e-05, train epoch 1.:  47%|████▋     | 43/91 [03:00<03:22,  4.22s/it]Validating lr=5e-05, train epoch 1.:  48%|████▊     | 44/91 [03:05<03:18,  4.22s/it]Validating lr=5e-05, train epoch 1.:  49%|████▉     | 45/91 [03:09<03:13,  4.21s/it]Validating lr=5e-05, train epoch 1.:  51%|█████     | 46/91 [03:13<03:09,  4.21s/it]Validating lr=5e-05, train epoch 1.:  52%|█████▏    | 47/91 [03:17<03:05,  4.21s/it]Validating lr=5e-05, train epoch 1.:  53%|█████▎    | 48/91 [03:22<03:01,  4.22s/it]Validating lr=5e-05, train epoch 1.:  54%|█████▍    | 49/91 [03:26<02:57,  4.23s/it]Validating lr=5e-05, train epoch 1.:  55%|█████▍    | 50/91 [03:30<02:53,  4.23s/it]Validating lr=5e-05, train epoch 1.:  56%|█████▌    | 51/91 [03:34<02:49,  4.24s/it]Validating lr=5e-05, train epoch 1.:  57%|█████▋    | 52/91 [03:39<02:44,  4.22s/it]Validating lr=5e-05, train epoch 1.:  58%|█████▊    | 53/91 [03:43<02:40,  4.22s/it]Validating lr=5e-05, train epoch 1.:  59%|█████▉    | 54/91 [03:47<02:35,  4.21s/it]Validating lr=5e-05, train epoch 1.:  60%|██████    | 55/91 [03:51<02:31,  4.20s/it]Validating lr=5e-05, train epoch 1.:  62%|██████▏   | 56/91 [03:55<02:26,  4.20s/it]Validating lr=5e-05, train epoch 1.:  63%|██████▎   | 57/91 [03:59<02:22,  4.20s/it]Validating lr=5e-05, train epoch 1.:  64%|██████▎   | 58/91 [04:04<02:18,  4.21s/it]Validating lr=5e-05, train epoch 1.:  65%|██████▍   | 59/91 [04:08<02:14,  4.20s/it]Validating lr=5e-05, train epoch 1.:  66%|██████▌   | 60/91 [04:12<02:10,  4.20s/it]Validating lr=5e-05, train epoch 1.:  67%|██████▋   | 61/91 [04:16<02:05,  4.20s/it]Validating lr=5e-05, train epoch 1.:  68%|██████▊   | 62/91 [04:21<02:02,  4.21s/it]Validating lr=5e-05, train epoch 1.:  69%|██████▉   | 63/91 [04:25<01:57,  4.21s/it]Validating lr=5e-05, train epoch 1.:  70%|███████   | 64/91 [04:29<01:53,  4.19s/it]Validating lr=5e-05, train epoch 1.:  71%|███████▏  | 65/91 [04:33<01:49,  4.20s/it]Validating lr=5e-05, train epoch 1.:  73%|███████▎  | 66/91 [04:37<01:45,  4.20s/it]Validating lr=5e-05, train epoch 1.:  74%|███████▎  | 67/91 [04:41<01:40,  4.20s/it]Validating lr=5e-05, train epoch 1.:  75%|███████▍  | 68/91 [04:46<01:36,  4.20s/it]Validating lr=5e-05, train epoch 1.:  76%|███████▌  | 69/91 [04:50<01:32,  4.21s/it]Validating lr=5e-05, train epoch 1.:  77%|███████▋  | 70/91 [04:54<01:28,  4.20s/it]Validating lr=5e-05, train epoch 1.:  78%|███████▊  | 71/91 [04:58<01:23,  4.20s/it]Validating lr=5e-05, train epoch 1.:  79%|███████▉  | 72/91 [05:02<01:19,  4.20s/it]Validating lr=5e-05, train epoch 1.:  80%|████████  | 73/91 [05:07<01:15,  4.22s/it]Validating lr=5e-05, train epoch 1.:  81%|████████▏ | 74/91 [05:11<01:11,  4.20s/it]Validating lr=5e-05, train epoch 1.:  82%|████████▏ | 75/91 [05:15<01:07,  4.21s/it]Validating lr=5e-05, train epoch 1.:  84%|████████▎ | 76/91 [05:19<01:03,  4.22s/it]Validating lr=5e-05, train epoch 1.:  85%|████████▍ | 77/91 [05:24<00:59,  4.23s/it]Validating lr=5e-05, train epoch 1.:  86%|████████▌ | 78/91 [05:28<00:54,  4.23s/it]Validating lr=5e-05, train epoch 1.:  87%|████████▋ | 79/91 [05:32<00:50,  4.23s/it]Validating lr=5e-05, train epoch 1.:  88%|████████▊ | 80/91 [05:36<00:46,  4.23s/it]Validating lr=5e-05, train epoch 1.:  89%|████████▉ | 81/91 [05:41<00:42,  4.25s/it]Validating lr=5e-05, train epoch 1.:  90%|█████████ | 82/91 [05:45<00:38,  4.24s/it]Validating lr=5e-05, train epoch 1.:  91%|█████████ | 83/91 [05:49<00:33,  4.23s/it]Validating lr=5e-05, train epoch 1.:  92%|█████████▏| 84/91 [05:53<00:29,  4.23s/it]Validating lr=5e-05, train epoch 1.:  93%|█████████▎| 85/91 [05:58<00:25,  4.24s/it]Validating lr=5e-05, train epoch 1.:  95%|█████████▍| 86/91 [06:02<00:21,  4.23s/it]Validating lr=5e-05, train epoch 1.:  96%|█████████▌| 87/91 [06:06<00:16,  4.24s/it]Validating lr=5e-05, train epoch 1.:  97%|█████████▋| 88/91 [06:10<00:12,  4.23s/it]Validating lr=5e-05, train epoch 1.:  98%|█████████▊| 89/91 [06:14<00:08,  4.22s/it]Validating lr=5e-05, train epoch 1.:  99%|█████████▉| 90/91 [06:19<00:04,  4.24s/it]Validating lr=5e-05, train epoch 1.: 100%|██████████| 91/91 [06:23<00:00,  4.24s/it]Validating lr=5e-05, train epoch 1.: 100%|██████████| 91/91 [06:23<00:00,  4.21s/it]
Evaluating for lr=5e-05:   0%|          | 0/9 [00:00<?, ?it/s]Initialized deepspeed on global rank 25, local rank 1 with world size 28.
Initialized deepspeed on global rank 11, local rank 3 with world size 28.
Initialized deepspeed on global rank 21, local rank 1 with world size 28.
[rank25]: Traceback (most recent call last):
[rank25]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank25]:     )
[rank25]:      
[rank25]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank25]:     out = model_engine(x, weather)
[rank25]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank25]:     return self._call_impl(*args, **kwargs)
[rank25]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank25]:     return forward_call(*args, **kwargs)
[rank25]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank25]:     ret_val = func(*args, **kwargs)
[rank25]:               ^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank25]:     loss = self.module(*inputs, **kwargs)
[rank25]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank25]:     return self._call_impl(*args, **kwargs)
[rank25]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank25]:     return forward_call(*args, **kwargs)
[rank25]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank25]:     weather = self.vt_embed(weather)
[rank25]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/soft/applicatiInitialized deepspeed on global rank 19, local rank 3 with world size 28.
ons/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank25]:     return self._call_impl(*args, **kwargs)
[rank25]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank25]:     return forward_call(*args, **kwargs)
[rank25]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank25]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank25]:                     ^^^^^^^
[rank25]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
Initialized deepspeed on global rank 3, local rank 3 with world size 28.
[rank11]: Traceback (most recent call last):
[rank11]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank11]:     )
[rank11]:      
[rank11]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank11]:     out = model_engine(x, weather)
[rank11]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank11]:     return self._call_impl(*args, **kwargs)
[rank11]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank11]:     return forward_call(*args, **kwargs)
[rank11]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank11]:     ret_val = func(*args, **kwargs)
[rank11]:               ^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank11]:     loss = self.module(*inputs, **kwargs)
[rank11]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank11]:     return self._call_impl(*args, **kwargs)
[rank11]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank11]:     return forward_call(*args, **kwargs)
[rank11]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank11]:     weather = self.vt_embed(weather)
[rank11]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank11]:     return self._call_impl(*args, **kwargs)
[rank11]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank11]:     return forward_call(*args, **kwargs)
[rank11]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank11]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank11]:                     ^^^^^^^
[rank11]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
Initialized deepspeed on global rank 2, local rank 2 with world size 28.
Initialized deepspeed on global rank 14, local rank 2 with world size 28.
[rank21]: Traceback (most recent call last):
[rank21]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank21]:     )
[rank21]:      
[rank21]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank21]:     out = model_engine(x, weather)
[rank21]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank21]:     return self._call_impl(*args, **kwargs)
[rank21]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank21]:     return forward_call(*args, **kwargs)
[rank21]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank21]:     ret_val = func(*args, **kwargs)
[rank21]:               ^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank21]:     loss = self.module(*inputs, **kwargs)
[rank21]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank21]:     return self._call_impl(*args, **kwargs)
[rank21]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank21]:     return forward_call(*args, **kwargs)
[rank21]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank21]:     weather = self.vt_embed(weather)
[rank21]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank21]:     return self._call_impl(*args, **kwargs)
[rank21]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank21]:     return forward_call(*args, **kwargs)
[rank21]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank21]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank21]:                     ^^^^^^^
[rank21]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank2]:     )
[rank2]:      
[rank2]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank2]:     out = model_engine(x, weather)
[rank2]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank2]:     ret_val = func(*args, **kwargs)
[rank2]:               ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank2]:     loss = self.module(*inputs, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank2]:     weather = self.vt_embed(weather)
[rank2]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank2]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank2]:                     ^^^^^^^
[rank2]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
Initialized deepspeed on global rank 6, local rank 2 with world size 28.
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank3]:     )
[rank3]:      
[rank3]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank3]:     out = model_engine(x, weather)
[rank3]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank3]:     ret_val = func(*args, **kwargs)
[rank3]:               ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank3]:     loss = self.module(*inputs, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank3]:     weather = self.vt_embed(weather)
[rank3]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank3]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank3]:                     ^^^^^^^
[rank3]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
[rank19]: Traceback (most recent call last):
[rank19]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank19]:     )
[rank19]:      
[rank19]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank19]:     out = model_engine(x, weather)
[rank19]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank19]:     return self._call_impl(*args, **kwargs)
[rank19]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank19]:     return forward_call(*args, **kwargs)
[rank19]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank19]:     ret_val = func(*args, **kwargs)
[rank19]:               ^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank19]:     loss = self.module(*inputs, **kwargs)
[rank19]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank19]:     return self._call_impl(*args, **kwargs)
[rank19]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank19]:     return forward_call(*args, **kwargs)
[rank19]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank19]:     weather = self.vt_embed(weather)
[rank19]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank19]:     return self._call_impl(*args, **kwargs)
[rank19]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank19]:     return forward_call(*args, **kwargs)
[rank19]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank19]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank19]:                     ^^^^^^^
[rank19]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
Initialized deepspeed on global rank 1, local rank 1 with world size 28.
[rank14]: Traceback (most recent call last):
[rank14]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank14]:     )
[rank14]:      
[rank14]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank14]:     out = model_engine(x, weather)
[rank14]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank14]:     return self._call_impl(*args, **kwargs)
[rank14]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank14]:     return forward_call(*args, **kwargs)
[rank14]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank14]:     ret_val = func(*args, **kwargs)
[rank14]:               ^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank14]:     loss = self.module(*inputs, **kwargs)
[rank14]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank14]:     return self._call_impl(*args, **kwargs)
[rank14]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank14]:     return forward_call(*args, **kwargs)
[rank14]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank14]:     weather = self.vt_embed(weather)
[rank14]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank14]:     return self._call_impl(*args, **kwargs)
[rank14]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank14]:     return forward_call(*args, **kwargs)
[rank14]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank14]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank14]:                     ^^^^^^^
[rank14]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank1]:     )
[rank1]:      
[rank1]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank1]:     out = model_engine(x, weather)
[rank1]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank1]:     ret_val = func(*args, **kwargs)
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank1]:     loss = self.module(*inputs, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank1]:     weather = self.vt_embed(weather)
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank1]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank1]:                     ^^^^^^^
[rank1]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
[rank6]: Traceback (most recent call last):
[rank6]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank6]:     )
[rank6]:      
[rank6]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank6]:     out = model_engine(x, weather)
[rank6]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank6]:     ret_val = func(*args, **kwargs)
[rank6]:               ^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank6]:     loss = self.module(*inputs, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank6]:     weather = self.vt_embed(weather)
[rank6]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank6]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank6]:                     ^^^^^^^
[rank6]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
Initialized deepspeed on global rank 23, local rank 3 with world size 28.
Initialized deepspeed on global rank 22, local rank 2 with world size 28.
[rank23]: Traceback (most recent call last):
[rank23]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank23]:     )
[rank23]:      
[rank23]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank23]:     out = model_engine(x, weather)
[rank23]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank23]:     return self._call_impl(*args, **kwargs)
[rank23]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank23]:     return forward_call(*args, **kwargs)
[rank23]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank23]:     ret_val = func(*args, **kwargs)
[rank23]:               ^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank23]:     loss = self.module(*inputs, **kwargs)
[rank23]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank23]:     return self._call_impl(*args, **kwargs)
[rank23]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank23]:     return forward_call(*args, **kwargs)
[rank23]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank23]:     weather = self.vt_embed(weather)
[rank23]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank23]:     return self._call_impl(*args, **kwargs)
[rank23]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank23]:     return forward_call(*args, **kwargs)
[rank23]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank23]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank23]:                     ^^^^^^^
[rank23]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
[rank22]: Traceback (most recent call last):
[rank22]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank22]:     )
[rank22]:      
[rank22]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank22]:     out = model_engine(x, weather)
[rank22]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank22]:     return self._call_impl(*args, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank22]:     return forward_call(*args, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank22]:     ret_val = func(*args, **kwargs)
[rank22]:               ^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank22]:     loss = self.module(*inputs, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank22]:     return self._call_impl(*args, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank22]:     return forward_call(*args, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank22]:     weather = self.vt_embed(weather)
[rank22]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank22]:     return self._call_impl(*args, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank22]:     return forward_call(*args, **kwargs)
[rank22]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank22]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank22]:                     ^^^^^^^
[rank22]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
Initialized deepspeed on global rank 15, local rank 3 with world size 28.
[rank15]: Traceback (most recent call last):
[rank15]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank15]:     )
[rank15]:      
[rank15]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank15]:     out = model_engine(x, weather)
[rank15]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank15]:     return self._call_impl(*args, **kwargs)
[rank15]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank15]:     return forward_call(*args, **kwargs)
[rank15]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank15]:     ret_val = func(*args, **kwargs)
[rank15]:               ^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank15]:     loss = self.module(*inputs, **kwargs)
[rank15]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank15]:     return self._call_impl(*args, **kwargs)
[rank15]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank15]:     return forward_call(*args, **kwargs)
[rank15]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank15]:     weather = self.vt_embed(weather)
[rank15]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank15]:     return self._call_impl(*args, **kwargs)
[rank15]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank15]:     return forward_call(*args, **kwargs)
[rank15]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank15]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank15]:                     ^^^^^^^
[rank15]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
Initialized deepspeed on global rank 13, local rank 1 with world size 28.
Initialized deepspeed on global rank 26, local rank 2 with world size 28.
[rank13]: Traceback (most recent call last):
[rank13]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank13]:     )
[rank13]:      
[rank13]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank13]:     out = model_engine(x, weather)
[rank13]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank13]:     return self._call_impl(*args, **kwargs)
[rank13]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank13]:     return forward_call(*args, **kwargs)
[rank13]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank13]:     ret_val = func(*args, **kwargs)
[rank13]:               ^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank13]:     loss = self.module(*inputs, **kwargs)
[rank13]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank13]:     return self._call_impl(*args, **kwargs)
[rank13]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank13]:     return forward_call(*args, **kwargs)
[rank13]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank13]:     weather = self.vt_embed(weather)
[rank13]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank13]:     return self._call_impl(*args, **kwargs)
[rank13]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank13]:     return forward_call(*args, **kwargs)
[rank13]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank13]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank13]:                     ^^^^^^^
[rank13]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
Initialized deepspeed on global rank 10, local rank 2 with world size 28.
[rank26]: Traceback (most recent call last):
[rank26]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank26]:     )
[rank26]:      
[rank26]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank26]:     out = model_engine(x, weather)
[rank26]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank26]:     return self._call_impl(*args, **kwargs)
[rank26]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank26]:     return forward_call(*args, **kwargs)
[rank26]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank26]:     ret_val = func(*args, **kwargs)
[rank26]:               ^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank26]:     loss = self.module(*inputs, **kwargs)
[rank26]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank26]:     return self._call_impl(*args, **kwargs)
[rank26]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank26]:     return forward_call(*args, **kwargs)
[rank26]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank26]:     weather = self.vt_embed(weather)
[rank26]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/soft/applicati[rank10]: Traceback (most recent call last):
[rank10]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank10]:     )
[rank10]:      
[rank10]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank10]:     out = model_engine(x, weather)
[rank10]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank10]:     return self._call_impl(*args, **kwargs)
[rank10]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank10]:     return forward_call(*args, **kwargs)
[rank10]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank10]:     ret_val = func(*args, **kwargs)
[rank10]:               ^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank10]:     loss = self.module(*inputs, **kwargs)
[rank10]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank10]:     return self._call_impl(*args, **kwargs)
[rank10]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank10]:     return forward_call(*args, **kwargs)
[rank10]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank10]:     weather = self.vt_embed(weather)
[rank10]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank26]:     return self._call_impl(*args, **kwargs)
[rank26]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank26]:     return forward_call(*args, **kwargs)
[rank26]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank26]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank26]:                     ^^^^^^^
[rank26]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
ons/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank10]:     return self._call_impl(*args, **kwargs)
[rank10]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank10]:     return forward_call(*args, **kwargs)
[rank10]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank10]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank10]:                     ^^^^^^^
[rank10]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
Initialized deepspeed on global rank 7, local rank 3 with world size 28.
Initialized deepspeed on global rank 9, local rank 1 with world size 28.
[rank7]: Traceback (most recent call last):
[rank7]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank7]:     )
[rank7]:      
[rank7]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank7]:     out = model_engine(x, weather)
[rank7]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank7]:     ret_val = func(*args, **kwargs)
[rank7]:               ^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank7]:     loss = self.module(*inputs, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank7]:     weather = self.vt_embed(weather)
[rank7]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank7]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank7]:                     ^^^^^^^
[rank7]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
[rank9]: Traceback (most recent call last):
[rank9]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank9]:     )
[rank9]:      
[rank9]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank9]:     out = model_engine(x, weather)
[rank9]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank9]:     return self._call_impl(*args, **kwargs)
[rank9]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank9]:     return forward_call(*args, **kwargs)
[rank9]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank9]:     ret_val = func(*args, **kwargs)
[rank9]:               ^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank9]:     loss = self.module(*inputs, **kwargs)
[rank9]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank9]:     return self._call_impl(*args, **kwargs)
[rank9]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank9]:     return forward_call(*args, **kwargs)
[rank9]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank9]:     weather = self.vt_embed(weather)
[rank9]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank9]:     return self._call_impl(*args, **kwargs)
[rank9]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank9]:     return forward_call(*args, **kwargs)
[rank9]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank9]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank9]:                     ^^^^^^^
[rank9]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
Initialized deepspeed on global rank 5, local rank 1 with world size 28.
[rank5]: Traceback (most recent call last):
[rank5]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank5]:     )
[rank5]:      
[rank5]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank5]:     out = model_engine(x, weather)
[rank5]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank5]:     ret_val = func(*args, **kwargs)
[rank5]:               ^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank5]:     loss = self.module(*inputs, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank5]:     weather = self.vt_embed(weather)
[rank5]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank5]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank5]:                     ^^^^^^^
[rank5]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
Initialized deepspeed on global rank 27, local rank 3 with world size 28.
Initialized deepspeed on global rank 12, local rank 0 with world size 28.
[rank27]: Traceback (most recent call last):
[rank27]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank27]:     )
[rank27]:      
[rank27]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank27]:     out = model_engine(x, weather)
[rank27]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank27]:     return self._call_impl(*args, **kwargs)
[rank27]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank27]:     return forward_call(*args, **kwargs)
[rank27]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank27]:     ret_val = func(*args, **kwargs)
[rank27]:               ^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank27]:     loss = self.module(*inputs, **kwargs)
[rank27]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank27]:     return self._call_impl(*args, **kwargs)
[rank27]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank27]:     return forward_call(*args, **kwargs)
[rank27]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank27]:     weather = self.vt_embed(weather)
[rank27]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank27]:     return self._call_impl(*args, **kwargs)
[rank27]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank27]:     return forward_call(*args, **kwargs)
[rank27]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank27]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank27]:                     ^^^^^^^
[rank27]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
[rank12]: Traceback (most recent call last):
[rank12]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank12]:     )
[rank12]:      
[rank12]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank12]:     out = model_engine(x, weather)
[rank12]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank12]:     return self._call_impl(*args, **kwargs)
[rank12]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank12]:     return forward_call(*args, **kwargs)
[rank12]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank12]:     ret_val = func(*args, **kwargs)
[rank12]:               ^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank12]:     loss = self.module(*inputs, **kwargs)
[rank12]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank12]:     return self._call_impl(*args, **kwargs)
[rank12]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank12]:     return forward_call(*args, **kwargs)
[rank12]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank12]:     weather = self.vt_embed(weather)
[rank12]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank12]:     return self._call_impl(*args, **kwargs)
[rank12]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank12]:     return forward_call(*args, **kwargs)
[rank12]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank12]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank12]:                     ^^^^^^^
[rank12]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
Initialized deepspeed on global rank 17, local rank 1 with world size 28.
[rank17]: Traceback (most recent call last):
[rank17]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank17]:     )
[rank17]:      
[rank17]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank17]:     out = model_engine(x, weather)
[rank17]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank17]:     return self._call_impl(*args, **kwargs)
[rank17]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank17]:     return forward_call(*args, **kwargs)
[rank17]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank17]:     ret_val = func(*args, **kwargs)
[rank17]:               ^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank17]:     loss = self.module(*inputs, **kwargs)
[rank17]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank17]:     return self._call_impl(*args, **kwargs)
[rank17]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank17]:     return forward_call(*args, **kwargs)
[rank17]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank17]:     weather = self.vt_embed(weather)
[rank17]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank17]:     return self._call_impl(*args, **kwargs)
[rank17]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank17]:     return forward_call(*args, **kwargs)
[rank17]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank17]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank17]:                     ^^^^^^^
[rank17]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
Evaluating for lr=5e-05:   0%|          | 0/9 [00:01<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank0]:     )
[rank0]:      
[rank0]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank0]:     out = model_engine(x, weather)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank0]:     weather = self.vt_embed(weather)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank0]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank0]:                     ^^^^^^^
[rank0]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
Initialized deepspeed on global rank 18, local rank 2 with world size 28.
[rank18]: Traceback (most recent call last):
[rank18]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank18]:     )
[rank18]:      
[rank18]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank18]:     out = model_engine(x, weather)
[rank18]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank18]:     return self._call_impl(*args, **kwargs)
[rank18]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank18]:     return forward_call(*args, **kwargs)
[rank18]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank18]:     ret_val = func(*args, **kwargs)
[rank18]:               ^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank18]:     loss = self.module(*inputs, **kwargs)
[rank18]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank18]:     return self._call_impl(*args, **kwargs)
[rank18]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank18]:     return forward_call(*args, **kwargs)
[rank18]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank18]:     weather = self.vt_embed(weather)
[rank18]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank18]:     return self._call_impl(*args, **kwargs)
[rank18]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank18]:     return forward_call(*args, **kwargs)
[rank18]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank18]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank18]:                     ^^^^^^^
[rank18]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
Initialized deepspeed on global rank 20, local rank 0 with world size 28.
Initialized deepspeed on global rank 8, local rank 0 with world size 28.
[rank8]: Traceback (most recent call last):
[rank8]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank8]:     )
[rank8]:      
[rank8]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank8]:     out = model_engine(x, weather)
[rank8]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank8]:     return self._call_impl(*args, **kwargs)
[rank8]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank8]:     return forward_call(*args, **kwargs)
[rank8]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank8]:     ret_val = func(*args, **kwargs)
[rank8]:               ^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank8]:     loss = self.module(*inputs, **kwargs)
[rank8]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank8]:     return self._call_impl(*args, **kwargs)
[rank8]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank8]:     return forward_call(*args, **kwargs)
[rank8]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank8]:     weather = self.vt_embed(weather)
[rank8]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank8]:     return self._call_impl(*args, **kwargs)
[rank8]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank8]:     return forward_call(*args, **kwargs)
[rank8]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank8]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank8]:                     ^^^^^^^
[rank8]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
[rank20]: Traceback (most recent call last):
[rank20]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank20]:     )
[rank20]:      
[rank20]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank20]:     out = model_engine(x, weather)
[rank20]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank20]:     return self._call_impl(*args, **kwargs)
[rank20]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank20]:     return forward_call(*args, **kwargs)
[rank20]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank20]:     ret_val = func(*args, **kwargs)
[rank20]:               ^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank20]:     loss = self.module(*inputs, **kwargs)
[rank20]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank20]:     return self._call_impl(*args, **kwargs)
[rank20]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank20]:     return forward_call(*args, **kwargs)
[rank20]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank20]:     weather = self.vt_embed(weather)
[rank20]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank20]:     return self._call_impl(*args, **kwargs)
[rank20]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank20]:     return forward_call(*args, **kwargs)
[rank20]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank20]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank20]:                     ^^^^^^^
[rank20]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
Initialized deepspeed on global rank 4, local rank 0 with world size 28.
[rank4]: Traceback (most recent call last):
[rank4]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank4]:     )
[rank4]:      
[rank4]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank4]:     out = model_engine(x, weather)
[rank4]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank4]:     ret_val = func(*args, **kwargs)
[rank4]:               ^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank4]:     loss = self.module(*inputs, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank4]:     weather = self.vt_embed(weather)
[rank4]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank4]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank4]:                     ^^^^^^^
[rank4]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
Initialized deepspeed on global rank 16, local rank 0 with world size 28.
Initialized deepspeed on global rank 24, local rank 0 with world size 28.
[rank16]: Traceback (most recent call last):
[rank16]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank16]:     )
[rank16]:      
[rank16]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank16]:     out = model_engine(x, weather)
[rank16]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank16]:     return self._call_impl(*args, **kwargs)
[rank16]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank16]:     return forward_call(*args, **kwargs)
[rank16]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank16]:     ret_val = func(*args, **kwargs)
[rank16]:               ^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank16]:     loss = self.module(*inputs, **kwargs)
[rank16]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank16]:     return self._call_impl(*args, **kwargs)
[rank16]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank16]:     return forward_call(*args, **kwargs)
[rank16]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank16]:     weather = self.vt_embed(weather)
[rank16]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank16]:     return self._call_impl(*args, **kwargs)
[rank16]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank16]:     return forward_call(*args, **kwargs)
[rank16]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank16]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank16]:                     ^^^^^^^
[rank16]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
[rank24]: Traceback (most recent call last):
[rank24]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 267, in <module>
[rank24]:     )
[rank24]:      
[rank24]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 137, in evaluate
[rank24]:     out = model_engine(x, weather)
[rank24]:           ^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank24]:     return self._call_impl(*args, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank24]:     return forward_call(*args, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank24]:     ret_val = func(*args, **kwargs)
[rank24]:               ^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank24]:     loss = self.module(*inputs, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank24]:     return self._call_impl(*args, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank24]:     return forward_call(*args, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1033, in forward
[rank24]:     weather = self.vt_embed(weather)
[rank24]:               ^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank24]:     return self._call_impl(*args, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank24]:     return forward_call(*args, **kwargs)
[rank24]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 894, in forward
[rank24]:     B, T, C, H, W = x.shape                 # (1,152,5,128,256)
[rank24]:                     ^^^^^^^
[rank24]: AttributeError: 'builtin_function_or_method' object has no attribute 'shape'
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 12 exited with code 1
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 16 exited with code 1
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 8 exited with code 1
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 24 exited with code 1
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 20 exited with code 1
x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 4 died from signal 15
x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 0 died from signal 15
x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 6 died from signal 15
x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 7 died from signal 15
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 14 exited with code 1
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 17 exited with code 1
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 18 exited with code 1
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 15 exited with code 1
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 13 exited with code 1
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 10 exited with code 1
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 9 exited with code 1
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 26 exited with code 1
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 27 exited with code 1
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 19 exited with code 1
x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 1 died from signal 15
x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 2 died from signal 15
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 22 exited with code 1
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 23 exited with code 1
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 25 exited with code 1
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 11 exited with code 1
x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 3 died from signal 15
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 21 exited with code 1
x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 5 died from signal 15
Application 9239d3c7 resources: utime=18654s stime=5858s maxrss=35752664KB inblock=348252826 oublock=536 minflt=250410250 majflt=46079 nvcsw=42076525 nivcsw=10653069
Training completed
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled

Lmod is automatically replacing "nvhpc/23.9" with "gcc-native/12.3".


Lmod is automatically replacing "PrgEnv-nvhpc/8.5.0" with "PrgEnv-gnu/8.5.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.28

declare -x APP2_STATE="23.12.0"
declare -x BASH_ENV="/usr/share/lmod/lmod/init/bash"
declare -x C3_RSH="ssh -oConnectTimeout=10 -oForwardX11=no"
declare -x CFLAGS="-I/soft/applications/conda/2024-04-29/mconda3/include"
declare -x COLORTERM="1"
declare -x COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin"
declare -x CONDA_DEFAULT_ENV="base"
declare -x CONDA_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/conda"
declare -x CONDA_PREFIX="/soft/applications/conda/2024-04-29/mconda3"
declare -x CONDA_PROMPT_MODIFIER="(2024-04-29/base) "
declare -x CONDA_PYTHON_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/python"
declare -x CONDA_SHLVL="1"
declare -x CPU="x86_64"
declare -x CRAYPAT_LD_LIBRARY_PATH="/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAYPAT_OPTS_EXECUTABLE="libexec64/opts"
declare -x CRAYPAT_ROOT="/opt/cray/pe/perftools/23.12.0"
declare -x CRAYPE_DIR="/opt/cray/pe/craype/2.7.30"
declare -x CRAYPE_NETWORK_TARGET="ofi"
declare -x CRAYPE_VERSION="2.7.30"
declare -x CRAY_CPU_TARGET="x86-milan"
declare -x CRAY_DSMML_BASEDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_DIR="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_PREFIX="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_ROOTDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_VER="0.2.2"
declare -x CRAY_DSMML_VERSION="0.2.2"
declare -x CRAY_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x CRAY_HDF5_PARALLEL_PREFIX="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x CRAY_HDF5_PARALLEL_VERSION="1.12.2.9"
declare -x CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:/opt/cray/pe/pmi/6.1.13/lib:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:/opt/cray/pe/mpich/8.1.28/gtl/lib:/opt/cray/pe/dsmml/0.2.2/dsmml/lib:/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAY_LMOD_COMPILER="gnu/12.0"
declare -x CRAY_LMOD_CPU="x86-milan/1.0"
declare -x CRAY_LMOD_MPI="cray-mpich/8.0"
declare -x CRAY_LMOD_NET="ofi/1.0"
declare -x CRAY_MPICH_BASEDIR="/opt/cray/pe/mpich/8.1.28/ofi"
declare -x CRAY_MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_PREFIX="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_ROOTDIR="/opt/cray/pe/mpich/8.1.28"
declare -x CRAY_MPICH_VER="8.1.28"
declare -x CRAY_MPICH_VERSION="8.1.28"
declare -x CRAY_PERFTOOLS_PREFIX="/opt/cray/pe/perftools/23.12.0"
declare -x CRAY_PERFTOOLS_VERSION="23.12.0"
declare -x CRAY_PMI_INCLUDE_OPTS="-I/opt/cray/pe/pmi/6.1.13/include"
declare -x CRAY_PMI_POST_LINK_OPTS="-L/opt/cray/pe/pmi/6.1.13/lib"
declare -x CRAY_PMI_PREFIX="/opt/cray/pe/pmi/6.1.13"
declare -x CRAY_PMI_VERSION="6.1.13"
declare -x CSHEDIT="emacs"
declare -x CUDA_HOME="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_TOOLKIT_BASE="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDNN_HOME="/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/"
declare -x ENVIRONMENT="BATCH"
declare -x ENV_NAME="conda/2024-04-29"
declare -x FROM_HEADER=""
declare -x GCC_PATH="/usr/bin"
declare -x GCC_PREFIX="/usr/lib64/gcc/x86_64-suse-linux/12"
declare -x GCC_VERSION="12.3"
declare -x GNU_VERSION="12.3"
declare -x GPG_TTY="not a tty"
declare -x GSETTINGS_SCHEMA_DIR="/soft/applications/conda/2024-04-29/mconda3/share/glib-2.0/schemas"
declare -x GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=""
declare -x G_BROKEN_FILENAMES="1"
declare -x G_FILENAME_ENCODING="@locale,UTF-8,ISO-8859-15,CP1252"
declare -x HDF5_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HDF5_ROOT="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HISTSIZE="1000"
declare -x HOME="/home/shourya01"
declare -x HOST="x3003c0s37b0n0"
declare -x HOSTNAME="x3003c0s37b0n0"
declare -x HOSTTYPE="x86_64"
declare -x HTTPS_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x HTTP_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x LANG="en_US.UTF-8"
declare -x LANGUAGE="en_US.UTF-8"
declare -x LDFLAGS="-L/soft/applications/conda/2024-04-29/mconda3/lib -Wl,--enable-new-dtags,-rpath,/soft/applications/conda/2024-04-29/mconda3/lib"
declare -x LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:/soft/perftools/darshan/darshan-3.4.4/lib:/opt/cray/pe/papi/7.0.1.2/lib64:/opt/cray/libfabric/1.15.2.0/lib64"
declare -x LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so"
declare -x LESS="-M -I -R"
declare -x LESSCLOSE="lessclose.sh %s %s"
declare -x LESSKEY="/etc/lesskey.bin"
declare -x LESSOPEN="lessopen.sh %s"
declare -x LESS_ADVANCED_PREPROCESSOR="no"
declare -x LMOD_CMD="/usr/share/lmod/lmod/libexec/lmod"
declare -x LMOD_DIR="/usr/share/lmod/lmod/libexec"
declare -x LMOD_FAMILY_COMPILER="gcc-native"
declare -x LMOD_FAMILY_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_CRAYPE="craype"
declare -x LMOD_FAMILY_CRAYPE_CPU="craype-x86-milan"
declare -x LMOD_FAMILY_CRAYPE_CPU_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_NETWORK="craype-network-ofi"
declare -x LMOD_FAMILY_CRAYPE_NETWORK_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_VERSION="2.7.30"
declare -x LMOD_FAMILY_GCC_COMPILER="gcc-native"
declare -x LMOD_FAMILY_GCC_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_HDF5="cray-hdf5-parallel"
declare -x LMOD_FAMILY_HDF5_VERSION="1.12.2.9"
declare -x LMOD_FAMILY_MPI="cray-mpich"
declare -x LMOD_FAMILY_MPI_VERSION="8.1.28"
declare -x LMOD_FAMILY_PRGENV="PrgEnv-gnu"
declare -x LMOD_FAMILY_PRGENV_VERSION="8.5.0"
declare -x LMOD_FAMILY_PYTHON="conda"
declare -x LMOD_FAMILY_PYTHON_VERSION="2024-04-29"
declare -x LMOD_PKG="/usr/share/lmod/lmod"
declare -x LMOD_ROOT="/usr/share/lmod"
declare -x LMOD_SETTARG_FULL_SUPPORT="no"
declare -x LMOD_SYSTEM_DEFAULT_MODULES="PrgEnv-nvhpc:craype-network-ofi:perftools-base:darshan:xalt"
declare -x LMOD_VERSION="8.7.34"
declare -x LMOD_sys="Linux"
declare -x LOADEDMODULES="libfabric/1.15.2.0:craype-network-ofi:perftools-base/23.12.0:darshan/3.4.4:xalt/3.0.2-202408282050:gcc-native/12.3:craype/2.7.30:cray-dsmml/0.2.2:cray-mpich/8.1.28:cray-pmi/6.1.13:cray-pals/1.3.4:cray-libpals/1.3.4:craype-x86-milan:PrgEnv-gnu/8.5.0:cray-hdf5-parallel/1.12.2.9:cudnn/9.1.0:conda/2024-04-29"
declare -x LOGNAME="shourya01"
declare -x MACHTYPE="x86_64-suse-linux"
declare -x MAIL="/var/spool/mail/shourya01"
declare -x MANPATH="/opt/cray/pals/1.3.4/man:/opt/cray/pe/pmi/6.1.13/man:/opt/cray/pe/mpich/8.1.28/ofi/man:/opt/cray/pe/mpich/8.1.28/man/mpich:/opt/cray/pe/dsmml/0.2.2/dsmml/man:/opt/cray/pe/craype/2.7.30/man:/opt/cray/pe/perftools/23.12.0/man:/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:/opt/cray/libfabric/1.15.2.0/share/man:/usr/share/lmod/lmod/share/man:/home/shourya01/.local/man:/usr/local/man:/usr/share/man:/usr/man:/opt/c3/man:/opt/pbs/share/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man"
declare -x MINICOM="-c on"
declare -x MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:/opt/cray/pe/lmod/modulefiles/mix_compilers:/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:/soft/modulefiles:/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:/usr/share/modulefiles/Linux:/usr/share/modulefiles/Core:/usr/share/lmod/lmod/modulefiles/Core:/usr/share/lmod/lmod/modulefiles:/opt/cray/pals/lmod/modulefiles/core:/opt/cray/modulefiles:/opt/cray/pe/lmod/modulefiles/core:/opt/cray/pe/lmod/modulefiles/craype-targets/default:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:/soft/xalt/modulefiles"
declare -x MODULEPATH_ROOT="/usr/share/modulefiles"
declare -x MODULESHOME="/usr/share/lmod/lmod"
declare -x MORE="-sl"
declare -x MPI4JAX_USE_CUDA_MPI="1"
declare -x MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x MPICH_GPU_SUPPORT_ENABLED="1"
declare -x NCCL_IB_DISABLE="1"
declare -x NCCL_SOCKET_IFNAME="hsn"
declare -x NCPUS="64"
declare -x OFFLOAD_INIT="on_start"
declare -x OLDPWD
declare -x OMP_NUM_THREADS="4"
declare -x OSCAR_HOME="/opt/oscar"
declare -x OSTYPE="linux"
declare -x PAGER="less"
declare -x PALS_TRANSFER="0"
declare -x PATH="/soft/applications/conda/2024-04-29/mconda3/bin:/soft/applications/conda/2024-04-29/mconda3/condabin:/soft/xalt/3.0.2-202408282050/bin:/soft/compilers/cudatoolkit/cuda-12.4.1/bin:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:/opt/cray/pe/hdf5/1.12.2.9/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/home/shourya01/.local/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/shourya01/bin:/opt/cray/pe/bin"
declare -x PAT_RT_PERFCTR_DISABLE_COMPONENTS="nvml,rocm_smi"
declare -x PBS_ACCOUNT="ParaLLMs"
declare -x PBS_ENVIRONMENT="PBS_BATCH"
declare -x PBS_JOBCOOKIE="616FCC98141241B67B87DFE32CD28260"
declare -x PBS_JOBDIR="/home/shourya01"
declare -x PBS_JOBID="5136209.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_JOBNAME="bash"
declare -x PBS_MOMPORT="15003"
declare -x PBS_NODEFILE="/var/spool/pbs/aux/5136209.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_NODENUM="0"
declare -x PBS_O_HOME="/home/shourya01"
declare -x PBS_O_HOST="polaris-login-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_O_INTERACTIVE_AUTH_METHOD="resvport"
declare -x PBS_O_LANG="en_US.UTF-8"
declare -x PBS_O_LOGNAME="shourya01"
declare -x PBS_O_MAIL="/var/spool/mail/shourya01"
declare -x PBS_O_PATH="/home/shourya01/.local/bin:/home/shourya01/.vscode-server/cli/servers/Stable-91fa95bccb027ece6a968589bb1d662fa9c8e170/server/bin/remote-cli:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/soft/xalt/3.0.2-202408282050/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/nvidia/23.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/extras/qd/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/shourya01/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/dbhome/db2cat/sqllib/bin:/dbhome/db2cat/sqllib/adm:/dbhome/db2cat/sqllib/misc:/dbhome/db2cat/sqllib/gskit/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/opt/cray/pe/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.vscode-server/extensions/ms-python.debugpy-2025.8.0/bundled/scripts/noConfigScripts"
declare -x PBS_O_QUEUE="debug-scaling"
declare -x PBS_O_SHELL="/bin/bash"
declare -x PBS_O_SYSTEM="Linux"
declare -x PBS_O_WORKDIR="/home/shourya01"
declare -x PBS_QUEUE="debug-scaling"
declare -x PBS_TASKNUM="1"
declare -x PELOCAL_PRGENV="true"
declare -x PERFTOOLS_VERSION="23.12.0"
declare -x PE_DSMML_MODULE_NAME="cray-dsmml"
declare -x PE_DSMML_PKGCONFIG_LIBS="dsmml"
declare -x PE_ENV="GNU"
declare -x PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel:mpichf90"
declare -x PE_GCC_EXTERNAL="native"
declare -x PE_GCC_LEVEL="12"
declare -x PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig"
declare -x PE_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x PE_HDF5_PARALLEL_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel"
declare -x PE_HDF5_PARALLEL_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel"
declare -x PE_MPICH_FIXED_PRGENV="GNU"
declare -x PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90"
declare -x PE_MPICH_GENCOMPILERS_GNU="12.3"
declare -x PE_MPICH_GTL_DIR_amd_gfx906="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx908="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx90a="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx940="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx942="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia70="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia80="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia90="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_ponteVecchio="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_LIBS_amd_gfx906="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx908="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx90a="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx940="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx942="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_nvidia70="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia80="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia90="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_ponteVecchio="-lmpi_gtl_ze"
declare -x PE_MPICH_MODULE_NAME="cray-mpich"
declare -x PE_MPICH_PKGCONFIG_LIBS="mpich"
declare -x PE_MPICH_PKGCONFIG_VARIABLES="PE_MPICH_GTL_DIR_@accelerator@:PE_MPICH_GTL_LIBS_@accelerator@"
declare -x PE_PALS_PKGCONFIG_LIBS="libpals"
declare -x PE_PERFTOOLS_MPICH_LIBDIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib"
declare -x PE_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel:mpich:dsmml:darshan-runtime"
declare -x PE_PKGCONFIG_PRODUCTS="PE_PALS:PE_PMI:PE_MPICH:PE_DSMML"
declare -x PE_PMI_PKGCONFIG_LIBS="cray-pmi"
declare -x PE_PRODUCT_LIST="CRAYPE_X86_MILAN"
declare -x PKGCONFIG_ENABLED="1"
declare -x PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:/opt/cray/pals/1.3.4/lib/pkgconfig:/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:/opt/cray/pe/craype/2.7.30/pkg-config:/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig"
declare -x PROFILEREAD="true"
declare -x PWD="/home/shourya01"
declare -x PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages"
declare -x PYTHONUSERBASE="/home/shourya01/.local/polaris/conda/2024-04-29"
declare -x QT_SYSTEM_DIR="/usr/share/desktop-data"
declare -x SHELL="/bin/bash"
declare -x SHLVL="2"
declare -x SLURM_MPI_TYPE="cray_shasta"
declare -x STARSHIP_SESSION_KEY="1161016614183573"
declare -x STARSHIP_SHELL="bash"
declare -x TMPDIR="/var/tmp/pbs.5136209.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x TRITON_DISABLE_AUTOTUNE="1"
declare -x TZ="Etc/UTC"
declare -x USER="shourya01"
declare -x USE_PCM_DB="2"
declare -x WINDOWMANAGER="xterm"
declare -x XALT_DIR="/soft/xalt/3.0.2-202408282050"
declare -x XALT_EXECUTABLE_TRACKING="yes"
declare -x XALT_SAMPLING="no"
declare -x XALT_SCALAR_AND_SPSR_SAMPLING="yes"
declare -x XCURSOR_THEME="DMZ"
declare -x XDG_CONFIG_DIRS="/etc/xdg"
declare -x XDG_DATA_DIRS="/usr/share"
declare -x XKEYSYMDB="/usr/X11R6/lib/X11/XKeysymDB"
declare -x XLA_FLAGS="--xla_gpu_force_compilation_parallelism=1 --xla_gpu_cuda_data_dir=/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x XLA_PYTHON_CLIENT_PREALLOCATE="false"
declare -x XML_CATALOG_FILES="file:///soft/applications/conda/2024-04-29/mconda3/etc/xml/catalog file:///etc/xml/catalog"
declare -x XNLSPATH="/usr/X11R6/lib/X11/nls"
declare -x _CE_CONDA=""
declare -x _CE_M=""
declare -x _LMFILES_="/opt/cray/modulefiles/libfabric/1.15.2.0:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-network-ofi.lua:/opt/cray/pe/lmod/modulefiles/core/perftools-base/23.12.0.lua:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles/darshan/3.4.4:/soft/xalt/modulefiles/xalt/3.0.2-202408282050:/opt/cray/pe/lmod/modulefiles/core/gcc-native/12.3.lua:/opt/cray/pe/lmod/modulefiles/core/craype/2.7.30.lua:/opt/cray/pe/lmod/modulefiles/core/cray-dsmml/0.2.2.lua:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0/cray-mpich/8.1.28.lua:/opt/cray/pe/lmod/modulefiles/core/cray-pmi/6.1.13.lua:/opt/cray/pals/lmod/modulefiles/core/cray-pals/1.3.4.lua:/opt/cray/pals/lmod/modulefiles/core/cray-libpals/1.3.4.lua:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-x86-milan.lua:/opt/cray/pe/lmod/modulefiles/core/PrgEnv-gnu/8.5.0.lua:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2.9.lua:/soft/modulefiles/cudnn/9.1.0.lua:/soft/modulefiles/conda/2024-04-29.lua"
declare -x _ModuleTable001_="X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpQcmdFbnYgPSAiUHJnRW52LWdudSIsCmNvbXBpbGVyID0gImdjYy1uYXRpdmUiLApjcmF5cGUgPSAiY3JheXBlIiwKY3JheXBlX2NwdSA9ICJjcmF5cGUteDg2LW1pbGFuIiwKY3JheXBlX25ldHdvcmsgPSAiY3JheXBlLW5ldHdvcmstb2ZpIiwKZ2NjX2NvbXBpbGVyID0gImdjYy1uYXRpdmUiLApoZGY1ID0gImNyYXktaGRmNS1wYXJhbGxlbCIsCm1waSA9ICJjcmF5LW1waWNoIiwKcHl0aG9uID0gImNvbmRhIiwKfSwKbVQgPSB7ClsiUHJnRW52LWdudSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUv"
declare -x _ModuleTable002_="bG1vZC9tb2R1bGVmaWxlcy9jb3JlL1ByZ0Vudi1nbnUvOC41LjAubHVhIiwKZnVsbE5hbWUgPSAiUHJnRW52LWdudS84LjUuMCIsCmxvYWRPcmRlciA9IDE0LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gIlByZ0Vudi1nbnUiLAp3ViA9ICJeMDAwMDAwMDguMDAwMDAwMDA1Lip6ZmluYWwiLAp9LApjb25kYSA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY29uZGEvMjAyNC0wNC0yOS5sdWEiLApmdWxsTmFtZSA9ICJjb25kYS8yMDI0LTA0LTI5IiwKbG9hZE9yZGVyID0gMTcsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY29uZGEiLAp3ViA9ICJeMDAw"
declare -x _ModuleTable003_="MDIwMjQuKnpmaW5hbC0uMDAwMDAwMDA0Lip6ZmluYWwtLjAwMDAwMDAyOS4qemZpbmFsIiwKfSwKWyJjcmF5LWRzbW1sIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1kc21tbC8wLjIuMi5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWRzbW1sLzAuMi4yIiwKbG9hZE9yZGVyID0gOCwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWRzbW1sIiwKd1YgPSAiXjAwMDAwMDAwLjAwMDAwMDAwMi4wMDAwMDAwMDIuKnpmaW5hbCIsCn0sClsiY3JheS1oZGY1LXBhcmFsbGVsIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9v"
declare -x _ModuleTable004_="ZmkvMS4wL2NyYXktbXBpY2gvOC4wL2NyYXktaGRmNS1wYXJhbGxlbC8xLjEyLjIuOS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLApsb2FkT3JkZXIgPSAxNSwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDEyLjAwMDAwMDAwMi4wMDAwMDAwMDkuKnpmaW5hbCIsCn0sClsiY3JheS1saWJwYWxzIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wYWxzL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5LWxpYnBhbHMvMS4zLjQubHVhIiwKZnVsbE5hbWUgPSAiY3JheS1s"
declare -x _ModuleTable005_="aWJwYWxzLzEuMy40IiwKbG9hZE9yZGVyID0gMTIsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1saWJwYWxzIiwKd1YgPSAiXjAwMDAwMDAxLjAwMDAwMDAwMy4wMDAwMDAwMDQuKnpmaW5hbCIsCn0sClsiY3JheS1tcGljaCJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMC9jcmF5LW1waWNoLzguMS4yOC5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LW1waWNoLzguMS4yOCIsCmxvYWRPcmRlciA9IDksCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1tcGljaCIsCndWID0gIl4w"
declare -x _ModuleTable006_="MDAwMDAwOC4wMDAwMDAwMDEuMDAwMDAwMDI4Lip6ZmluYWwiLAp9LApbImNyYXktcGFscyJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGFscy9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wYWxzLzEuMy40Lmx1YSIsCmZ1bGxOYW1lID0gImNyYXktcGFscy8xLjMuNCIsCmxvYWRPcmRlciA9IDExLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcGFscyIsCndWID0gIl4wMDAwMDAwMS4wMDAwMDAwMDMuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImNyYXktcG1pIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wbWkvNi4xLjEzLmx1YSIsCmZ1bGxOYW1lID0gImNy"
declare -x _ModuleTable007_="YXktcG1pLzYuMS4xMyIsCmxvYWRPcmRlciA9IDEwLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcG1pIiwKd1YgPSAiXjAwMDAwMDA2LjAwMDAwMDAwMS4wMDAwMDAwMTMuKnpmaW5hbCIsCn0sCmNyYXlwZSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5cGUvMi43LjMwLmx1YSIsCmZ1bGxOYW1lID0gImNyYXlwZS8yLjcuMzAiLApsb2FkT3JkZXIgPSA3LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZSIsCndWID0gIl4wMDAwMDAwMi4wMDAwMDAwMDcuMDAwMDAwMDMwLip6ZmluYWwiLAp9LApb"
declare -x _ModuleTable008_="ImNyYXlwZS1uZXR3b3JrLW9maSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS1uZXR3b3JrLW9maS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5cGUtbmV0d29yay1vZmkiLApsb2FkT3JkZXIgPSAyLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS1uZXR3b3JrLW9maSIsCndWID0gIk0uKnpmaW5hbCIsCn0sClsiY3JheXBlLXg4Ni1taWxhbiJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS14ODYtbWlsYW4ubHVhIiwKZnVsbE5hbWUgPSAiY3JheXBl"
declare -x _ModuleTable009_="LXg4Ni1taWxhbiIsCmxvYWRPcmRlciA9IDEzLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS14ODYtbWlsYW4iLAp3ViA9ICJNLip6ZmluYWwiLAp9LApjdWRubiA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY3Vkbm4vOS4xLjAubHVhIiwKZnVsbE5hbWUgPSAiY3Vkbm4vOS4xLjAiLApsb2FkT3JkZXIgPSAxNiwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjdWRubi85LjEuMCIsCndWID0gIjAwMDAwMDAwOS4wMDAwMDAwMDEuKnpmaW5hbCIsCn0sCmRhcnNoYW4gPSB7CmZuID0gIi9zb2Z0L3BlcmZ0b29scy9k"
declare -x _ModuleTable010_="YXJzaGFuL2RhcnNoYW4tMy40LjQvc2hhcmUvY3JheXBlLTIueC9tb2R1bGVmaWxlcy9kYXJzaGFuLzMuNC40IiwKZnVsbE5hbWUgPSAiZGFyc2hhbi8zLjQuNCIsCmxvYWRPcmRlciA9IDQsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiZGFyc2hhbiIsCndWID0gIjAwMDAwMDAwMy4wMDAwMDAwMDQuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImdjYy1uYXRpdmUiXSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9nY2MtbmF0aXZlLzEyLjMubHVhIiwKZnVsbE5hbWUgPSAiZ2NjLW5hdGl2ZS8xMi4zIiwKbG9hZE9yZGVyID0gNiwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0"
declare -x _ModuleTable011_="YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJnY2MtbmF0aXZlIiwKd1YgPSAiXjAwMDAwMDEyLjAwMDAwMDAwMy4qemZpbmFsIiwKfSwKbGliZmFicmljID0gewpmbiA9ICIvb3B0L2NyYXkvbW9kdWxlZmlsZXMvbGliZmFicmljLzEuMTUuMi4wIiwKZnVsbE5hbWUgPSAibGliZmFicmljLzEuMTUuMi4wIiwKbG9hZE9yZGVyID0gMSwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJsaWJmYWJyaWMiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDE1LjAwMDAwMDAwMi4qemZpbmFsIiwKfSwKWyJwZXJmdG9vbHMtYmFzZSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb3JlL3BlcmZ0b29s"
declare -x _ModuleTable012_="cy1iYXNlLzIzLjEyLjAubHVhIiwKZnVsbE5hbWUgPSAicGVyZnRvb2xzLWJhc2UvMjMuMTIuMCIsCmxvYWRPcmRlciA9IDMsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAicGVyZnRvb2xzLWJhc2UiLAp3ViA9ICJeMDAwMDAwMjMuMDAwMDAwMDEyLip6ZmluYWwiLAp9LAp4YWx0ID0gewpmbiA9ICIvc29mdC94YWx0L21vZHVsZWZpbGVzL3hhbHQvMy4wLjItMjAyNDA4MjgyMDUwIiwKZnVsbE5hbWUgPSAieGFsdC8zLjAuMi0yMDI0MDgyODIwNTAiLApsb2FkT3JkZXIgPSA1LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gInhhbHQiLAp3ViA9ICJeMDAwMDAw"
declare -x _ModuleTable013_="MDMuMDAwMDAwMDAwLjAwMDAwMDAwMi4qemZpbmFsLS4yMDI0MDgyODIwNTAuKnpmaW5hbCIsCn0sCn0sCm1wYXRoQSA9IHsKCiIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9oZGY1LXBhcmFsbGVsL2dudS8xMi4wL29maS8xLjAvY3JheS1tcGljaC84LjAvY3JheS1oZGY1LXBhcmFsbGVsLzEuMTIuMiIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3B1L3g4Ni1taWxhbi8xLjAiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9vZmkvMS4wL2NyYXktbXBpY2gvOC4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMCIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxl"
declare -x _ModuleTable014_="ZmlsZXMvbWl4X2NvbXBpbGVycyIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29tcGlsZXIvZ251LzEyLjAiLCAiL3NvZnQvbW9kdWxlZmlsZXMiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL3BlcmZ0b29scy8yMy4xMi4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9uZXQvb2ZpLzEuMCIsICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0xpbnV4IgosICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmUiLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZSIKLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BhbHMvbG1vZC9tb2R1bGVmaWxlcy9jb3JlIgosICIvb3B0L2Ny"
declare -x _ModuleTable015_="YXkvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZSIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdCIKLCAiL3NvZnQvcGVyZnRvb2xzL2RhcnNoYW4vZGFyc2hhbi0zLjQuNC9zaGFyZS9jcmF5cGUtMi54L21vZHVsZWZpbGVzIiwgIi9zb2Z0L3hhbHQvbW9kdWxlZmlsZXMiLAp9LApzeXN0ZW1CYXNlTVBBVEggPSAiL3Vzci9zaGFyZS9tb2R1bGVmaWxlcy9MaW51eDovdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmU6L3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZTovdXNyL3NoYXJlL2xtb2QvbG1vZC9tb2R1bGVmaWxlczovb3B0L2NyYXkvcGFscy9sbW9kL21vZHVs"
declare -x _ModuleTable016_="ZWZpbGVzL2NvcmU6L29wdC9jcmF5L21vZHVsZWZpbGVzOi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmU6L29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdDovc29mdC9wZXJmdG9vbHMvZGFyc2hhbi9kYXJzaGFuLTMuNC40L3NoYXJlL2NyYXlwZS0yLngvbW9kdWxlZmlsZXM6L3NvZnQveGFsdC9tb2R1bGVmaWxlcyIsCn0K"
declare -x _ModuleTable_Sz_="16"
declare -x __LMOD_Priority_PATH="/soft/xalt/3.0.2-202408282050/bin:-100"
declare -x __LMOD_REF_COUNT_COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin:1"
declare -x __LMOD_REF_COUNT_CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:1;/opt/cray/pe/pmi/6.1.13/lib:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:1;/opt/cray/pe/mpich/8.1.28/gtl/lib:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib:1;/opt/cray/pe/perftools/23.12.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:1;/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:1;/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:1;/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:1;/soft/perftools/darshan/darshan-3.4.4/lib:1;/opt/cray/pe/papi/7.0.1.2/lib64:1;/opt/cray/libfabric/1.15.2.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so:1"
declare -x __LMOD_REF_COUNT_MANPATH="/opt/cray/pals/1.3.4/man:2;/opt/cray/pe/pmi/6.1.13/man:1;/opt/cray/pe/mpich/8.1.28/ofi/man:1;/opt/cray/pe/mpich/8.1.28/man/mpich:1;/opt/cray/pe/dsmml/0.2.2/dsmml/man:1;/opt/cray/pe/craype/2.7.30/man:1;/opt/cray/pe/perftools/23.12.0/man:1;/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:1;/opt/cray/libfabric/1.15.2.0/share/man:1;/usr/share/lmod/lmod/share/man:1;/home/shourya01/.local/man:1;/usr/local/man:1;/usr/share/man:1;/usr/man:1;/opt/c3/man:1;/opt/pbs/share/man:1;/opt/clmgr/man:1;/opt/sgi/share/man:1;/opt/clmgr/share/man:1;/opt/clmgr/lib/cm-cli/man:1"
declare -x __LMOD_REF_COUNT_MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:1;/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:1;/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:1;/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:1;/opt/cray/pe/lmod/modulefiles/mix_compilers:1;/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:1;/soft/modulefiles:1;/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:1;/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:1;/usr/share/modulefiles/Linux:1;/usr/share/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles:1;/opt/cray/pals/lmod/modulefiles/core:1;/opt/cray/modulefiles:1;/opt/cray/pe/lmod/modulefiles/core:1;/opt/cray/pe/lmod/modulefiles/craype-targets/default:1;/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:1;/soft/xalt/modulefiles:1"
declare -x __LMOD_REF_COUNT_PATH="/soft/xalt/3.0.2-202408282050/bin:1;/soft/compilers/cudatoolkit/cuda-12.4.1/bin:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:1;/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:1;/opt/cray/pe/hdf5/1.12.2.9/bin:1;/opt/cray/pals/1.3.4/bin:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:1;/opt/cray/pe/mpich/8.1.28/bin:1;/opt/cray/pe/craype/2.7.30/bin:1;/home/shourya01/.local/bin:4;/soft/perftools/darshan/darshan-3.4.4/bin:1;/opt/cray/pe/perftools/23.12.0/bin:1;/opt/cray/pe/papi/7.0.1.2/bin:1;/opt/cray/libfabric/1.15.2.0/bin:1;/opt/clmgr/sbin:1;/opt/clmgr/bin:1;/opt/sgi/sbin:1;/opt/sgi/bin:1;/usr/local/bin:1;/usr/bin:1;/bin:2;/opt/c3/bin:1;/usr/lib/mit/bin:1;/usr/lib/mit/sbin:1;/opt/pbs/bin:1;/sbin:1;/home/shourya01/bin:1;/opt/cray/pe/bin:1"
declare -x __LMOD_REF_COUNT_PE_DSMML_PKGCONFIG_LIBS="dsmml:1"
declare -x __LMOD_REF_COUNT_PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:1;hdf5_fortran_parallel:1;mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FIXED_PRGENV="GNU:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_GENCOMPILERS_GNU="12.3:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_PKGCONFIG_LIBS="mpich:1"
declare -x __LMOD_REF_COUNT_PE_PALS_PKGCONFIG_LIBS="libpals:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_LIBS="hdf5_hl_parallel:1;hdf5_parallel:1;mpich:1;dsmml:1;darshan-runtime:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_PRODUCTS="PE_PALS:1;PE_PMI:1;PE_MPICH:1;PE_DSMML:1"
declare -x __LMOD_REF_COUNT_PE_PMI_PKGCONFIG_LIBS="cray-pmi:1"
declare -x __LMOD_REF_COUNT_PE_PRODUCT_LIST="CRAYPE_X86_MILAN:1;PERFTOOLS:1;CRAYPAT:1"
declare -x __LMOD_REF_COUNT_PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:1;/opt/cray/pals/1.3.4/lib/pkgconfig:1;/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:1;/opt/cray/pe/craype/2.7.30/pkg-config:1;/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:1;/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages:1"
declare -x ftp_proxy="http://proxy.alcf.anl.gov:3128"
declare -x http_proxy="http://proxy.alcf.anl.gov:3128"
declare -x https_proxy="http://proxy.alcf.anl.gov:3128"
declare -x no_proxy="admin,polaris-adminvm-01,localhost,*.cm.polaris.alcf.anl.gov,polaris-*,*.polaris.alcf.anl.gov,*.alcf.anl.gov"
Running on 7 nodes
Total number of GPUs: 28
Connected to tcp://x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov:7919
Found executable /soft/applications/conda/2024-04-29/mconda3/bin/python
Launching application 366c2578-b793-4b31-bda0-fd21fe8f8c88
Using PMI port 40396,40397
[2025-06-19 11:33:07,893] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:07,893] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:07,894] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:07,894] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,142] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,142] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,142] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,142] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,182] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,182] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,182] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,182] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,308] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,308] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,308] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,308] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,322] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,322] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,322] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,322] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,404] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,404] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,404] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:08,404] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:09,149] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:09,149] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:09,149] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 11:33:09,149] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:11,784] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:11,784] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:11,784] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-19 11:33:11,784] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:11,784] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:11,784] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:11,803] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:11,803] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:12,401] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:12,401] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:12,401] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:12,401] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:12,401] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:12,401] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:12,401] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-19 11:33:12,401] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:12,856] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:12,856] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:12,856] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:12,856] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:12,856] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:12,856] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:12,856] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:12,856] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:12,981] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:12,981] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:12,982] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:12,981] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:12,981] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:12,981] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:12,981] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-19 11:33:12,981] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:13,128] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:13,128] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:13,128] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:13,128] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:13,128] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-19 11:33:13,128] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:13,128] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:13,128] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:14,230] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:14,230] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:14,230] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-19 11:33:14,230] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:14,230] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:14,230] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:14,230] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:14,230] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:14,655] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:14,655] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:14,655] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:14,655] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:14,655] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=10, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 11:33:14,655] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 11:33:14,655] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=11, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=24, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,655] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=8, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=9, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=20, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=25, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=21, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=22, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=26, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=16, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=17, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=23, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=18, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=19, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=12, local_rank=0, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=13, local_rank=1, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=14, local_rank=2, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=15, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=27, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=28, master_addr=10.140.57.42, master_port=29500
[2025-06-19 11:33:14,656] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Initialized deepspeed on global rank 0, local rank 0 with world size 28.
[2025-06-19 11:37:25,984] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2+5f631abc, git-hash=5f631abc, git-branch=HEAD
[2025-06-19 11:37:38,247] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-06-19 11:37:38,248] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-06-19 11:37:38,248] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-06-19 11:37:38,264] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-06-19 11:37:38,264] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-06-19 11:37:38,264] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-06-19 11:37:38,264] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-06-19 11:37:38,264] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2025-06-19 11:37:38,265] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2025-06-19 11:37:38,265] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-06-19 11:37:38,265] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-06-19 11:37:38,265] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2025-06-19 11:37:38,265] [INFO] [config.py:1000:print]   amp_params ................... False
[2025-06-19 11:37:38,265] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-06-19 11:37:38,265] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2025-06-19 11:37:38,265] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2025-06-19 11:37:38,265] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2025-06-19 11:37:38,265] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2025-06-19 11:37:38,265] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2025-06-19 11:37:38,265] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x14c0b0d71fd0>
[2025-06-19 11:37:38,265] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2025-06-19 11:37:38,265] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2025-06-19 11:37:38,265] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-06-19 11:37:38,265] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2025-06-19 11:37:38,265] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2025-06-19 11:37:38,265] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   dump_state ................... False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   global_rank .................. 0
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 5e-05, 'weight_decay': 0.01}
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   pld_params ................... False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   steps_per_print .............. 100000
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   train_batch_size ............. 3584
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   world_size ................... 28
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2025-06-19 11:37:38,266] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2025-06-19 11:37:38,267] [INFO] [config.py:986:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 128, 
    "train_batch_size": 3.584000e+03, 
    "steps_per_print": 1.000000e+05, 
    "gradient_accumulation_steps": 1, 
    "fp16": {
        "enabled": false
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 5e-05, 
            "weight_decay": 0.01
        }
    }, 
    "comms_logger": {
        "enabled": true, 
        "verbose": false
    }, 
    "zero_optimization": {
        "stage": 0
    }
}
Validating lr=5e-05, train epoch 0.:   0%|          | 0/91 [00:00<?, ?it/s]Validating lr=5e-05, train epoch 0.:   1%|          | 1/91 [00:05<08:03,  5.37s/it]Validating lr=5e-05, train epoch 0.:   2%|▏         | 2/91 [00:09<06:53,  4.65s/it]Validating lr=5e-05, train epoch 0.:   3%|▎         | 3/91 [00:13<06:30,  4.44s/it]Validating lr=5e-05, train epoch 0.:   4%|▍         | 4/91 [00:17<06:17,  4.34s/it]Validating lr=5e-05, train epoch 0.:   5%|▌         | 5/91 [00:22<06:09,  4.29s/it]Validating lr=5e-05, train epoch 0.:   7%|▋         | 6/91 [00:26<06:02,  4.27s/it]Validating lr=5e-05, train epoch 0.:   8%|▊         | 7/91 [00:30<05:57,  4.25s/it]Validating lr=5e-05, train epoch 0.:   9%|▉         | 8/91 [00:34<05:54,  4.27s/it]Validating lr=5e-05, train epoch 0.:  10%|▉         | 9/91 [00:39<05:50,  4.27s/it]Validating lr=5e-05, train epoch 0.:  11%|█         | 10/91 [00:43<05:44,  4.26s/it]Validating lr=5e-05, train epoch 0.:  12%|█▏        | 11/91 [00:47<05:38,  4.23s/it]Validating lr=5e-05, train epoch 0.:  13%|█▎        | 12/91 [00:51<05:33,  4.22s/it]Validating lr=5e-05, train epoch 0.:  14%|█▍        | 13/91 [00:55<05:29,  4.23s/it]Validating lr=5e-05, train epoch 0.:  15%|█▌        | 14/91 [01:00<05:24,  4.22s/it]Validating lr=5e-05, train epoch 0.:  16%|█▋        | 15/91 [01:04<05:21,  4.24s/it]Validating lr=5e-05, train epoch 0.:  18%|█▊        | 16/91 [01:08<05:17,  4.23s/it]Validating lr=5e-05, train epoch 0.:  19%|█▊        | 17/91 [01:12<05:11,  4.20s/it]Validating lr=5e-05, train epoch 0.:  20%|█▉        | 18/91 [01:16<05:06,  4.20s/it]Validating lr=5e-05, train epoch 0.:  21%|██        | 19/91 [01:21<05:02,  4.20s/it]Validating lr=5e-05, train epoch 0.:  22%|██▏       | 20/91 [01:25<04:57,  4.19s/it]Validating lr=5e-05, train epoch 0.:  23%|██▎       | 21/91 [01:29<04:54,  4.20s/it]Validating lr=5e-05, train epoch 0.:  24%|██▍       | 22/91 [01:33<04:50,  4.21s/it]Validating lr=5e-05, train epoch 0.:  25%|██▌       | 23/91 [01:38<04:45,  4.20s/it]Validating lr=5e-05, train epoch 0.:  26%|██▋       | 24/91 [01:42<04:41,  4.21s/it]Validating lr=5e-05, train epoch 0.:  27%|██▋       | 25/91 [01:46<04:36,  4.20s/it]Validating lr=5e-05, train epoch 0.:  29%|██▊       | 26/91 [01:50<04:32,  4.20s/it]Validating lr=5e-05, train epoch 0.:  30%|██▉       | 27/91 [01:54<04:30,  4.22s/it]Validating lr=5e-05, train epoch 0.:  31%|███       | 28/91 [01:59<04:25,  4.21s/it]Validating lr=5e-05, train epoch 0.:  32%|███▏      | 29/91 [02:03<04:21,  4.21s/it]Validating lr=5e-05, train epoch 0.:  33%|███▎      | 30/91 [02:07<04:17,  4.22s/it]Validating lr=5e-05, train epoch 0.:  34%|███▍      | 31/91 [02:11<04:12,  4.21s/it]Validating lr=5e-05, train epoch 0.:  35%|███▌      | 32/91 [02:15<04:07,  4.20s/it]Validating lr=5e-05, train epoch 0.:  36%|███▋      | 33/91 [02:20<04:03,  4.20s/it]Validating lr=5e-05, train epoch 0.:  37%|███▋      | 34/91 [02:24<03:59,  4.20s/it]Validating lr=5e-05, train epoch 0.:  38%|███▊      | 35/91 [02:28<03:54,  4.19s/it]Validating lr=5e-05, train epoch 0.:  40%|███▉      | 36/91 [02:32<03:50,  4.18s/it]Validating lr=5e-05, train epoch 0.:  41%|████      | 37/91 [02:36<03:45,  4.18s/it]Validating lr=5e-05, train epoch 0.:  42%|████▏     | 38/91 [02:40<03:41,  4.17s/it]Validating lr=5e-05, train epoch 0.:  43%|████▎     | 39/91 [02:45<03:36,  4.17s/it]Validating lr=5e-05, train epoch 0.:  44%|████▍     | 40/91 [02:49<03:32,  4.17s/it]Validating lr=5e-05, train epoch 0.:  45%|████▌     | 41/91 [02:53<03:28,  4.18s/it]Validating lr=5e-05, train epoch 0.:  46%|████▌     | 42/91 [02:57<03:24,  4.18s/it]Validating lr=5e-05, train epoch 0.:  47%|████▋     | 43/91 [03:01<03:21,  4.19s/it]Validating lr=5e-05, train epoch 0.:  48%|████▊     | 44/91 [03:06<03:18,  4.21s/it]Validating lr=5e-05, train epoch 0.:  49%|████▉     | 45/91 [03:10<03:13,  4.22s/it]Validating lr=5e-05, train epoch 0.:  51%|█████     | 46/91 [03:14<03:09,  4.21s/it]Validating lr=5e-05, train epoch 0.:  52%|█████▏    | 47/91 [03:18<03:04,  4.20s/it]Validating lr=5e-05, train epoch 0.:  53%|█████▎    | 48/91 [03:23<03:01,  4.22s/it]Validating lr=5e-05, train epoch 0.:  54%|█████▍    | 49/91 [03:27<02:56,  4.21s/it]Validating lr=5e-05, train epoch 0.:  55%|█████▍    | 50/91 [03:31<02:53,  4.22s/it]Validating lr=5e-05, train epoch 0.:  56%|█████▌    | 51/91 [03:35<02:48,  4.21s/it]Validating lr=5e-05, train epoch 0.:  57%|█████▋    | 52/91 [03:39<02:43,  4.20s/it]Validating lr=5e-05, train epoch 0.:  58%|█████▊    | 53/91 [03:43<02:39,  4.19s/it]Validating lr=5e-05, train epoch 0.:  59%|█████▉    | 54/91 [03:48<02:35,  4.19s/it]Validating lr=5e-05, train epoch 0.:  60%|██████    | 55/91 [03:52<02:30,  4.19s/it]Validating lr=5e-05, train epoch 0.:  62%|██████▏   | 56/91 [03:56<02:26,  4.20s/it]Validating lr=5e-05, train epoch 0.:  63%|██████▎   | 57/91 [04:00<02:22,  4.19s/it]Validating lr=5e-05, train epoch 0.:  64%|██████▎   | 58/91 [04:04<02:18,  4.20s/it]Validating lr=5e-05, train epoch 0.:  65%|██████▍   | 59/91 [04:09<02:14,  4.20s/it]Validating lr=5e-05, train epoch 0.:  66%|██████▌   | 60/91 [04:13<02:11,  4.23s/it]Validating lr=5e-05, train epoch 0.:  67%|██████▋   | 61/91 [04:17<02:06,  4.21s/it]Validating lr=5e-05, train epoch 0.:  68%|██████▊   | 62/91 [04:21<02:01,  4.21s/it]Validating lr=5e-05, train epoch 0.:  69%|██████▉   | 63/91 [04:26<01:57,  4.21s/it]Validating lr=5e-05, train epoch 0.:  70%|███████   | 64/91 [04:30<01:53,  4.20s/it]Validating lr=5e-05, train epoch 0.:  71%|███████▏  | 65/91 [04:34<01:49,  4.20s/it]Validating lr=5e-05, train epoch 0.:  73%|███████▎  | 66/91 [04:38<01:44,  4.20s/it]Validating lr=5e-05, train epoch 0.:  74%|███████▎  | 67/91 [04:42<01:40,  4.19s/it]Validating lr=5e-05, train epoch 0.:  75%|███████▍  | 68/91 [04:46<01:36,  4.19s/it]Validating lr=5e-05, train epoch 0.:  76%|███████▌  | 69/91 [04:51<01:32,  4.21s/it]Validating lr=5e-05, train epoch 0.:  77%|███████▋  | 70/91 [04:55<01:28,  4.21s/it]Validating lr=5e-05, train epoch 0.:  78%|███████▊  | 71/91 [04:59<01:23,  4.20s/it]Validating lr=5e-05, train epoch 0.:  79%|███████▉  | 72/91 [05:03<01:19,  4.20s/it]Validating lr=5e-05, train epoch 0.:  80%|████████  | 73/91 [05:07<01:15,  4.20s/it]Validating lr=5e-05, train epoch 0.:  81%|████████▏ | 74/91 [05:12<01:11,  4.20s/it]Validating lr=5e-05, train epoch 0.:  82%|████████▏ | 75/91 [05:16<01:07,  4.20s/it]Validating lr=5e-05, train epoch 0.:  84%|████████▎ | 76/91 [05:20<01:03,  4.22s/it]Validating lr=5e-05, train epoch 0.:  85%|████████▍ | 77/91 [05:24<00:59,  4.22s/it]Validating lr=5e-05, train epoch 0.:  86%|████████▌ | 78/91 [05:29<00:54,  4.22s/it]Validating lr=5e-05, train epoch 0.:  87%|████████▋ | 79/91 [05:33<00:50,  4.22s/it]Validating lr=5e-05, train epoch 0.:  88%|████████▊ | 80/91 [05:37<00:46,  4.22s/it]Validating lr=5e-05, train epoch 0.:  89%|████████▉ | 81/91 [05:41<00:41,  4.20s/it]Validating lr=5e-05, train epoch 0.:  90%|█████████ | 82/91 [05:45<00:37,  4.19s/it]Validating lr=5e-05, train epoch 0.:  91%|█████████ | 83/91 [05:50<00:33,  4.19s/it]Validating lr=5e-05, train epoch 0.:  92%|█████████▏| 84/91 [05:54<00:29,  4.21s/it]Validating lr=5e-05, train epoch 0.:  93%|█████████▎| 85/91 [05:58<00:25,  4.21s/it]Validating lr=5e-05, train epoch 0.:  95%|█████████▍| 86/91 [06:02<00:20,  4.20s/it]Validating lr=5e-05, train epoch 0.:  96%|█████████▌| 87/91 [06:06<00:16,  4.21s/it]Validating lr=5e-05, train epoch 0.:  97%|█████████▋| 88/91 [06:11<00:12,  4.21s/it]Validating lr=5e-05, train epoch 0.:  98%|█████████▊| 89/91 [06:15<00:08,  4.20s/it]Validating lr=5e-05, train epoch 0.:  99%|█████████▉| 90/91 [06:19<00:04,  4.22s/it]Validating lr=5e-05, train epoch 0.: 100%|██████████| 91/91 [06:23<00:00,  4.22s/it]Validating lr=5e-05, train epoch 0.: 100%|██████████| 91/91 [06:23<00:00,  4.22s/it]
Validating lr=5e-05, train epoch 1.:   0%|          | 0/91 [00:00<?, ?it/s]Validating lr=5e-05, train epoch 1.:   1%|          | 1/91 [00:04<06:17,  4.19s/it]Validating lr=5e-05, train epoch 1.:   2%|▏         | 2/91 [00:08<06:12,  4.19s/it]Validating lr=5e-05, train epoch 1.:   3%|▎         | 3/91 [00:12<06:09,  4.19s/it]Validating lr=5e-05, train epoch 1.:   4%|▍         | 4/91 [00:16<06:07,  4.23s/it]Validating lr=5e-05, train epoch 1.:   5%|▌         | 5/91 [00:21<06:04,  4.24s/it]Validating lr=5e-05, train epoch 1.:   7%|▋         | 6/91 [00:25<05:59,  4.23s/it]Validating lr=5e-05, train epoch 1.:   8%|▊         | 7/91 [00:29<05:53,  4.21s/it]Validating lr=5e-05, train epoch 1.:   9%|▉         | 8/91 [00:33<05:47,  4.19s/it]Validating lr=5e-05, train epoch 1.:  10%|▉         | 9/91 [00:37<05:43,  4.19s/it]Validating lr=5e-05, train epoch 1.:  11%|█         | 10/91 [00:42<05:39,  4.19s/it]Validating lr=5e-05, train epoch 1.:  12%|█▏        | 11/91 [00:46<05:34,  4.18s/it]Validating lr=5e-05, train epoch 1.:  13%|█▎        | 12/91 [00:50<05:31,  4.19s/it]Validating lr=5e-05, train epoch 1.:  14%|█▍        | 13/91 [00:54<05:26,  4.19s/it]Validating lr=5e-05, train epoch 1.:  15%|█▌        | 14/91 [00:58<05:22,  4.19s/it]Validating lr=5e-05, train epoch 1.:  16%|█▋        | 15/91 [01:03<05:19,  4.21s/it]Validating lr=5e-05, train epoch 1.:  18%|█▊        | 16/91 [01:07<05:15,  4.20s/it]Validating lr=5e-05, train epoch 1.:  19%|█▊        | 17/91 [01:11<05:09,  4.18s/it]Validating lr=5e-05, train epoch 1.:  20%|█▉        | 18/91 [01:15<05:05,  4.18s/it]Validating lr=5e-05, train epoch 1.:  21%|██        | 19/91 [01:19<05:02,  4.20s/it]Validating lr=5e-05, train epoch 1.:  22%|██▏       | 20/91 [01:23<04:57,  4.19s/it]Validating lr=5e-05, train epoch 1.:  23%|██▎       | 21/91 [01:28<04:53,  4.19s/it]Validating lr=5e-05, train epoch 1.:  24%|██▍       | 22/91 [01:32<04:48,  4.19s/it]Validating lr=5e-05, train epoch 1.:  25%|██▌       | 23/91 [01:36<04:45,  4.20s/it]Validating lr=5e-05, train epoch 1.:  26%|██▋       | 24/91 [01:40<04:41,  4.21s/it]Validating lr=5e-05, train epoch 1.:  27%|██▋       | 25/91 [01:44<04:37,  4.20s/it]Validating lr=5e-05, train epoch 1.:  29%|██▊       | 26/91 [01:49<04:33,  4.20s/it]Validating lr=5e-05, train epoch 1.:  30%|██▉       | 27/91 [01:53<04:29,  4.21s/it]Validating lr=5e-05, train epoch 1.:  31%|███       | 28/91 [01:57<04:26,  4.23s/it]Validating lr=5e-05, train epoch 1.:  32%|███▏      | 29/91 [02:01<04:21,  4.21s/it]Validating lr=5e-05, train epoch 1.:  33%|███▎      | 30/91 [02:05<04:16,  4.20s/it]Validating lr=5e-05, train epoch 1.:  34%|███▍      | 31/91 [02:10<04:12,  4.20s/it]Validating lr=5e-05, train epoch 1.:  35%|███▌      | 32/91 [02:14<04:08,  4.21s/it]Validating lr=5e-05, train epoch 1.:  36%|███▋      | 33/91 [02:18<04:04,  4.22s/it]Validating lr=5e-05, train epoch 1.:  37%|███▋      | 34/91 [02:22<04:00,  4.22s/it]Validating lr=5e-05, train epoch 1.:  38%|███▊      | 35/91 [02:27<03:56,  4.23s/it]Validating lr=5e-05, train epoch 1.:  40%|███▉      | 36/91 [02:31<03:51,  4.22s/it]Validating lr=5e-05, train epoch 1.:  41%|████      | 37/91 [02:35<03:47,  4.22s/it]Validating lr=5e-05, train epoch 1.:  42%|████▏     | 38/91 [02:39<03:43,  4.22s/it]Validating lr=5e-05, train epoch 1.:  43%|████▎     | 39/91 [02:43<03:39,  4.22s/it]Validating lr=5e-05, train epoch 1.:  44%|████▍     | 40/91 [02:48<03:34,  4.21s/it]Validating lr=5e-05, train epoch 1.:  45%|████▌     | 41/91 [02:52<03:30,  4.21s/it]Validating lr=5e-05, train epoch 1.:  46%|████▌     | 42/91 [02:56<03:25,  4.20s/it]Validating lr=5e-05, train epoch 1.:  47%|████▋     | 43/91 [03:00<03:20,  4.19s/it]Validating lr=5e-05, train epoch 1.:  48%|████▊     | 44/91 [03:04<03:17,  4.20s/it]Validating lr=5e-05, train epoch 1.:  49%|████▉     | 45/91 [03:09<03:12,  4.19s/it]Validating lr=5e-05, train epoch 1.:  51%|█████     | 46/91 [03:13<03:09,  4.20s/it]Validating lr=5e-05, train epoch 1.:  52%|█████▏    | 47/91 [03:17<03:04,  4.20s/it]Validating lr=5e-05, train epoch 1.:  53%|█████▎    | 48/91 [03:21<03:01,  4.21s/it]Validating lr=5e-05, train epoch 1.:  54%|█████▍    | 49/91 [03:26<02:57,  4.22s/it]Validating lr=5e-05, train epoch 1.:  55%|█████▍    | 50/91 [03:30<02:52,  4.22s/it]Validating lr=5e-05, train epoch 1.:  56%|█████▌    | 51/91 [03:34<02:47,  4.20s/it]Validating lr=5e-05, train epoch 1.:  57%|█████▋    | 52/91 [03:38<02:43,  4.20s/it]Validating lr=5e-05, train epoch 1.:  58%|█████▊    | 53/91 [03:42<02:39,  4.19s/it]Validating lr=5e-05, train epoch 1.:  59%|█████▉    | 54/91 [03:47<02:35,  4.20s/it]Validating lr=5e-05, train epoch 1.:  60%|██████    | 55/91 [03:51<02:31,  4.20s/it]Validating lr=5e-05, train epoch 1.:  62%|██████▏   | 56/91 [03:55<02:26,  4.20s/it]Validating lr=5e-05, train epoch 1.:  63%|██████▎   | 57/91 [03:59<02:22,  4.19s/it]Validating lr=5e-05, train epoch 1.:  64%|██████▎   | 58/91 [04:03<02:18,  4.19s/it]Validating lr=5e-05, train epoch 1.:  65%|██████▍   | 59/91 [04:07<02:13,  4.19s/it]Validating lr=5e-05, train epoch 1.:  66%|██████▌   | 60/91 [04:12<02:10,  4.20s/it]Validating lr=5e-05, train epoch 1.:  67%|██████▋   | 61/91 [04:16<02:05,  4.20s/it]Validating lr=5e-05, train epoch 1.:  68%|██████▊   | 62/91 [04:20<02:01,  4.19s/it]Validating lr=5e-05, train epoch 1.:  69%|██████▉   | 63/91 [04:24<01:57,  4.21s/it]Validating lr=5e-05, train epoch 1.:  70%|███████   | 64/91 [04:29<01:54,  4.22s/it]Validating lr=5e-05, train epoch 1.:  71%|███████▏  | 65/91 [04:33<01:49,  4.23s/it]Validating lr=5e-05, train epoch 1.:  73%|███████▎  | 66/91 [04:37<01:45,  4.22s/it]Validating lr=5e-05, train epoch 1.:  74%|███████▎  | 67/91 [04:41<01:40,  4.20s/it]Validating lr=5e-05, train epoch 1.:  75%|███████▍  | 68/91 [04:45<01:36,  4.19s/it]Validating lr=5e-05, train epoch 1.:  76%|███████▌  | 69/91 [04:49<01:32,  4.18s/it]Validating lr=5e-05, train epoch 1.:  77%|███████▋  | 70/91 [04:54<01:28,  4.20s/it]Validating lr=5e-05, train epoch 1.:  78%|███████▊  | 71/91 [04:58<01:24,  4.20s/it]Validating lr=5e-05, train epoch 1.:  79%|███████▉  | 72/91 [05:02<01:19,  4.20s/it]Validating lr=5e-05, train epoch 1.:  80%|████████  | 73/91 [05:06<01:15,  4.19s/it]Validating lr=5e-05, train epoch 1.:  81%|████████▏ | 74/91 [05:11<01:11,  4.21s/it]Validating lr=5e-05, train epoch 1.:  82%|████████▏ | 75/91 [05:15<01:07,  4.22s/it]Validating lr=5e-05, train epoch 1.:  84%|████████▎ | 76/91 [05:19<01:03,  4.23s/it]Validating lr=5e-05, train epoch 1.:  85%|████████▍ | 77/91 [05:23<00:58,  4.21s/it]Validating lr=5e-05, train epoch 1.:  86%|████████▌ | 78/91 [05:27<00:54,  4.21s/it]Validating lr=5e-05, train epoch 1.:  87%|████████▋ | 79/91 [05:32<00:50,  4.22s/it]Validating lr=5e-05, train epoch 1.:  88%|████████▊ | 80/91 [05:36<00:46,  4.21s/it]Validating lr=5e-05, train epoch 1.:  89%|████████▉ | 81/91 [05:40<00:42,  4.20s/it]Validating lr=5e-05, train epoch 1.:  90%|█████████ | 82/91 [05:44<00:37,  4.22s/it]Validating lr=5e-05, train epoch 1.:  91%|█████████ | 83/91 [05:49<00:33,  4.23s/it]Validating lr=5e-05, train epoch 1.:  92%|█████████▏| 84/91 [05:53<00:29,  4.22s/it]Validating lr=5e-05, train epoch 1.:  93%|█████████▎| 85/91 [05:57<00:25,  4.21s/it]Validating lr=5e-05, train epoch 1.:  95%|█████████▍| 86/91 [06:01<00:20,  4.20s/it]Validating lr=5e-05, train epoch 1.:  96%|█████████▌| 87/91 [06:05<00:16,  4.21s/it]Validating lr=5e-05, train epoch 1.:  97%|█████████▋| 88/91 [06:10<00:12,  4.23s/it]Validating lr=5e-05, train epoch 1.:  98%|█████████▊| 89/91 [06:14<00:08,  4.21s/it]Validating lr=5e-05, train epoch 1.:  99%|█████████▉| 90/91 [06:18<00:04,  4.21s/it]Validating lr=5e-05, train epoch 1.: 100%|██████████| 91/91 [06:22<00:00,  4.20s/it]Validating lr=5e-05, train epoch 1.: 100%|██████████| 91/91 [06:22<00:00,  4.21s/it]
Evaluating for lr=5e-05:   0%|          | 0/9 [00:00<?, ?it/s]Evaluating for lr=5e-05:  11%|█         | 1/9 [00:01<00:14,  1.82s/it]Evaluating for lr=5e-05:  22%|██▏       | 2/9 [00:03<00:12,  1.81s/it]Evaluating for lr=5e-05:  33%|███▎      | 3/9 [00:05<00:10,  1.81s/it]Evaluating for lr=5e-05:  44%|████▍     | 4/9 [00:07<00:09,  1.83s/it]Evaluating for lr=5e-05:  56%|█████▌    | 5/9 [00:09<00:07,  1.85s/it]Evaluating for lr=5e-05:  67%|██████▋   | 6/9 [00:10<00:05,  1.83s/it]Evaluating for lr=5e-05:  78%|███████▊  | 7/9 [00:12<00:03,  1.83s/it]Evaluating for lr=5e-05:  89%|████████▉ | 8/9 [00:14<00:01,  1.82s/it]Evaluating for lr=5e-05: 100%|██████████| 9/9 [00:16<00:00,  1.84s/it]Evaluating for lr=5e-05: 100%|██████████| 9/9 [00:16<00:00,  1.83s/it]
Initialized deepspeed on global rank 1, local rank 1 with world size 28.
Initialized deepspeed on global rank 3, local rank 3 with world size 28.
Initialized deepspeed on global rank 2, local rank 2 with world size 28.
Initialized deepspeed on global rank 17, local rank 1 with world size 28.
Initialized deepspeed on global rank 18, local rank 2 with world size 28.
Initialized deepspeed on global rank 19, local rank 3 with world size 28.
Initialized deepspeed on global rank 16, local rank 0 with world size 28.
Initialized deepspeed on global rank 9, local rank 1 with world size 28.
Initialized deepspeed on global rank 10, local rank 2 with world size 28.
Initialized deepspeed on global rank 8, local rank 0 with world size 28.
Initialized deepspeed on global rank 11, local rank 3 with world size 28.
Initialized deepspeed on global rank 4, local rank 0 with world size 28.
Initialized deepspeed on global rank 25, local rank 1 with world size 28.
Initialized deepspeed on global rank 6, local rank 2 with world size 28.
Initialized deepspeed on global rank 5, local rank 1 with world size 28.
Initialized deepspeed on global rank 26, local rank 2 with world size 28.
Initialized deepspeed on global rank 7, local rank 3 with world size 28.
Initialized deepspeed on global rank 24, local rank 0 with world size 28.
Initialized deepspeed on global rank 27, local rank 3 with world size 28.
Initialized deepspeed on global rank 20, local rank 0 with world size 28.
Initialized deepspeed on global rank 12, local rank 0 with world size 28.
Initialized deepspeed on global rank 13, local rank 1 with world size 28.
Initialized deepspeed on global rank 21, local rank 1 with world size 28.
Initialized deepspeed on global rank 23, local rank 3 with world size 28.
Initialized deepspeed on global rank 22, local rank 2 with world size 28.
Initialized deepspeed on global rank 14, local rank 2 with world size 28.
Initialized deepspeed on global rank 15, local rank 3 with world size 28.
[rank17]: Traceback (most recent call last):
[rank17]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank17]:     results_tuning.iloc[len(results_tuning)] = {
[rank17]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank17]:     self._has_valid_setitem_indexer(key)
[rank17]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank17]:     raise IndexError("iloc cannot enlarge its target object")
[rank17]: IndexError: iloc cannot enlarge its target object
[rank18]: Traceback (most recent call last):
[rank18]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank18]:     results_tuning.iloc[len(results_tuning)] = {
[rank18]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank18]:     self._has_valid_setitem_indexer(key)
[rank18]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank18]:     raise IndexError("iloc cannot enlarge its target object")
[rank18]: IndexError: iloc cannot enlarge its target object
[rank19]: Traceback (most recent call last):
[rank19]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank19]:     results_tuning.iloc[len(results_tuning)] = {
[rank19]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank19]:     self._has_valid_setitem_indexer(key)
[rank19]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank19]:     raise IndexError("iloc cannot enlarge its target object")
[rank19]: IndexError: iloc cannot enlarge its target object
[rank8]: Traceback (most recent call last):
[rank8]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank8]:     results_tuning.iloc[len(results_tuning)] = {
[rank8]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank8]:     self._has_valid_setitem_indexer(key)
[rank8]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank8]:     raise IndexError("iloc cannot enlarge its target object")
[rank8]: IndexError: iloc cannot enlarge its target object
[rank16]: Traceback (most recent call last):
[rank16]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank16]:     results_tuning.iloc[len(results_tuning)] = {
[rank16]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank16]:     self._has_valid_setitem_indexer(key)
[rank16]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank16]:     raise IndexError("iloc cannot enlarge its target object")
[rank16]: IndexError: iloc cannot enlarge its target object
[rank9]: Traceback (most recent call last):
[rank9]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank9]:     results_tuning.iloc[len(results_tuning)] = {
[rank9]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank9]:     self._has_valid_setitem_indexer(key)
[rank9]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank9]:     raise IndexError("iloc cannot enlarge its target object")
[rank9]: IndexError: iloc cannot enlarge its target object
[rank10]: Traceback (most recent call last):
[rank10]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank10]:     results_tuning.iloc[len(results_tuning)] = {
[rank10]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank10]:     self._has_valid_setitem_indexer(key)
[rank10]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank10]:     raise IndexError("iloc cannot enlarge its target object")
[rank10]: IndexError: iloc cannot enlarge its target object
[rank11]: Traceback (most recent call last):
[rank11]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank11]:     results_tuning.iloc[len(results_tuning)] = {
[rank11]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank11]:     self._has_valid_setitem_indexer(key)
[rank11]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank11]:     raise IndexError("iloc cannot enlarge its target object")
[rank11]: IndexError: iloc cannot enlarge its target object
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank1]:     results_tuning.iloc[len(results_tuning)] = {
[rank1]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank1]:     self._has_valid_setitem_indexer(key)
[rank1]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank1]:     raise IndexError("iloc cannot enlarge its target object")
[rank1]: IndexError: iloc cannot enlarge its target object
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank2]:     results_tuning.iloc[len(results_tuning)] = {
[rank2]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank2]:     self._has_valid_setitem_indexer(key)
[rank2]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank2]:     raise IndexError("iloc cannot enlarge its target object")
[rank2]: IndexError: iloc cannot enlarge its target object
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank3]:     results_tuning.iloc[len(results_tuning)] = {
[rank3]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank3]:     self._has_valid_setitem_indexer(key)
[rank3]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank3]:     raise IndexError("iloc cannot enlarge its target object")
[rank3]: IndexError: iloc cannot enlarge its target object
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank0]:     results_tuning.iloc[len(results_tuning)] = {
[rank0]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank0]:     self._has_valid_setitem_indexer(key)
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank0]:     raise IndexError("iloc cannot enlarge its target object")
[rank0]: IndexError: iloc cannot enlarge its target object
[rank4]: Traceback (most recent call last):
[rank4]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank4]:     results_tuning.iloc[len(results_tuning)] = {
[rank4]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank4]:     self._has_valid_setitem_indexer(key)
[rank4]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank4]:     raise IndexError("iloc cannot enlarge its target object")
[rank4]: IndexError: iloc cannot enlarge its target object
[rank5]: Traceback (most recent call last):
[rank5]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank5]:     results_tuning.iloc[len(results_tuning)] = {
[rank5]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank5]:     self._has_valid_setitem_indexer(key)
[rank5]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank5]:     raise IndexError("iloc cannot enlarge its target object")
[rank5]: IndexError: iloc cannot enlarge its target object
[rank6]: Traceback (most recent call last):
[rank6]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank6]:     results_tuning.iloc[len(results_tuning)] = {
[rank6]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank6]:     self._has_valid_setitem_indexer(key)
[rank6]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank6]:     raise IndexError("iloc cannot enlarge its target object")
[rank6]: IndexError: iloc cannot enlarge its target object
[rank7]: Traceback (most recent call last):
[rank7]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank7]:     results_tuning.iloc[len(results_tuning)] = {
[rank7]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank7]:     self._has_valid_setitem_indexer(key)
[rank7]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank7]:     raise IndexError("iloc cannot enlarge its target object")
[rank7]: IndexError: iloc cannot enlarge its target object
[rank24]: Traceback (most recent call last):
[rank24]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank24]:     results_tuning.iloc[len(results_tuning)] = {
[rank24]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank24]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank24]:     self._has_valid_setitem_indexer(key)
[rank24]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank24]:     raise IndexError("iloc cannot enlarge its target object")
[rank24]: IndexError: iloc cannot enlarge its target object
[rank25]: Traceback (most recent call last):
[rank25]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank25]:     results_tuning.iloc[len(results_tuning)] = {
[rank25]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank25]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank25]:     self._has_valid_setitem_indexer(key)
[rank25]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank25]:     raise IndexError("iloc cannot enlarge its target object")
[rank25]: IndexError: iloc cannot enlarge its target object
[rank26]: Traceback (most recent call last):
[rank26]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank26]:     results_tuning.iloc[len(results_tuning)] = {
[rank26]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank26]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank26]:     self._has_valid_setitem_indexer(key)
[rank26]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank26]:     raise IndexError("iloc cannot enlarge its target object")
[rank26]: IndexError: iloc cannot enlarge its target object
[rank27]: Traceback (most recent call last):
[rank27]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank27]:     results_tuning.iloc[len(results_tuning)] = {
[rank27]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank27]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank27]:     self._has_valid_setitem_indexer(key)
[rank27]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank27]:     raise IndexError("iloc cannot enlarge its target object")
[rank27]: IndexError: iloc cannot enlarge its target object
[rank20]: Traceback (most recent call last):
[rank20]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank20]:     results_tuning.iloc[len(results_tuning)] = {
[rank20]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank20]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank20]:     self._has_valid_setitem_indexer(key)
[rank20]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank20]:     raise IndexError("iloc cannot enlarge its target object")
[rank20]: IndexError: iloc cannot enlarge its target object
[rank21]: Traceback (most recent call last):
[rank21]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank21]:     results_tuning.iloc[len(results_tuning)] = {
[rank21]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank21]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank21]:     self._has_valid_setitem_indexer(key)
[rank21]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank21]:     raise IndexError("iloc cannot enlarge its target object")
[rank21]: IndexError: iloc cannot enlarge its target object
[rank22]: Traceback (most recent call last):
[rank22]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank22]:     results_tuning.iloc[len(results_tuning)] = {
[rank22]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank22]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank22]:     self._has_valid_setitem_indexer(key)
[rank22]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank22]:     raise IndexError("iloc cannot enlarge its target object")
[rank22]: IndexError: iloc cannot enlarge its target object
[rank23]: Traceback (most recent call last):
[rank23]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank23]:     results_tuning.iloc[len(results_tuning)] = {
[rank23]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank23]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank23]:     self._has_valid_setitem_indexer(key)
[rank23]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank23]:     raise IndexError("iloc cannot enlarge its target object")
[rank23]: IndexError: iloc cannot enlarge its target object
[rank12]: Traceback (most recent call last):
[rank12]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank12]:     results_tuning.iloc[len(results_tuning)] = {
[rank12]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank12]:     self._has_valid_setitem_indexer(key)
[rank12]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank12]:     raise IndexError("iloc cannot enlarge its target object")
[rank12]: IndexError: iloc cannot enlarge its target object
[rank13]: Traceback (most recent call last):
[rank13]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank13]:     results_tuning.iloc[len(results_tuning)] = {
[rank13]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank13]:     self._has_valid_setitem_indexer(key)
[rank13]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank13]:     raise IndexError("iloc cannot enlarge its target object")
[rank13]: IndexError: iloc cannot enlarge its target object
[rank14]: Traceback (most recent call last):
[rank14]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank14]:     results_tuning.iloc[len(results_tuning)] = {
[rank14]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank14]:     self._has_valid_setitem_indexer(key)
[rank14]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank14]:     raise IndexError("iloc cannot enlarge its target object")
[rank14]: IndexError: iloc cannot enlarge its target object
[rank15]: Traceback (most recent call last):
[rank15]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 282, in <module>
[rank15]:     results_tuning.iloc[len(results_tuning)] = {
[rank15]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 908, in __setitem__
[rank15]:     self._has_valid_setitem_indexer(key)
[rank15]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/indexing.py", line 1646, in _has_valid_setitem_indexer
[rank15]:     raise IndexError("iloc cannot enlarge its target object")
[rank15]: IndexError: iloc cannot enlarge its target object
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 24 exited with code 1
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 16 exited with code 1
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 20 exited with code 1
x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 4 died from signal 15
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 12 exited with code 1
x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 0 died from signal 15
x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 8 died from signal 15
x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 5 died from signal 15
x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 7 died from signal 15
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 27 exited with code 1
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 25 exited with code 1
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 17 exited with code 1
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 18 exited with code 1
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 26 exited with code 1
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 19 exited with code 1
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 21 exited with code 1
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 23 exited with code 1
x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 2 died from signal 15
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 22 exited with code 1
x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 9 died from signal 15
x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 10 died from signal 15
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 15 exited with code 1
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 14 exited with code 1
x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 3 died from signal 15
x3003c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 1 died from signal 15
x3003c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 6 died from signal 15
x3003c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 11 died from signal 15
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 13 exited with code 1
Application 366c2578 resources: utime=18379s stime=5742s maxrss=35867788KB inblock=405241930 oublock=536 minflt=344639255 majflt=52513 nvcsw=45378575 nivcsw=11089999
Training completed
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled

Lmod is automatically replacing "nvhpc/23.9" with "gcc-native/12.3".


Lmod is automatically replacing "PrgEnv-nvhpc/8.5.0" with "PrgEnv-gnu/8.5.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.28

declare -x APP2_STATE="23.12.0"
declare -x BASH_ENV="/usr/share/lmod/lmod/init/bash"
declare -x C3_RSH="ssh -oConnectTimeout=10 -oForwardX11=no"
declare -x CFLAGS="-I/soft/applications/conda/2024-04-29/mconda3/include"
declare -x COLORTERM="1"
declare -x COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin"
declare -x CONDA_DEFAULT_ENV="base"
declare -x CONDA_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/conda"
declare -x CONDA_PREFIX="/soft/applications/conda/2024-04-29/mconda3"
declare -x CONDA_PROMPT_MODIFIER="(2024-04-29/base) "
declare -x CONDA_PYTHON_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/python"
declare -x CONDA_SHLVL="1"
declare -x CPU="x86_64"
declare -x CRAYPAT_LD_LIBRARY_PATH="/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAYPAT_OPTS_EXECUTABLE="libexec64/opts"
declare -x CRAYPAT_ROOT="/opt/cray/pe/perftools/23.12.0"
declare -x CRAYPE_DIR="/opt/cray/pe/craype/2.7.30"
declare -x CRAYPE_NETWORK_TARGET="ofi"
declare -x CRAYPE_VERSION="2.7.30"
declare -x CRAY_CPU_TARGET="x86-milan"
declare -x CRAY_DSMML_BASEDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_DIR="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_PREFIX="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_ROOTDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_VER="0.2.2"
declare -x CRAY_DSMML_VERSION="0.2.2"
declare -x CRAY_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x CRAY_HDF5_PARALLEL_PREFIX="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x CRAY_HDF5_PARALLEL_VERSION="1.12.2.9"
declare -x CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:/opt/cray/pe/pmi/6.1.13/lib:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:/opt/cray/pe/mpich/8.1.28/gtl/lib:/opt/cray/pe/dsmml/0.2.2/dsmml/lib:/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAY_LMOD_COMPILER="gnu/12.0"
declare -x CRAY_LMOD_CPU="x86-milan/1.0"
declare -x CRAY_LMOD_MPI="cray-mpich/8.0"
declare -x CRAY_LMOD_NET="ofi/1.0"
declare -x CRAY_MPICH_BASEDIR="/opt/cray/pe/mpich/8.1.28/ofi"
declare -x CRAY_MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_PREFIX="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_ROOTDIR="/opt/cray/pe/mpich/8.1.28"
declare -x CRAY_MPICH_VER="8.1.28"
declare -x CRAY_MPICH_VERSION="8.1.28"
declare -x CRAY_PERFTOOLS_PREFIX="/opt/cray/pe/perftools/23.12.0"
declare -x CRAY_PERFTOOLS_VERSION="23.12.0"
declare -x CRAY_PMI_INCLUDE_OPTS="-I/opt/cray/pe/pmi/6.1.13/include"
declare -x CRAY_PMI_POST_LINK_OPTS="-L/opt/cray/pe/pmi/6.1.13/lib"
declare -x CRAY_PMI_PREFIX="/opt/cray/pe/pmi/6.1.13"
declare -x CRAY_PMI_VERSION="6.1.13"
declare -x CSHEDIT="emacs"
declare -x CUDA_HOME="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_TOOLKIT_BASE="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDNN_HOME="/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/"
declare -x ENVIRONMENT="BATCH"
declare -x ENV_NAME="conda/2024-04-29"
declare -x FROM_HEADER=""
declare -x GCC_PATH="/usr/bin"
declare -x GCC_PREFIX="/usr/lib64/gcc/x86_64-suse-linux/12"
declare -x GCC_VERSION="12.3"
declare -x GNU_VERSION="12.3"
declare -x GPG_TTY="not a tty"
declare -x GSETTINGS_SCHEMA_DIR="/soft/applications/conda/2024-04-29/mconda3/share/glib-2.0/schemas"
declare -x GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=""
declare -x G_BROKEN_FILENAMES="1"
declare -x G_FILENAME_ENCODING="@locale,UTF-8,ISO-8859-15,CP1252"
declare -x HDF5_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HDF5_ROOT="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HISTSIZE="1000"
declare -x HOME="/home/shourya01"
declare -x HOST="x3002c0s19b0n0"
declare -x HOSTNAME="x3002c0s19b0n0"
declare -x HOSTTYPE="x86_64"
declare -x HTTPS_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x HTTP_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x LANG="en_US.UTF-8"
declare -x LANGUAGE="en_US.UTF-8"
declare -x LDFLAGS="-L/soft/applications/conda/2024-04-29/mconda3/lib -Wl,--enable-new-dtags,-rpath,/soft/applications/conda/2024-04-29/mconda3/lib"
declare -x LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:/soft/perftools/darshan/darshan-3.4.4/lib:/opt/cray/pe/papi/7.0.1.2/lib64:/opt/cray/libfabric/1.15.2.0/lib64"
declare -x LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so"
declare -x LESS="-M -I -R"
declare -x LESSCLOSE="lessclose.sh %s %s"
declare -x LESSKEY="/etc/lesskey.bin"
declare -x LESSOPEN="lessopen.sh %s"
declare -x LESS_ADVANCED_PREPROCESSOR="no"
declare -x LMOD_CMD="/usr/share/lmod/lmod/libexec/lmod"
declare -x LMOD_DIR="/usr/share/lmod/lmod/libexec"
declare -x LMOD_FAMILY_COMPILER="gcc-native"
declare -x LMOD_FAMILY_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_CRAYPE="craype"
declare -x LMOD_FAMILY_CRAYPE_CPU="craype-x86-milan"
declare -x LMOD_FAMILY_CRAYPE_CPU_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_NETWORK="craype-network-ofi"
declare -x LMOD_FAMILY_CRAYPE_NETWORK_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_VERSION="2.7.30"
declare -x LMOD_FAMILY_GCC_COMPILER="gcc-native"
declare -x LMOD_FAMILY_GCC_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_HDF5="cray-hdf5-parallel"
declare -x LMOD_FAMILY_HDF5_VERSION="1.12.2.9"
declare -x LMOD_FAMILY_MPI="cray-mpich"
declare -x LMOD_FAMILY_MPI_VERSION="8.1.28"
declare -x LMOD_FAMILY_PRGENV="PrgEnv-gnu"
declare -x LMOD_FAMILY_PRGENV_VERSION="8.5.0"
declare -x LMOD_FAMILY_PYTHON="conda"
declare -x LMOD_FAMILY_PYTHON_VERSION="2024-04-29"
declare -x LMOD_PKG="/usr/share/lmod/lmod"
declare -x LMOD_ROOT="/usr/share/lmod"
declare -x LMOD_SETTARG_FULL_SUPPORT="no"
declare -x LMOD_SYSTEM_DEFAULT_MODULES="PrgEnv-nvhpc:craype-network-ofi:perftools-base:darshan:xalt"
declare -x LMOD_VERSION="8.7.34"
declare -x LMOD_sys="Linux"
declare -x LOADEDMODULES="libfabric/1.15.2.0:craype-network-ofi:perftools-base/23.12.0:darshan/3.4.4:xalt/3.0.2-202408282050:gcc-native/12.3:craype/2.7.30:cray-dsmml/0.2.2:cray-mpich/8.1.28:cray-pmi/6.1.13:cray-pals/1.3.4:cray-libpals/1.3.4:craype-x86-milan:PrgEnv-gnu/8.5.0:cray-hdf5-parallel/1.12.2.9:cudnn/9.1.0:conda/2024-04-29"
declare -x LOGNAME="shourya01"
declare -x MACHTYPE="x86_64-suse-linux"
declare -x MAIL="/var/spool/mail/shourya01"
declare -x MANPATH="/opt/cray/pals/1.3.4/man:/opt/cray/pe/pmi/6.1.13/man:/opt/cray/pe/mpich/8.1.28/ofi/man:/opt/cray/pe/mpich/8.1.28/man/mpich:/opt/cray/pe/dsmml/0.2.2/dsmml/man:/opt/cray/pe/craype/2.7.30/man:/opt/cray/pe/perftools/23.12.0/man:/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:/opt/cray/libfabric/1.15.2.0/share/man:/usr/share/lmod/lmod/share/man:/home/shourya01/.local/man:/usr/local/man:/usr/share/man:/usr/man:/opt/c3/man:/opt/pbs/share/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man"
declare -x MINICOM="-c on"
declare -x MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:/opt/cray/pe/lmod/modulefiles/mix_compilers:/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:/soft/modulefiles:/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:/usr/share/modulefiles/Linux:/usr/share/modulefiles/Core:/usr/share/lmod/lmod/modulefiles/Core:/usr/share/lmod/lmod/modulefiles:/opt/cray/pals/lmod/modulefiles/core:/opt/cray/modulefiles:/opt/cray/pe/lmod/modulefiles/core:/opt/cray/pe/lmod/modulefiles/craype-targets/default:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:/soft/xalt/modulefiles"
declare -x MODULEPATH_ROOT="/usr/share/modulefiles"
declare -x MODULESHOME="/usr/share/lmod/lmod"
declare -x MORE="-sl"
declare -x MPI4JAX_USE_CUDA_MPI="1"
declare -x MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x MPICH_GPU_SUPPORT_ENABLED="1"
declare -x NCCL_IB_DISABLE="1"
declare -x NCCL_SOCKET_IFNAME="hsn"
declare -x NCPUS="64"
declare -x OFFLOAD_INIT="on_start"
declare -x OLDPWD
declare -x OMP_NUM_THREADS="4"
declare -x OSCAR_HOME="/opt/oscar"
declare -x OSTYPE="linux"
declare -x PAGER="less"
declare -x PALS_TRANSFER="0"
declare -x PATH="/soft/applications/conda/2024-04-29/mconda3/bin:/soft/applications/conda/2024-04-29/mconda3/condabin:/soft/xalt/3.0.2-202408282050/bin:/soft/compilers/cudatoolkit/cuda-12.4.1/bin:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:/opt/cray/pe/hdf5/1.12.2.9/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/home/shourya01/.local/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/shourya01/bin:/opt/cray/pe/bin"
declare -x PAT_RT_PERFCTR_DISABLE_COMPONENTS="nvml,rocm_smi"
declare -x PBS_ACCOUNT="ParaLLMs"
declare -x PBS_ENVIRONMENT="PBS_BATCH"
declare -x PBS_JOBCOOKIE="16054E5C1B642C890636306C0945F4BF"
declare -x PBS_JOBDIR="/home/shourya01"
declare -x PBS_JOBID="5137284.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_JOBNAME="bash"
declare -x PBS_MOMPORT="15003"
declare -x PBS_NODEFILE="/var/spool/pbs/aux/5137284.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_NODENUM="0"
declare -x PBS_O_HOME="/home/shourya01"
declare -x PBS_O_HOST="polaris-login-02.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_O_INTERACTIVE_AUTH_METHOD="resvport"
declare -x PBS_O_LANG="en_US.UTF-8"
declare -x PBS_O_LOGNAME="shourya01"
declare -x PBS_O_MAIL="/var/spool/mail/shourya01"
declare -x PBS_O_PATH="/home/shourya01/.local/bin:/home/shourya01/.vscode-server/cli/servers/Stable-91fa95bccb027ece6a968589bb1d662fa9c8e170/server/bin/remote-cli:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/soft/xalt/3.0.2-202408282050/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/nvidia/23.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/extras/qd/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/shourya01/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/dbhome/db2cat/sqllib/bin:/dbhome/db2cat/sqllib/adm:/dbhome/db2cat/sqllib/misc:/dbhome/db2cat/sqllib/gskit/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/opt/cray/pe/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.vscode-server/extensions/ms-python.debugpy-2025.8.0/bundled/scripts/noConfigScripts"
declare -x PBS_O_QUEUE="debug-scaling"
declare -x PBS_O_SHELL="/bin/bash"
declare -x PBS_O_SYSTEM="Linux"
declare -x PBS_O_WORKDIR="/home/shourya01"
declare -x PBS_QUEUE="debug-scaling"
declare -x PBS_TASKNUM="1"
declare -x PELOCAL_PRGENV="true"
declare -x PERFTOOLS_VERSION="23.12.0"
declare -x PE_DSMML_MODULE_NAME="cray-dsmml"
declare -x PE_DSMML_PKGCONFIG_LIBS="dsmml"
declare -x PE_ENV="GNU"
declare -x PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel:mpichf90"
declare -x PE_GCC_EXTERNAL="native"
declare -x PE_GCC_LEVEL="12"
declare -x PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig"
declare -x PE_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x PE_HDF5_PARALLEL_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel"
declare -x PE_HDF5_PARALLEL_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel"
declare -x PE_MPICH_FIXED_PRGENV="GNU"
declare -x PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90"
declare -x PE_MPICH_GENCOMPILERS_GNU="12.3"
declare -x PE_MPICH_GTL_DIR_amd_gfx906="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx908="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx90a="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx940="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx942="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia70="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia80="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia90="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_ponteVecchio="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_LIBS_amd_gfx906="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx908="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx90a="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx940="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx942="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_nvidia70="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia80="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia90="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_ponteVecchio="-lmpi_gtl_ze"
declare -x PE_MPICH_MODULE_NAME="cray-mpich"
declare -x PE_MPICH_PKGCONFIG_LIBS="mpich"
declare -x PE_MPICH_PKGCONFIG_VARIABLES="PE_MPICH_GTL_DIR_@accelerator@:PE_MPICH_GTL_LIBS_@accelerator@"
declare -x PE_PALS_PKGCONFIG_LIBS="libpals"
declare -x PE_PERFTOOLS_MPICH_LIBDIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib"
declare -x PE_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel:mpich:dsmml:darshan-runtime"
declare -x PE_PKGCONFIG_PRODUCTS="PE_PALS:PE_PMI:PE_MPICH:PE_DSMML"
declare -x PE_PMI_PKGCONFIG_LIBS="cray-pmi"
declare -x PE_PRODUCT_LIST="CRAYPE_X86_MILAN"
declare -x PKGCONFIG_ENABLED="1"
declare -x PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:/opt/cray/pals/1.3.4/lib/pkgconfig:/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:/opt/cray/pe/craype/2.7.30/pkg-config:/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig"
declare -x PROFILEREAD="true"
declare -x PWD="/home/shourya01"
declare -x PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages"
declare -x PYTHONUSERBASE="/home/shourya01/.local/polaris/conda/2024-04-29"
declare -x QT_SYSTEM_DIR="/usr/share/desktop-data"
declare -x SHELL="/bin/bash"
declare -x SHLVL="2"
declare -x SLURM_MPI_TYPE="cray_shasta"
declare -x STARSHIP_SESSION_KEY="4210312142301411"
declare -x STARSHIP_SHELL="bash"
declare -x TMPDIR="/var/tmp/pbs.5137284.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x TRITON_DISABLE_AUTOTUNE="1"
declare -x TZ="Etc/UTC"
declare -x USER="shourya01"
declare -x USE_PCM_DB="2"
declare -x WINDOWMANAGER="xterm"
declare -x XALT_DIR="/soft/xalt/3.0.2-202408282050"
declare -x XALT_EXECUTABLE_TRACKING="yes"
declare -x XALT_SAMPLING="no"
declare -x XALT_SCALAR_AND_SPSR_SAMPLING="yes"
declare -x XCURSOR_THEME="DMZ"
declare -x XDG_CONFIG_DIRS="/etc/xdg"
declare -x XDG_DATA_DIRS="/usr/share"
declare -x XKEYSYMDB="/usr/X11R6/lib/X11/XKeysymDB"
declare -x XLA_FLAGS="--xla_gpu_force_compilation_parallelism=1 --xla_gpu_cuda_data_dir=/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x XLA_PYTHON_CLIENT_PREALLOCATE="false"
declare -x XML_CATALOG_FILES="file:///soft/applications/conda/2024-04-29/mconda3/etc/xml/catalog file:///etc/xml/catalog"
declare -x XNLSPATH="/usr/X11R6/lib/X11/nls"
declare -x _CE_CONDA=""
declare -x _CE_M=""
declare -x _LMFILES_="/opt/cray/modulefiles/libfabric/1.15.2.0:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-network-ofi.lua:/opt/cray/pe/lmod/modulefiles/core/perftools-base/23.12.0.lua:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles/darshan/3.4.4:/soft/xalt/modulefiles/xalt/3.0.2-202408282050:/opt/cray/pe/lmod/modulefiles/core/gcc-native/12.3.lua:/opt/cray/pe/lmod/modulefiles/core/craype/2.7.30.lua:/opt/cray/pe/lmod/modulefiles/core/cray-dsmml/0.2.2.lua:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0/cray-mpich/8.1.28.lua:/opt/cray/pe/lmod/modulefiles/core/cray-pmi/6.1.13.lua:/opt/cray/pals/lmod/modulefiles/core/cray-pals/1.3.4.lua:/opt/cray/pals/lmod/modulefiles/core/cray-libpals/1.3.4.lua:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-x86-milan.lua:/opt/cray/pe/lmod/modulefiles/core/PrgEnv-gnu/8.5.0.lua:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2.9.lua:/soft/modulefiles/cudnn/9.1.0.lua:/soft/modulefiles/conda/2024-04-29.lua"
declare -x _ModuleTable001_="X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpQcmdFbnYgPSAiUHJnRW52LWdudSIsCmNvbXBpbGVyID0gImdjYy1uYXRpdmUiLApjcmF5cGUgPSAiY3JheXBlIiwKY3JheXBlX2NwdSA9ICJjcmF5cGUteDg2LW1pbGFuIiwKY3JheXBlX25ldHdvcmsgPSAiY3JheXBlLW5ldHdvcmstb2ZpIiwKZ2NjX2NvbXBpbGVyID0gImdjYy1uYXRpdmUiLApoZGY1ID0gImNyYXktaGRmNS1wYXJhbGxlbCIsCm1waSA9ICJjcmF5LW1waWNoIiwKcHl0aG9uID0gImNvbmRhIiwKfSwKbVQgPSB7ClsiUHJnRW52LWdudSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUv"
declare -x _ModuleTable002_="bG1vZC9tb2R1bGVmaWxlcy9jb3JlL1ByZ0Vudi1nbnUvOC41LjAubHVhIiwKZnVsbE5hbWUgPSAiUHJnRW52LWdudS84LjUuMCIsCmxvYWRPcmRlciA9IDE0LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gIlByZ0Vudi1nbnUiLAp3ViA9ICJeMDAwMDAwMDguMDAwMDAwMDA1Lip6ZmluYWwiLAp9LApjb25kYSA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY29uZGEvMjAyNC0wNC0yOS5sdWEiLApmdWxsTmFtZSA9ICJjb25kYS8yMDI0LTA0LTI5IiwKbG9hZE9yZGVyID0gMTcsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY29uZGEiLAp3ViA9ICJeMDAw"
declare -x _ModuleTable003_="MDIwMjQuKnpmaW5hbC0uMDAwMDAwMDA0Lip6ZmluYWwtLjAwMDAwMDAyOS4qemZpbmFsIiwKfSwKWyJjcmF5LWRzbW1sIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1kc21tbC8wLjIuMi5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWRzbW1sLzAuMi4yIiwKbG9hZE9yZGVyID0gOCwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWRzbW1sIiwKd1YgPSAiXjAwMDAwMDAwLjAwMDAwMDAwMi4wMDAwMDAwMDIuKnpmaW5hbCIsCn0sClsiY3JheS1oZGY1LXBhcmFsbGVsIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9v"
declare -x _ModuleTable004_="ZmkvMS4wL2NyYXktbXBpY2gvOC4wL2NyYXktaGRmNS1wYXJhbGxlbC8xLjEyLjIuOS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLApsb2FkT3JkZXIgPSAxNSwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDEyLjAwMDAwMDAwMi4wMDAwMDAwMDkuKnpmaW5hbCIsCn0sClsiY3JheS1saWJwYWxzIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wYWxzL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5LWxpYnBhbHMvMS4zLjQubHVhIiwKZnVsbE5hbWUgPSAiY3JheS1s"
declare -x _ModuleTable005_="aWJwYWxzLzEuMy40IiwKbG9hZE9yZGVyID0gMTIsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1saWJwYWxzIiwKd1YgPSAiXjAwMDAwMDAxLjAwMDAwMDAwMy4wMDAwMDAwMDQuKnpmaW5hbCIsCn0sClsiY3JheS1tcGljaCJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMC9jcmF5LW1waWNoLzguMS4yOC5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LW1waWNoLzguMS4yOCIsCmxvYWRPcmRlciA9IDksCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1tcGljaCIsCndWID0gIl4w"
declare -x _ModuleTable006_="MDAwMDAwOC4wMDAwMDAwMDEuMDAwMDAwMDI4Lip6ZmluYWwiLAp9LApbImNyYXktcGFscyJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGFscy9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wYWxzLzEuMy40Lmx1YSIsCmZ1bGxOYW1lID0gImNyYXktcGFscy8xLjMuNCIsCmxvYWRPcmRlciA9IDExLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcGFscyIsCndWID0gIl4wMDAwMDAwMS4wMDAwMDAwMDMuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImNyYXktcG1pIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wbWkvNi4xLjEzLmx1YSIsCmZ1bGxOYW1lID0gImNy"
declare -x _ModuleTable007_="YXktcG1pLzYuMS4xMyIsCmxvYWRPcmRlciA9IDEwLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcG1pIiwKd1YgPSAiXjAwMDAwMDA2LjAwMDAwMDAwMS4wMDAwMDAwMTMuKnpmaW5hbCIsCn0sCmNyYXlwZSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5cGUvMi43LjMwLmx1YSIsCmZ1bGxOYW1lID0gImNyYXlwZS8yLjcuMzAiLApsb2FkT3JkZXIgPSA3LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZSIsCndWID0gIl4wMDAwMDAwMi4wMDAwMDAwMDcuMDAwMDAwMDMwLip6ZmluYWwiLAp9LApb"
declare -x _ModuleTable008_="ImNyYXlwZS1uZXR3b3JrLW9maSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS1uZXR3b3JrLW9maS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5cGUtbmV0d29yay1vZmkiLApsb2FkT3JkZXIgPSAyLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS1uZXR3b3JrLW9maSIsCndWID0gIk0uKnpmaW5hbCIsCn0sClsiY3JheXBlLXg4Ni1taWxhbiJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS14ODYtbWlsYW4ubHVhIiwKZnVsbE5hbWUgPSAiY3JheXBl"
declare -x _ModuleTable009_="LXg4Ni1taWxhbiIsCmxvYWRPcmRlciA9IDEzLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS14ODYtbWlsYW4iLAp3ViA9ICJNLip6ZmluYWwiLAp9LApjdWRubiA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY3Vkbm4vOS4xLjAubHVhIiwKZnVsbE5hbWUgPSAiY3Vkbm4vOS4xLjAiLApsb2FkT3JkZXIgPSAxNiwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjdWRubi85LjEuMCIsCndWID0gIjAwMDAwMDAwOS4wMDAwMDAwMDEuKnpmaW5hbCIsCn0sCmRhcnNoYW4gPSB7CmZuID0gIi9zb2Z0L3BlcmZ0b29scy9k"
declare -x _ModuleTable010_="YXJzaGFuL2RhcnNoYW4tMy40LjQvc2hhcmUvY3JheXBlLTIueC9tb2R1bGVmaWxlcy9kYXJzaGFuLzMuNC40IiwKZnVsbE5hbWUgPSAiZGFyc2hhbi8zLjQuNCIsCmxvYWRPcmRlciA9IDQsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiZGFyc2hhbiIsCndWID0gIjAwMDAwMDAwMy4wMDAwMDAwMDQuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImdjYy1uYXRpdmUiXSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9nY2MtbmF0aXZlLzEyLjMubHVhIiwKZnVsbE5hbWUgPSAiZ2NjLW5hdGl2ZS8xMi4zIiwKbG9hZE9yZGVyID0gNiwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0"
declare -x _ModuleTable011_="YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJnY2MtbmF0aXZlIiwKd1YgPSAiXjAwMDAwMDEyLjAwMDAwMDAwMy4qemZpbmFsIiwKfSwKbGliZmFicmljID0gewpmbiA9ICIvb3B0L2NyYXkvbW9kdWxlZmlsZXMvbGliZmFicmljLzEuMTUuMi4wIiwKZnVsbE5hbWUgPSAibGliZmFicmljLzEuMTUuMi4wIiwKbG9hZE9yZGVyID0gMSwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJsaWJmYWJyaWMiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDE1LjAwMDAwMDAwMi4qemZpbmFsIiwKfSwKWyJwZXJmdG9vbHMtYmFzZSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb3JlL3BlcmZ0b29s"
declare -x _ModuleTable012_="cy1iYXNlLzIzLjEyLjAubHVhIiwKZnVsbE5hbWUgPSAicGVyZnRvb2xzLWJhc2UvMjMuMTIuMCIsCmxvYWRPcmRlciA9IDMsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAicGVyZnRvb2xzLWJhc2UiLAp3ViA9ICJeMDAwMDAwMjMuMDAwMDAwMDEyLip6ZmluYWwiLAp9LAp4YWx0ID0gewpmbiA9ICIvc29mdC94YWx0L21vZHVsZWZpbGVzL3hhbHQvMy4wLjItMjAyNDA4MjgyMDUwIiwKZnVsbE5hbWUgPSAieGFsdC8zLjAuMi0yMDI0MDgyODIwNTAiLApsb2FkT3JkZXIgPSA1LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gInhhbHQiLAp3ViA9ICJeMDAwMDAw"
declare -x _ModuleTable013_="MDMuMDAwMDAwMDAwLjAwMDAwMDAwMi4qemZpbmFsLS4yMDI0MDgyODIwNTAuKnpmaW5hbCIsCn0sCn0sCm1wYXRoQSA9IHsKCiIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9oZGY1LXBhcmFsbGVsL2dudS8xMi4wL29maS8xLjAvY3JheS1tcGljaC84LjAvY3JheS1oZGY1LXBhcmFsbGVsLzEuMTIuMiIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3B1L3g4Ni1taWxhbi8xLjAiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9vZmkvMS4wL2NyYXktbXBpY2gvOC4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMCIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxl"
declare -x _ModuleTable014_="ZmlsZXMvbWl4X2NvbXBpbGVycyIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29tcGlsZXIvZ251LzEyLjAiLCAiL3NvZnQvbW9kdWxlZmlsZXMiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL3BlcmZ0b29scy8yMy4xMi4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9uZXQvb2ZpLzEuMCIsICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0xpbnV4IgosICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmUiLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZSIKLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BhbHMvbG1vZC9tb2R1bGVmaWxlcy9jb3JlIgosICIvb3B0L2Ny"
declare -x _ModuleTable015_="YXkvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZSIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdCIKLCAiL3NvZnQvcGVyZnRvb2xzL2RhcnNoYW4vZGFyc2hhbi0zLjQuNC9zaGFyZS9jcmF5cGUtMi54L21vZHVsZWZpbGVzIiwgIi9zb2Z0L3hhbHQvbW9kdWxlZmlsZXMiLAp9LApzeXN0ZW1CYXNlTVBBVEggPSAiL3Vzci9zaGFyZS9tb2R1bGVmaWxlcy9MaW51eDovdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmU6L3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZTovdXNyL3NoYXJlL2xtb2QvbG1vZC9tb2R1bGVmaWxlczovb3B0L2NyYXkvcGFscy9sbW9kL21vZHVs"
declare -x _ModuleTable016_="ZWZpbGVzL2NvcmU6L29wdC9jcmF5L21vZHVsZWZpbGVzOi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmU6L29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdDovc29mdC9wZXJmdG9vbHMvZGFyc2hhbi9kYXJzaGFuLTMuNC40L3NoYXJlL2NyYXlwZS0yLngvbW9kdWxlZmlsZXM6L3NvZnQveGFsdC9tb2R1bGVmaWxlcyIsCn0K"
declare -x _ModuleTable_Sz_="16"
declare -x __LMOD_Priority_PATH="/soft/xalt/3.0.2-202408282050/bin:-100"
declare -x __LMOD_REF_COUNT_COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin:1"
declare -x __LMOD_REF_COUNT_CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:1;/opt/cray/pe/pmi/6.1.13/lib:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:1;/opt/cray/pe/mpich/8.1.28/gtl/lib:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib:1;/opt/cray/pe/perftools/23.12.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:1;/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:1;/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:1;/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:1;/soft/perftools/darshan/darshan-3.4.4/lib:1;/opt/cray/pe/papi/7.0.1.2/lib64:1;/opt/cray/libfabric/1.15.2.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so:1"
declare -x __LMOD_REF_COUNT_MANPATH="/opt/cray/pals/1.3.4/man:2;/opt/cray/pe/pmi/6.1.13/man:1;/opt/cray/pe/mpich/8.1.28/ofi/man:1;/opt/cray/pe/mpich/8.1.28/man/mpich:1;/opt/cray/pe/dsmml/0.2.2/dsmml/man:1;/opt/cray/pe/craype/2.7.30/man:1;/opt/cray/pe/perftools/23.12.0/man:1;/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:1;/opt/cray/libfabric/1.15.2.0/share/man:1;/usr/share/lmod/lmod/share/man:1;/home/shourya01/.local/man:1;/usr/local/man:1;/usr/share/man:1;/usr/man:1;/opt/c3/man:1;/opt/pbs/share/man:1;/opt/clmgr/man:1;/opt/sgi/share/man:1;/opt/clmgr/share/man:1;/opt/clmgr/lib/cm-cli/man:1"
declare -x __LMOD_REF_COUNT_MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:1;/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:1;/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:1;/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:1;/opt/cray/pe/lmod/modulefiles/mix_compilers:1;/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:1;/soft/modulefiles:1;/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:1;/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:1;/usr/share/modulefiles/Linux:1;/usr/share/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles:1;/opt/cray/pals/lmod/modulefiles/core:1;/opt/cray/modulefiles:1;/opt/cray/pe/lmod/modulefiles/core:1;/opt/cray/pe/lmod/modulefiles/craype-targets/default:1;/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:1;/soft/xalt/modulefiles:1"
declare -x __LMOD_REF_COUNT_PATH="/soft/xalt/3.0.2-202408282050/bin:1;/soft/compilers/cudatoolkit/cuda-12.4.1/bin:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:1;/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:1;/opt/cray/pe/hdf5/1.12.2.9/bin:1;/opt/cray/pals/1.3.4/bin:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:1;/opt/cray/pe/mpich/8.1.28/bin:1;/opt/cray/pe/craype/2.7.30/bin:1;/home/shourya01/.local/bin:4;/soft/perftools/darshan/darshan-3.4.4/bin:1;/opt/cray/pe/perftools/23.12.0/bin:1;/opt/cray/pe/papi/7.0.1.2/bin:1;/opt/cray/libfabric/1.15.2.0/bin:1;/opt/clmgr/sbin:1;/opt/clmgr/bin:1;/opt/sgi/sbin:1;/opt/sgi/bin:1;/usr/local/bin:1;/usr/bin:1;/bin:2;/opt/c3/bin:1;/usr/lib/mit/bin:1;/usr/lib/mit/sbin:1;/opt/pbs/bin:1;/sbin:1;/home/shourya01/bin:1;/opt/cray/pe/bin:1"
declare -x __LMOD_REF_COUNT_PE_DSMML_PKGCONFIG_LIBS="dsmml:1"
declare -x __LMOD_REF_COUNT_PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:1;hdf5_fortran_parallel:1;mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FIXED_PRGENV="GNU:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_GENCOMPILERS_GNU="12.3:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_PKGCONFIG_LIBS="mpich:1"
declare -x __LMOD_REF_COUNT_PE_PALS_PKGCONFIG_LIBS="libpals:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_LIBS="hdf5_hl_parallel:1;hdf5_parallel:1;mpich:1;dsmml:1;darshan-runtime:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_PRODUCTS="PE_PALS:1;PE_PMI:1;PE_MPICH:1;PE_DSMML:1"
declare -x __LMOD_REF_COUNT_PE_PMI_PKGCONFIG_LIBS="cray-pmi:1"
declare -x __LMOD_REF_COUNT_PE_PRODUCT_LIST="CRAYPE_X86_MILAN:1;PERFTOOLS:1;CRAYPAT:1"
declare -x __LMOD_REF_COUNT_PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:1;/opt/cray/pals/1.3.4/lib/pkgconfig:1;/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:1;/opt/cray/pe/craype/2.7.30/pkg-config:1;/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:1;/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages:1"
declare -x ftp_proxy="http://proxy.alcf.anl.gov:3128"
declare -x http_proxy="http://proxy.alcf.anl.gov:3128"
declare -x https_proxy="http://proxy.alcf.anl.gov:3128"
declare -x no_proxy="admin,polaris-adminvm-01,localhost,*.cm.polaris.alcf.anl.gov,polaris-*,*.polaris.alcf.anl.gov,*.alcf.anl.gov"
Running on 5 nodes
Total number of GPUs: 20
Connected to tcp://x3002c0s19b0n0.hsn.cm.polaris.alcf.anl.gov:7919
Found executable /soft/applications/conda/2024-04-29/mconda3/bin/python
Launching application c482e8bf-ef79-425e-a2bf-bb4b26e5f132
Using PMI port 38843,38844
[2025-06-19 21:20:36,666] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 21:20:36,670] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 21:20:36,670] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 21:20:36,693] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 21:20:36,693] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 21:20:36,693] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 21:20:36,694] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 21:20:36,698] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 21:20:36,808] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 21:20:36,808] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 21:20:36,808] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 21:20:36,808] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 21:20:36,934] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 21:20:36,934] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 21:20:36,934] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 21:20:36,934] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 21:20:36,985] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 21:20:36,985] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 21:20:36,985] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-19 21:20:36,985] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:41,752] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:41,752] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 21:20:41,752] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:41,752] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 21:20:41,752] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-19 21:20:41,752] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:41,752] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 21:20:41,752] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:42,031] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:42,031] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 21:20:42,031] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:42,031] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 21:20:42,031] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-19 21:20:42,031] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:42,031] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 21:20:42,031] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:42,363] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:42,363] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 21:20:42,363] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:42,363] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 21:20:42,363] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:42,364] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 21:20:42,364] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-19 21:20:42,363] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:42,450] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:42,450] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 21:20:42,450] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-19 21:20:42,450] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:42,450] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 21:20:42,450] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:42,450] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 21:20:42,450] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:42,685] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:42,685] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 21:20:42,685] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:42,685] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 21:20:42,685] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-19 21:20:42,685] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-19 21:20:42,685] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=10, local_rank=2, world_size=20, master_addr=10.140.57.7, master_port=29500
[2025-06-19 21:20:42,685] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=11, local_rank=3, world_size=20, master_addr=10.140.57.7, master_port=29500
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=8, local_rank=0, world_size=20, master_addr=10.140.57.7, master_port=29500
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=9, local_rank=1, world_size=20, master_addr=10.140.57.7, master_port=29500
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=1, world_size=20, master_addr=10.140.57.7, master_port=29500
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=12, local_rank=0, world_size=20, master_addr=10.140.57.7, master_port=29500
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=2, world_size=20, master_addr=10.140.57.7, master_port=29500
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=20, master_addr=10.140.57.7, master_port=29500
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=16, local_rank=0, world_size=20, master_addr=10.140.57.7, master_port=29500
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=17, local_rank=1, world_size=20, master_addr=10.140.57.7, master_port=29500
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=3, world_size=20, master_addr=10.140.57.7, master_port=29500
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=13, local_rank=1, world_size=20, master_addr=10.140.57.7, master_port=29500
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=0, world_size=20, master_addr=10.140.57.7, master_port=29500
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=20, master_addr=10.140.57.7, master_port=29500
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=18, local_rank=2, world_size=20, master_addr=10.140.57.7, master_port=29500
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=19, local_rank=3, world_size=20, master_addr=10.140.57.7, master_port=29500
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=20, master_addr=10.140.57.7, master_port=29500
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=14, local_rank=2, world_size=20, master_addr=10.140.57.7, master_port=29500
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=20, master_addr=10.140.57.7, master_port=29500
[2025-06-19 21:20:42,685] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-06-19 21:20:42,685] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=15, local_rank=3, world_size=20, master_addr=10.140.57.7, master_port=29500
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Initialized deepspeed on global rank 0, local rank 0 with world size 20.
[2025-06-19 21:24:58,543] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2+5f631abc, git-hash=5f631abc, git-branch=HEAD
[2025-06-19 21:25:07,649] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-06-19 21:25:07,651] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-06-19 21:25:07,651] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-06-19 21:25:07,665] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-06-19 21:25:07,665] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-06-19 21:25:07,665] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-06-19 21:25:07,665] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-06-19 21:25:07,665] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2025-06-19 21:25:07,666] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2025-06-19 21:25:07,666] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-06-19 21:25:07,666] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-06-19 21:25:07,666] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2025-06-19 21:25:07,666] [INFO] [config.py:1000:print]   amp_params ................... False
[2025-06-19 21:25:07,666] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-06-19 21:25:07,666] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2025-06-19 21:25:07,666] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2025-06-19 21:25:07,666] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2025-06-19 21:25:07,666] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x15060d7d1fd0>
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   dump_state ................... False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   global_rank .................. 0
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 5e-05, 'weight_decay': 0.01}
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   pld_params ................... False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   steps_per_print .............. 100000
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   train_batch_size ............. 2560
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   world_size ................... 20
[2025-06-19 21:25:07,667] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2025-06-19 21:25:07,668] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-06-19 21:25:07,668] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2025-06-19 21:25:07,668] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2025-06-19 21:25:07,668] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2025-06-19 21:25:07,668] [INFO] [config.py:986:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 128, 
    "train_batch_size": 2.560000e+03, 
    "steps_per_print": 1.000000e+05, 
    "gradient_accumulation_steps": 1, 
    "fp16": {
        "enabled": false
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 5e-05, 
            "weight_decay": 0.01
        }
    }, 
    "comms_logger": {
        "enabled": true, 
        "verbose": false
    }, 
    "zero_optimization": {
        "stage": 0
    }
}
Validating lr=5e-05, train epoch 0.:   0%|          | 0/128 [00:00<?, ?it/s]Validating lr=5e-05, train epoch 0.:   1%|          | 1/128 [00:05<11:15,  5.32s/it]Validating lr=5e-05, train epoch 0.:   2%|▏         | 2/128 [00:09<10:06,  4.82s/it]Validating lr=5e-05, train epoch 0.:   2%|▏         | 3/128 [00:14<09:38,  4.63s/it]Validating lr=5e-05, train epoch 0.:   3%|▎         | 4/128 [00:18<09:20,  4.52s/it]Validating lr=5e-05, train epoch 0.:   4%|▍         | 5/128 [00:22<09:08,  4.46s/it]Validating lr=5e-05, train epoch 0.:   5%|▍         | 6/128 [00:27<08:59,  4.42s/it]Validating lr=5e-05, train epoch 0.:   5%|▌         | 7/128 [00:31<08:55,  4.43s/it]Validating lr=5e-05, train epoch 0.:   6%|▋         | 8/128 [00:36<08:50,  4.42s/it]Validating lr=5e-05, train epoch 0.:   7%|▋         | 9/128 [00:40<08:42,  4.39s/it]Validating lr=5e-05, train epoch 0.:   8%|▊         | 10/128 [00:44<08:35,  4.37s/it]Validating lr=5e-05, train epoch 0.:   9%|▊         | 11/128 [00:49<08:29,  4.36s/it]Validating lr=5e-05, train epoch 0.:   9%|▉         | 12/128 [00:53<08:26,  4.37s/it]Validating lr=5e-05, train epoch 0.:  10%|█         | 13/128 [00:57<08:21,  4.36s/it]Validating lr=5e-05, train epoch 0.:  11%|█         | 14/128 [01:02<08:17,  4.37s/it]Validating lr=5e-05, train epoch 0.:  12%|█▏        | 15/128 [01:06<08:12,  4.36s/it]Validating lr=5e-05, train epoch 0.:  12%|█▎        | 16/128 [01:10<08:08,  4.36s/it]Validating lr=5e-05, train epoch 0.:  13%|█▎        | 17/128 [01:15<08:06,  4.38s/it]Validating lr=5e-05, train epoch 0.:  14%|█▍        | 18/128 [01:19<08:02,  4.39s/it]Validating lr=5e-05, train epoch 0.:  15%|█▍        | 19/128 [01:24<07:56,  4.37s/it]Validating lr=5e-05, train epoch 0.:  16%|█▌        | 20/128 [01:28<07:51,  4.36s/it]Validating lr=5e-05, train epoch 0.:  16%|█▋        | 21/128 [01:32<07:47,  4.37s/it]Validating lr=5e-05, train epoch 0.:  17%|█▋        | 22/128 [01:37<07:43,  4.38s/it]Validating lr=5e-05, train epoch 0.:  18%|█▊        | 23/128 [01:41<07:40,  4.39s/it]Validating lr=5e-05, train epoch 0.:  19%|█▉        | 24/128 [01:45<07:36,  4.39s/it]Validating lr=5e-05, train epoch 0.:  20%|█▉        | 25/128 [01:50<07:30,  4.37s/it]Validating lr=5e-05, train epoch 0.:  20%|██        | 26/128 [01:54<07:25,  4.37s/it]Validating lr=5e-05, train epoch 0.:  21%|██        | 27/128 [01:59<07:20,  4.36s/it]Validating lr=5e-05, train epoch 0.:  22%|██▏       | 28/128 [02:03<07:15,  4.36s/it]Validating lr=5e-05, train epoch 0.:  23%|██▎       | 29/128 [02:07<07:11,  4.36s/it]Validating lr=5e-05, train epoch 0.:  23%|██▎       | 30/128 [02:12<07:07,  4.36s/it]Validating lr=5e-05, train epoch 0.:  24%|██▍       | 31/128 [02:16<07:04,  4.38s/it]Validating lr=5e-05, train epoch 0.:  25%|██▌       | 32/128 [02:20<07:00,  4.38s/it]Validating lr=5e-05, train epoch 0.:  26%|██▌       | 33/128 [02:25<06:54,  4.36s/it]Validating lr=5e-05, train epoch 0.:  27%|██▋       | 34/128 [02:29<06:49,  4.36s/it]Validating lr=5e-05, train epoch 0.:  27%|██▋       | 35/128 [02:33<06:46,  4.37s/it]Validating lr=5e-05, train epoch 0.:  28%|██▊       | 36/128 [02:38<06:42,  4.38s/it]Validating lr=5e-05, train epoch 0.:  29%|██▉       | 37/128 [02:42<06:38,  4.38s/it]Validating lr=5e-05, train epoch 0.:  30%|██▉       | 38/128 [02:47<06:34,  4.38s/it]Validating lr=5e-05, train epoch 0.:  30%|███       | 39/128 [02:51<06:31,  4.40s/it]Validating lr=5e-05, train epoch 0.:  31%|███▏      | 40/128 [02:55<06:26,  4.40s/it]Validating lr=5e-05, train epoch 0.:  32%|███▏      | 41/128 [03:00<06:21,  4.39s/it]Validating lr=5e-05, train epoch 0.:  33%|███▎      | 42/128 [03:04<06:16,  4.38s/it]Validating lr=5e-05, train epoch 0.:  34%|███▎      | 43/128 [03:09<06:13,  4.39s/it]Validating lr=5e-05, train epoch 0.:  34%|███▍      | 44/128 [03:13<06:08,  4.39s/it]Validating lr=5e-05, train epoch 0.:  35%|███▌      | 45/128 [03:17<06:03,  4.38s/it]Validating lr=5e-05, train epoch 0.:  36%|███▌      | 46/128 [03:22<05:57,  4.36s/it]Validating lr=5e-05, train epoch 0.:  37%|███▋      | 47/128 [03:26<05:52,  4.35s/it]Validating lr=5e-05, train epoch 0.:  38%|███▊      | 48/128 [03:30<05:47,  4.35s/it]Validating lr=5e-05, train epoch 0.:  38%|███▊      | 49/128 [03:35<05:42,  4.33s/it]Validating lr=5e-05, train epoch 0.:  39%|███▉      | 50/128 [03:39<05:37,  4.33s/it]Validating lr=5e-05, train epoch 0.:  40%|███▉      | 51/128 [03:43<05:34,  4.34s/it]Validating lr=5e-05, train epoch 0.:  41%|████      | 52/128 [03:48<05:29,  4.34s/it]Validating lr=5e-05, train epoch 0.:  41%|████▏     | 53/128 [03:52<05:26,  4.35s/it]Validating lr=5e-05, train epoch 0.:  42%|████▏     | 54/128 [03:56<05:22,  4.35s/it]Validating lr=5e-05, train epoch 0.:  43%|████▎     | 55/128 [04:01<05:19,  4.37s/it]Validating lr=5e-05, train epoch 0.:  44%|████▍     | 56/128 [04:05<05:15,  4.38s/it]Validating lr=5e-05, train epoch 0.:  45%|████▍     | 57/128 [04:10<05:10,  4.38s/it]Validating lr=5e-05, train epoch 0.:  45%|████▌     | 58/128 [04:14<05:06,  4.38s/it]Validating lr=5e-05, train epoch 0.:  46%|████▌     | 59/128 [04:18<05:02,  4.38s/it]Validating lr=5e-05, train epoch 0.:  47%|████▋     | 60/128 [04:23<04:57,  4.38s/it]Validating lr=5e-05, train epoch 0.:  48%|████▊     | 61/128 [04:27<04:52,  4.37s/it]Validating lr=5e-05, train epoch 0.:  48%|████▊     | 62/128 [04:31<04:48,  4.36s/it]Validating lr=5e-05, train epoch 0.:  49%|████▉     | 63/128 [04:36<04:44,  4.37s/it]Validating lr=5e-05, train epoch 0.:  50%|█████     | 64/128 [04:40<04:39,  4.37s/it]Validating lr=5e-05, train epoch 0.:  51%|█████     | 65/128 [04:45<04:35,  4.37s/it]Validating lr=5e-05, train epoch 0.:  52%|█████▏    | 66/128 [04:49<04:30,  4.36s/it]Validating lr=5e-05, train epoch 0.:  52%|█████▏    | 67/128 [04:53<04:24,  4.34s/it]Validating lr=5e-05, train epoch 0.:  53%|█████▎    | 68/128 [04:57<04:19,  4.32s/it]Validating lr=5e-05, train epoch 0.:  54%|█████▍    | 69/128 [05:02<04:14,  4.32s/it]Validating lr=5e-05, train epoch 0.:  55%|█████▍    | 70/128 [05:06<04:11,  4.34s/it]Validating lr=5e-05, train epoch 0.:  55%|█████▌    | 71/128 [05:10<04:06,  4.33s/it]Validating lr=5e-05, train epoch 0.:  56%|█████▋    | 72/128 [05:15<04:02,  4.33s/it]Validating lr=5e-05, train epoch 0.:  57%|█████▋    | 73/128 [05:19<03:57,  4.32s/it]Validating lr=5e-05, train epoch 0.:  58%|█████▊    | 74/128 [05:23<03:54,  4.34s/it]Validating lr=5e-05, train epoch 0.:  59%|█████▊    | 75/128 [05:28<03:50,  4.34s/it]Validating lr=5e-05, train epoch 0.:  59%|█████▉    | 76/128 [05:32<03:46,  4.36s/it]Validating lr=5e-05, train epoch 0.:  60%|██████    | 77/128 [05:37<03:41,  4.35s/it]Validating lr=5e-05, train epoch 0.:  61%|██████    | 78/128 [05:41<03:38,  4.36s/it]Validating lr=5e-05, train epoch 0.:  62%|██████▏   | 79/128 [05:45<03:33,  4.35s/it]Validating lr=5e-05, train epoch 0.:  62%|██████▎   | 80/128 [05:50<03:29,  4.37s/it]Validating lr=5e-05, train epoch 0.:  63%|██████▎   | 81/128 [05:54<03:24,  4.35s/it]Validating lr=5e-05, train epoch 0.:  64%|██████▍   | 82/128 [05:58<03:19,  4.34s/it]Validating lr=5e-05, train epoch 0.:  65%|██████▍   | 83/128 [06:03<03:15,  4.35s/it]Validating lr=5e-05, train epoch 0.:  66%|██████▌   | 84/128 [06:07<03:11,  4.36s/it]Validating lr=5e-05, train epoch 0.:  66%|██████▋   | 85/128 [06:11<03:07,  4.35s/it]Validating lr=5e-05, train epoch 0.:  67%|██████▋   | 86/128 [06:16<03:02,  4.35s/it]Validating lr=5e-05, train epoch 0.:  68%|██████▊   | 87/128 [06:20<02:59,  4.37s/it]Validating lr=5e-05, train epoch 0.:  69%|██████▉   | 88/128 [06:25<02:55,  4.38s/it]Validating lr=5e-05, train epoch 0.:  70%|██████▉   | 89/128 [06:29<02:50,  4.37s/it]Validating lr=5e-05, train epoch 0.:  70%|███████   | 90/128 [06:33<02:46,  4.38s/it]Validating lr=5e-05, train epoch 0.:  71%|███████   | 91/128 [06:38<02:41,  4.37s/it]Validating lr=5e-05, train epoch 0.:  72%|███████▏  | 92/128 [06:42<02:37,  4.36s/it]Validating lr=5e-05, train epoch 0.:  73%|███████▎  | 93/128 [06:46<02:32,  4.36s/it]Validating lr=5e-05, train epoch 0.:  73%|███████▎  | 94/128 [06:51<02:28,  4.38s/it]Validating lr=5e-05, train epoch 0.:  74%|███████▍  | 95/128 [06:55<02:24,  4.37s/it]Validating lr=5e-05, train epoch 0.:  75%|███████▌  | 96/128 [06:59<02:19,  4.35s/it]Validating lr=5e-05, train epoch 0.:  76%|███████▌  | 97/128 [07:04<02:14,  4.35s/it]Validating lr=5e-05, train epoch 0.:  77%|███████▋  | 98/128 [07:08<02:10,  4.34s/it]Validating lr=5e-05, train epoch 0.:  77%|███████▋  | 99/128 [07:12<02:06,  4.36s/it]Validating lr=5e-05, train epoch 0.:  78%|███████▊  | 100/128 [07:17<02:02,  4.36s/it]Validating lr=5e-05, train epoch 0.:  79%|███████▉  | 101/128 [07:21<01:57,  4.36s/it]Validating lr=5e-05, train epoch 0.:  80%|███████▉  | 102/128 [07:26<01:53,  4.36s/it]Validating lr=5e-05, train epoch 0.:  80%|████████  | 103/128 [07:30<01:49,  4.37s/it]Validating lr=5e-05, train epoch 0.:  81%|████████▏ | 104/128 [07:34<01:45,  4.38s/it]Validating lr=5e-05, train epoch 0.:  82%|████████▏ | 105/128 [07:39<01:40,  4.36s/it]Validating lr=5e-05, train epoch 0.:  83%|████████▎ | 106/128 [07:43<01:36,  4.37s/it]Validating lr=5e-05, train epoch 0.:  84%|████████▎ | 107/128 [07:47<01:31,  4.37s/it]Validating lr=5e-05, train epoch 0.:  84%|████████▍ | 108/128 [07:52<01:27,  4.35s/it]Validating lr=5e-05, train epoch 0.:  85%|████████▌ | 109/128 [07:56<01:22,  4.36s/it]Validating lr=5e-05, train epoch 0.:  86%|████████▌ | 110/128 [08:01<01:18,  4.37s/it]Validating lr=5e-05, train epoch 0.:  87%|████████▋ | 111/128 [08:05<01:14,  4.37s/it]Validating lr=5e-05, train epoch 0.:  88%|████████▊ | 112/128 [08:09<01:09,  4.36s/it]Validating lr=5e-05, train epoch 0.:  88%|████████▊ | 113/128 [08:14<01:05,  4.35s/it]Validating lr=5e-05, train epoch 0.:  89%|████████▉ | 114/128 [08:18<01:01,  4.36s/it]Validating lr=5e-05, train epoch 0.:  90%|████████▉ | 115/128 [08:22<00:56,  4.36s/it]Validating lr=5e-05, train epoch 0.:  91%|█████████ | 116/128 [08:27<00:52,  4.36s/it]Validating lr=5e-05, train epoch 0.:  91%|█████████▏| 117/128 [08:31<00:47,  4.35s/it]Validating lr=5e-05, train epoch 0.:  92%|█████████▏| 118/128 [08:35<00:43,  4.37s/it]Validating lr=5e-05, train epoch 0.:  93%|█████████▎| 119/128 [08:40<00:39,  4.36s/it]Validating lr=5e-05, train epoch 0.:  94%|█████████▍| 120/128 [08:44<00:34,  4.36s/it]Validating lr=5e-05, train epoch 0.:  95%|█████████▍| 121/128 [08:48<00:30,  4.36s/it]Validating lr=5e-05, train epoch 0.:  95%|█████████▌| 122/128 [08:53<00:26,  4.36s/it]Validating lr=5e-05, train epoch 0.:  96%|█████████▌| 123/128 [08:57<00:21,  4.36s/it]Validating lr=5e-05, train epoch 0.:  97%|█████████▋| 124/128 [09:02<00:17,  4.36s/it]Validating lr=5e-05, train epoch 0.:  98%|█████████▊| 125/128 [09:06<00:13,  4.37s/it]Validating lr=5e-05, train epoch 0.:  98%|█████████▊| 126/128 [09:10<00:08,  4.37s/it]Validating lr=5e-05, train epoch 0.:  99%|█████████▉| 127/128 [09:15<00:04,  4.35s/it]Validating lr=5e-05, train epoch 0.: 100%|██████████| 128/128 [09:19<00:00,  4.36s/it]Validating lr=5e-05, train epoch 0.: 100%|██████████| 128/128 [09:19<00:00,  4.37s/it]
Validating lr=5e-05, train epoch 1.:   0%|          | 0/128 [00:00<?, ?it/s]Validating lr=5e-05, train epoch 1.:   1%|          | 1/128 [00:04<09:15,  4.38s/it]Validating lr=5e-05, train epoch 1.:   2%|▏         | 2/128 [00:08<09:08,  4.35s/it]Validating lr=5e-05, train epoch 1.:   2%|▏         | 3/128 [00:13<09:03,  4.35s/it]Validating lr=5e-05, train epoch 1.:   3%|▎         | 4/128 [00:17<08:57,  4.34s/it]Validating lr=5e-05, train epoch 1.:   4%|▍         | 5/128 [00:21<08:56,  4.36s/it]Validating lr=5e-05, train epoch 1.:   5%|▍         | 6/128 [00:26<08:55,  4.39s/it]Validating lr=5e-05, train epoch 1.:   5%|▌         | 7/128 [00:30<08:48,  4.37s/it]Validating lr=5e-05, train epoch 1.:   6%|▋         | 8/128 [00:34<08:46,  4.39s/it]Validating lr=5e-05, train epoch 1.:   7%|▋         | 9/128 [00:39<08:43,  4.40s/it]Validating lr=5e-05, train epoch 1.:   8%|▊         | 10/128 [00:43<08:37,  4.38s/it]Validating lr=5e-05, train epoch 1.:   9%|▊         | 11/128 [00:48<08:31,  4.37s/it]Validating lr=5e-05, train epoch 1.:   9%|▉         | 12/128 [00:52<08:25,  4.36s/it]Validating lr=5e-05, train epoch 1.:  10%|█         | 13/128 [00:56<08:21,  4.36s/it]Validating lr=5e-05, train epoch 1.:  11%|█         | 14/128 [01:01<08:18,  4.37s/it]Validating lr=5e-05, train epoch 1.:  12%|█▏        | 15/128 [01:05<08:15,  4.39s/it]Validating lr=5e-05, train epoch 1.:  12%|█▎        | 16/128 [01:09<08:10,  4.38s/it]Validating lr=5e-05, train epoch 1.:  13%|█▎        | 17/128 [01:14<08:06,  4.38s/it]Validating lr=5e-05, train epoch 1.:  14%|█▍        | 18/128 [01:18<08:02,  4.39s/it]Validating lr=5e-05, train epoch 1.:  15%|█▍        | 19/128 [01:23<08:00,  4.41s/it]Validating lr=5e-05, train epoch 1.:  16%|█▌        | 20/128 [01:27<07:55,  4.40s/it]Validating lr=5e-05, train epoch 1.:  16%|█▋        | 21/128 [01:31<07:50,  4.40s/it]Validating lr=5e-05, train epoch 1.:  17%|█▋        | 22/128 [01:36<07:45,  4.40s/it]Validating lr=5e-05, train epoch 1.:  18%|█▊        | 23/128 [01:40<07:39,  4.38s/it]Validating lr=5e-05, train epoch 1.:  19%|█▉        | 24/128 [01:45<07:34,  4.37s/it]Validating lr=5e-05, train epoch 1.:  20%|█▉        | 25/128 [01:49<07:30,  4.37s/it]Validating lr=5e-05, train epoch 1.:  20%|██        | 26/128 [01:53<07:24,  4.36s/it]Validating lr=5e-05, train epoch 1.:  21%|██        | 27/128 [01:58<07:20,  4.36s/it]Validating lr=5e-05, train epoch 1.:  22%|██▏       | 28/128 [02:02<07:15,  4.36s/it]Validating lr=5e-05, train epoch 1.:  23%|██▎       | 29/128 [02:06<07:11,  4.36s/it]Validating lr=5e-05, train epoch 1.:  23%|██▎       | 30/128 [02:11<07:06,  4.36s/it]Validating lr=5e-05, train epoch 1.:  24%|██▍       | 31/128 [02:15<07:03,  4.36s/it]Validating lr=5e-05, train epoch 1.:  25%|██▌       | 32/128 [02:19<06:58,  4.36s/it]Validating lr=5e-05, train epoch 1.:  26%|██▌       | 33/128 [02:24<06:52,  4.35s/it]Validating lr=5e-05, train epoch 1.:  27%|██▋       | 34/128 [02:28<06:48,  4.35s/it]Validating lr=5e-05, train epoch 1.:  27%|██▋       | 35/128 [02:32<06:42,  4.33s/it]Validating lr=5e-05, train epoch 1.:  28%|██▊       | 36/128 [02:37<06:40,  4.35s/it]Validating lr=5e-05, train epoch 1.:  29%|██▉       | 37/128 [02:41<06:35,  4.35s/it]Validating lr=5e-05, train epoch 1.:  30%|██▉       | 38/128 [02:45<06:30,  4.34s/it]Validating lr=5e-05, train epoch 1.:  30%|███       | 39/128 [02:50<06:29,  4.37s/it]Validating lr=5e-05, train epoch 1.:  31%|███▏      | 40/128 [02:54<06:25,  4.38s/it]Validating lr=5e-05, train epoch 1.:  32%|███▏      | 41/128 [02:59<06:21,  4.39s/it]Validating lr=5e-05, train epoch 1.:  33%|███▎      | 42/128 [03:03<06:18,  4.40s/it]Validating lr=5e-05, train epoch 1.:  34%|███▎      | 43/128 [03:07<06:12,  4.39s/it]Validating lr=5e-05, train epoch 1.:  34%|███▍      | 44/128 [03:12<06:07,  4.37s/it]Validating lr=5e-05, train epoch 1.:  35%|███▌      | 45/128 [03:16<06:03,  4.37s/it]Validating lr=5e-05, train epoch 1.:  36%|███▌      | 46/128 [03:21<05:57,  4.36s/it]Validating lr=5e-05, train epoch 1.:  37%|███▋      | 47/128 [03:25<05:53,  4.36s/it]Validating lr=5e-05, train epoch 1.:  38%|███▊      | 48/128 [03:29<05:48,  4.36s/it]Validating lr=5e-05, train epoch 1.:  38%|███▊      | 49/128 [03:34<05:44,  4.36s/it]Validating lr=5e-05, train epoch 1.:  39%|███▉      | 50/128 [03:38<05:40,  4.37s/it]Validating lr=5e-05, train epoch 1.:  40%|███▉      | 51/128 [03:42<05:37,  4.38s/it]Validating lr=5e-05, train epoch 1.:  41%|████      | 52/128 [03:47<05:33,  4.39s/it]Validating lr=5e-05, train epoch 1.:  41%|████▏     | 53/128 [03:51<05:29,  4.39s/it]Validating lr=5e-05, train epoch 1.:  42%|████▏     | 54/128 [03:56<05:24,  4.38s/it]Validating lr=5e-05, train epoch 1.:  43%|████▎     | 55/128 [04:00<05:18,  4.37s/it]Validating lr=5e-05, train epoch 1.:  44%|████▍     | 56/128 [04:04<05:13,  4.36s/it]Validating lr=5e-05, train epoch 1.:  45%|████▍     | 57/128 [04:09<05:08,  4.35s/it]Validating lr=5e-05, train epoch 1.:  45%|████▌     | 58/128 [04:13<05:04,  4.35s/it]Validating lr=5e-05, train epoch 1.:  46%|████▌     | 59/128 [04:17<05:00,  4.36s/it]Validating lr=5e-05, train epoch 1.:  47%|████▋     | 60/128 [04:22<04:55,  4.34s/it]Validating lr=5e-05, train epoch 1.:  48%|████▊     | 61/128 [04:26<04:51,  4.36s/it]Validating lr=5e-05, train epoch 1.:  48%|████▊     | 62/128 [04:30<04:48,  4.37s/it]Validating lr=5e-05, train epoch 1.:  49%|████▉     | 63/128 [04:35<04:44,  4.38s/it]Validating lr=5e-05, train epoch 1.:  50%|█████     | 64/128 [04:39<04:39,  4.37s/it]Validating lr=5e-05, train epoch 1.:  51%|█████     | 65/128 [04:44<04:35,  4.38s/it]Validating lr=5e-05, train epoch 1.:  52%|█████▏    | 66/128 [04:48<04:31,  4.38s/it]Validating lr=5e-05, train epoch 1.:  52%|█████▏    | 67/128 [04:52<04:28,  4.40s/it]Validating lr=5e-05, train epoch 1.:  53%|█████▎    | 68/128 [04:57<04:23,  4.39s/it]Validating lr=5e-05, train epoch 1.:  54%|█████▍    | 69/128 [05:01<04:18,  4.38s/it]Validating lr=5e-05, train epoch 1.:  55%|█████▍    | 70/128 [05:05<04:14,  4.39s/it]Validating lr=5e-05, train epoch 1.:  55%|█████▌    | 71/128 [05:10<04:09,  4.38s/it]Validating lr=5e-05, train epoch 1.:  56%|█████▋    | 72/128 [05:14<04:05,  4.38s/it]Validating lr=5e-05, train epoch 1.:  57%|█████▋    | 73/128 [05:19<04:00,  4.38s/it]Validating lr=5e-05, train epoch 1.:  58%|█████▊    | 74/128 [05:23<03:57,  4.40s/it]Validating lr=5e-05, train epoch 1.:  59%|█████▊    | 75/128 [05:27<03:53,  4.41s/it]Validating lr=5e-05, train epoch 1.:  59%|█████▉    | 76/128 [05:32<03:48,  4.40s/it]Validating lr=5e-05, train epoch 1.:  60%|██████    | 77/128 [05:36<03:42,  4.37s/it]Validating lr=5e-05, train epoch 1.:  61%|██████    | 78/128 [05:41<03:38,  4.37s/it]Validating lr=5e-05, train epoch 1.:  62%|██████▏   | 79/128 [05:45<03:33,  4.35s/it]Validating lr=5e-05, train epoch 1.:  62%|██████▎   | 80/128 [05:49<03:28,  4.35s/it]Validating lr=5e-05, train epoch 1.:  63%|██████▎   | 81/128 [05:54<03:24,  4.35s/it]Validating lr=5e-05, train epoch 1.:  64%|██████▍   | 82/128 [05:58<03:19,  4.34s/it]Validating lr=5e-05, train epoch 1.:  65%|██████▍   | 83/128 [06:02<03:15,  4.35s/it]Validating lr=5e-05, train epoch 1.:  66%|██████▌   | 84/128 [06:07<03:11,  4.36s/it]Validating lr=5e-05, train epoch 1.:  66%|██████▋   | 85/128 [06:11<03:07,  4.36s/it]Validating lr=5e-05, train epoch 1.:  67%|██████▋   | 86/128 [06:15<03:03,  4.36s/it]Validating lr=5e-05, train epoch 1.:  68%|██████▊   | 87/128 [06:20<02:59,  4.37s/it]Validating lr=5e-05, train epoch 1.:  69%|██████▉   | 88/128 [06:24<02:54,  4.36s/it]Validating lr=5e-05, train epoch 1.:  70%|██████▉   | 89/128 [06:28<02:49,  4.35s/it]Validating lr=5e-05, train epoch 1.:  70%|███████   | 90/128 [06:33<02:45,  4.36s/it]Validating lr=5e-05, train epoch 1.:  71%|███████   | 91/128 [06:37<02:41,  4.36s/it]Validating lr=5e-05, train epoch 1.:  72%|███████▏  | 92/128 [06:42<02:36,  4.36s/it]Validating lr=5e-05, train epoch 1.:  73%|███████▎  | 93/128 [06:46<02:32,  4.36s/it]Validating lr=5e-05, train epoch 1.:  73%|███████▎  | 94/128 [06:50<02:28,  4.36s/it]Validating lr=5e-05, train epoch 1.:  74%|███████▍  | 95/128 [06:55<02:24,  4.38s/it]Validating lr=5e-05, train epoch 1.:  75%|███████▌  | 96/128 [06:59<02:20,  4.39s/it]Validating lr=5e-05, train epoch 1.:  76%|███████▌  | 97/128 [07:04<02:16,  4.41s/it]Validating lr=5e-05, train epoch 1.:  77%|███████▋  | 98/128 [07:08<02:11,  4.38s/it]Validating lr=5e-05, train epoch 1.:  77%|███████▋  | 99/128 [07:12<02:06,  4.38s/it]Validating lr=5e-05, train epoch 1.:  78%|███████▊  | 100/128 [07:17<02:02,  4.37s/it]Validating lr=5e-05, train epoch 1.:  79%|███████▉  | 101/128 [07:21<01:57,  4.36s/it]Validating lr=5e-05, train epoch 1.:  80%|███████▉  | 102/128 [07:25<01:53,  4.36s/it]Validating lr=5e-05, train epoch 1.:  80%|████████  | 103/128 [07:30<01:48,  4.35s/it]Validating lr=5e-05, train epoch 1.:  81%|████████▏ | 104/128 [07:34<01:44,  4.34s/it]Validating lr=5e-05, train epoch 1.:  82%|████████▏ | 105/128 [07:38<01:40,  4.35s/it]Validating lr=5e-05, train epoch 1.:  83%|████████▎ | 106/128 [07:43<01:35,  4.35s/it]Validating lr=5e-05, train epoch 1.:  84%|████████▎ | 107/128 [07:47<01:31,  4.35s/it]Validating lr=5e-05, train epoch 1.:  84%|████████▍ | 108/128 [07:51<01:27,  4.35s/it]Validating lr=5e-05, train epoch 1.:  85%|████████▌ | 109/128 [07:56<01:23,  4.39s/it]Validating lr=5e-05, train epoch 1.:  86%|████████▌ | 110/128 [08:00<01:19,  4.39s/it]Validating lr=5e-05, train epoch 1.:  87%|████████▋ | 111/128 [08:05<01:14,  4.38s/it]Validating lr=5e-05, train epoch 1.:  88%|████████▊ | 112/128 [08:09<01:10,  4.39s/it]Validating lr=5e-05, train epoch 1.:  88%|████████▊ | 113/128 [08:13<01:05,  4.37s/it]Validating lr=5e-05, train epoch 1.:  89%|████████▉ | 114/128 [08:18<01:01,  4.37s/it]Validating lr=5e-05, train epoch 1.:  90%|████████▉ | 115/128 [08:22<00:56,  4.38s/it]Validating lr=5e-05, train epoch 1.:  91%|█████████ | 116/128 [08:26<00:52,  4.38s/it]Validating lr=5e-05, train epoch 1.:  91%|█████████▏| 117/128 [08:31<00:48,  4.39s/it]Validating lr=5e-05, train epoch 1.:  92%|█████████▏| 118/128 [08:35<00:43,  4.38s/it]Validating lr=5e-05, train epoch 1.:  93%|█████████▎| 119/128 [08:40<00:39,  4.38s/it]Validating lr=5e-05, train epoch 1.:  94%|█████████▍| 120/128 [08:44<00:35,  4.38s/it]Validating lr=5e-05, train epoch 1.:  95%|█████████▍| 121/128 [08:48<00:30,  4.39s/it]Validating lr=5e-05, train epoch 1.:  95%|█████████▌| 122/128 [08:53<00:26,  4.40s/it]Validating lr=5e-05, train epoch 1.:  96%|█████████▌| 123/128 [08:57<00:21,  4.40s/it]Validating lr=5e-05, train epoch 1.:  97%|█████████▋| 124/128 [09:02<00:17,  4.38s/it]Validating lr=5e-05, train epoch 1.:  98%|█████████▊| 125/128 [09:06<00:13,  4.38s/it]Validating lr=5e-05, train epoch 1.:  98%|█████████▊| 126/128 [09:10<00:08,  4.38s/it]Validating lr=5e-05, train epoch 1.:  99%|█████████▉| 127/128 [09:15<00:04,  4.38s/it]Validating lr=5e-05, train epoch 1.: 100%|██████████| 128/128 [09:19<00:00,  4.36s/it]Validating lr=5e-05, train epoch 1.: 100%|██████████| 128/128 [09:19<00:00,  4.37s/it]
Evaluating for lr=5e-05:   0%|          | 0/13 [00:00<?, ?it/s]Evaluating for lr=5e-05:   8%|▊         | 1/13 [00:01<00:21,  1.79s/it]Evaluating for lr=5e-05:  15%|█▌        | 2/13 [00:03<00:19,  1.80s/it]Evaluating for lr=5e-05:  23%|██▎       | 3/13 [00:05<00:18,  1.82s/it]Evaluating for lr=5e-05:  31%|███       | 4/13 [00:07<00:16,  1.82s/it]Evaluating for lr=5e-05:  38%|███▊      | 5/13 [00:09<00:14,  1.83s/it]Evaluating for lr=5e-05:  46%|████▌     | 6/13 [00:10<00:12,  1.84s/it]Evaluating for lr=5e-05:  54%|█████▍    | 7/13 [00:12<00:10,  1.83s/it]Evaluating for lr=5e-05:  62%|██████▏   | 8/13 [00:14<00:09,  1.81s/it]Evaluating for lr=5e-05:  69%|██████▉   | 9/13 [00:16<00:07,  1.82s/it]Evaluating for lr=5e-05:  77%|███████▋  | 10/13 [00:18<00:05,  1.83s/it]Evaluating for lr=5e-05:  85%|████████▍ | 11/13 [00:20<00:03,  1.84s/it]Evaluating for lr=5e-05:  92%|█████████▏| 12/13 [00:21<00:01,  1.83s/it]Evaluating for lr=5e-05: 100%|██████████| 13/13 [00:23<00:00,  1.83s/it]Evaluating for lr=5e-05: 100%|██████████| 13/13 [00:23<00:00,  1.83s/it]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/shourya01/stormer_deepspeed/train2.py", line 292, in <module>
[rank0]:     results_tuning.to_csv(str(Path(args.results_base_dir) / Path(args.experiment_name) / Path(args.tuning_file)))
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/util/_decorators.py", line 333, in wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/core/generic.py", line 3967, in to_csv
[rank0]:     return DataFrameRenderer(formatter).to_csv(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/io/formats/format.py", line 1014, in to_csv
[rank0]:     csv_formatter.save()
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/io/formats/csvs.py", line 251, in save
[rank0]:     with get_handle(
[rank0]:          ^^^^^^^^^^^
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/io/common.py", line 749, in get_handle
[rank0]:     check_parent_directory(str(handle))
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/pandas/io/common.py", line 616, in check_parent_directory
[rank0]:     raise OSError(rf"Cannot save file into a non-existent directory: '{parent}'")
[rank0]: OSError: Cannot save file into a non-existent directory: '/eagle/ParaLLMs/weather_load_forecasting/results/california-timesfm-stormer'
[rank6]:[E ProcessGroupNCCL.cpp:563] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800002 milliseconds before timing out.
[rank6]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 6] Timeout at NCCL work: 271, last enqueued NCCL work: 271, last completed NCCL work: 270.
[rank6]:[E ProcessGroupNCCL.cpp:577] [Rank 6] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank6]:[E ProcessGroupNCCL.cpp:583] [Rank 6] To avoid data inconsistency, we are taking the entire process down.
[rank6]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 6] Process group watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800002 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c8d415e6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c89a32d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14c89a306b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14c89a307035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14c89a307e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14c8fac84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14c8ffffa6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14c8ffdba50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 6] Process group watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800002 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c8d415e6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c89a32d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14c89a306b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14c89a307035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14c89a307e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14c8fac84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14c8ffffa6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14c8ffdba50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c8d415e6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c89a32d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14c899fc7d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14c8fac84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14c8ffffa6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14c8ffdba50f in /lib64/libc.so.6)

[rank17]:[E ProcessGroupNCCL.cpp:563] [Rank 17] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800015 milliseconds before timing out.
[rank9]:[E ProcessGroupNCCL.cpp:563] [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800014 milliseconds before timing out.
[rank9]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 9] Timeout at NCCL work: 271, last enqueued NCCL work: 271, last completed NCCL work: 270.
[rank9]:[E ProcessGroupNCCL.cpp:577] [Rank 9] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank9]:[E ProcessGroupNCCL.cpp:583] [Rank 9] To avoid data inconsistency, we are taking the entire process down.
[rank9]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 9] Process group watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800014 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1520cc1fd6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15208972d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152089706b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x152089707035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x152089707e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1520ea084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1520ef3886ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1520ef14850f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 9] Process group watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800014 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1520cc1fd6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15208972d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x152089706b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x152089707035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x152089707e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1520ea084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1520ef3886ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1520ef14850f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1520cc1fd6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15208972d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1520893c7d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1520ea084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1520ef3886ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1520ef14850f in /lib64/libc.so.6)

[rank7]:[E ProcessGroupNCCL.cpp:563] [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800021 milliseconds before timing out.
[rank18]:[E ProcessGroupNCCL.cpp:563] [Rank 18] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800029 milliseconds before timing out.
[rank7]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 7] Timeout at NCCL work: 271, last enqueued NCCL work: 271, last completed NCCL work: 270.
[rank7]:[E ProcessGroupNCCL.cpp:577] [Rank 7] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank7]:[E ProcessGroupNCCL.cpp:583] [Rank 7] To avoid data inconsistency, we are taking the entire process down.
[rank7]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800021 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1471ac0846f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1471679db4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1471679b4b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1471679b5035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1471679b5e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1471c8a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1471cdf1e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1471cdcde50f in /lib64/libc.so.6)

[rank15]:[E ProcessGroupNCCL.cpp:563] [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800022 milliseconds before timing out.
[rank14]:[E ProcessGroupNCCL.cpp:563] [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800023 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800021 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1471ac0846f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1471679db4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1471679b4b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1471679b5035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1471679b5e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1471c8a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1471cdf1e6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1471cdcde50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1471ac0846f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1471679db4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x147167675d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1471c8a84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1471cdf1e6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1471cdcde50f in /lib64/libc.so.6)

[rank17]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 17] Timeout at NCCL work: 271, last enqueued NCCL work: 271, last completed NCCL work: 270.
[rank17]:[E ProcessGroupNCCL.cpp:577] [Rank 17] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank17]:[E ProcessGroupNCCL.cpp:583] [Rank 17] To avoid data inconsistency, we are taking the entire process down.
[rank18]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 18] Timeout at NCCL work: 271, last enqueued NCCL work: 271, last completed NCCL work: 270.
[rank18]:[E ProcessGroupNCCL.cpp:577] [Rank 18] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank18]:[E ProcessGroupNCCL.cpp:583] [Rank 18] To avoid data inconsistency, we are taking the entire process down.
[rank17]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 17] Process group watchdog thread terminated with exception: [Rank 17] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800015 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d4810846f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d42c6eb4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d42c6c4b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d42c6c5035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d42c6c5e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d48da84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d492f1d6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d492cdd50f in /lib64/libc.so.6)

[rank18]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 18] Process group watchdog thread terminated with exception: [Rank 18] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800029 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c60028b6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c5b165b4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14c5b1634b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14c5b1635035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14c5b1635e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14c612c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14c61812b6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14c617eeb50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 17] Process group watchdog thread terminated with exception: [Rank 17] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800015 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d4810846f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d42c6eb4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d42c6c4b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d42c6c5035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d42c6c5e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d48da84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d492f1d6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d492cdd50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d4810846f9 in /soft/applicatioterminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 18] Process group watchdog thread terminated with exception: [Rank 18] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800029 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c60028b6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c5b165b4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14c5b1634b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14c5b1635035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14c5b1635e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14c612c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14c61812b6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14c617eeb50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::chans/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d42c6eb4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14d42c385d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14d48da84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14d492f1d6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14d492cdd50f in /lib64/libc.so.6)

r_traits<char>, std::allocator<char> >) + 0xa9 (0x14c60028b6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c5b165b4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14c5b12f5d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14c612c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14c61812b6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14c617eeb50f in /lib64/libc.so.6)

[rank5]:[E ProcessGroupNCCL.cpp:563] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800036 milliseconds before timing out.
[rank5]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 5] Timeout at NCCL work: 271, last enqueued NCCL work: 271, last completed NCCL work: 270.
[rank5]:[E ProcessGroupNCCL.cpp:577] [Rank 5] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank5]:[E ProcessGroupNCCL.cpp:583] [Rank 5] To avoid data inconsistency, we are taking the entire process down.
[rank5]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 5] Process group watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800036 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x145e501266f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x145dfb72d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x145dfb706b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x145dfb707035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x145dfb707e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x145e5c084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x145e613be6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x145e6117e50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 5] Process group watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800036 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x145e501266f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x145dfb72d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x145dfb706b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x145dfb707035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x145dfb707e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x145e5c084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x145e613be6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x145e6117e50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x145e501266f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x145dfb72d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x145dfb3c7d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x145e5c084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x145e613be6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x145e6117e50f in /lib64/libc.so.6)

[rank8]:[E ProcessGroupNCCL.cpp:563] [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800039 milliseconds before timing out.
[rank8]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 8] Timeout at NCCL work: 271, last enqueued NCCL work: 271, last completed NCCL work: 270.
[rank8]:[E ProcessGroupNCCL.cpp:577] [Rank 8] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank8]:[E ProcessGroupNCCL.cpp:583] [Rank 8] To avoid data inconsistency, we are taking the entire process down.
[rank8]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 8] Process group watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800039 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15532cb036f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1552e965b4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1552e9634b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1552e9635035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1552e9635e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x15534aa84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15534fedc6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15534fc9c50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 8] Process group watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800039 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15532cb036f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1552e965b4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1552e9634b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1552e9635035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1552e9635e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x15534aa84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x15534fedc6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x15534fc9c50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x15532cb036f9 in /soft/applications[rank14]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 14] Timeout at NCCL work: 271, last enqueued NCCL work: 271, last completed NCCL work: 270.
[rank14]:[E ProcessGroupNCCL.cpp:577] [Rank 14] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank14]:[E ProcessGroupNCCL.cpp:583] [Rank 14] To avoid data inconsistency, we are taking the entire process down.
[rank14]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 14] Process group watchdog thread terminated with exception: [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800023 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d6986b96f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d6427804a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d642759b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d64275a035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d64275ae6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d6a6590e95 in /soft/applications/conda/2024-04-29/mc/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1552e965b4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1552e92f5d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x15534aa84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x15534fedc6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x15534fc9c50f in /lib64/libc.so.6)

[rank15]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 15] Timeout at NCCL work: 271, last enqueued NCCL work: 271, last completed NCCL work: 270.
[rank15]:[E ProcessGroupNCCL.cpp:577] [Rank 15] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank15]:[E ProcessGroupNCCL.cpp:583] [Rank 15] To avoid data inconsistency, we are taking the entire process down.
[rank15]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 15] Process group watchdog thread terminated with exception: [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800022 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a9c51016f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a9766eb4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14a9766c4b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14a9766c5035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14a9766c5e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14a9d7c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d6a99526ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d6a971250f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 14] Process group watchdog thread terminated with exception: [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800023 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d6986b96f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d6427804a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d642759b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d64275a035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d64275ae6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d6a6590e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d6a99526ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d6a971250f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/toonda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14a9dcfc06ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14a9dcd8050f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 15] Process group watchdog thread terminated with exception: [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800022 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a9c51016f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a9766eb4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14a9766c4b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14a9766c5035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14a9766c5e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14a9d7c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14a9dcfc06ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14a9dcd8050f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d6986b96f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d6427804a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14d64241ad40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14d6a6590e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14d6a99526ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14d6a971250f in /lib64/libc.so.6)

rch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14a9c51016f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14a9766eb4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14a976385d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14a9d7c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14a9dcfc06ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14a9dcd8050f in /lib64/libc.so.6)

[rank1]:[E ProcessGroupNCCL.cpp:563] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800055 milliseconds before timing out.
[rank1]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 1] Timeout at NCCL work: 271, last enqueued NCCL work: 271, last completed NCCL work: 270.
[rank1]:[E ProcessGroupNCCL.cpp:577] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E ProcessGroupNCCL.cpp:583] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800055 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1462381036f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1461e27804a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1461e2759b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1461e275a035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1461e275ae6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146244284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1462495aa6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14624936a50f in /lib64/libc.so.6)

[rank16]:[E ProcessGroupNCCL.cpp:563] [Rank 16] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800067 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800055 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1462381036f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1461e27804a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x1461e2759b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x1461e275a035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x1461e275ae6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x146244284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1462495aa6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14624936a50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1462381036f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x1461e27804a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x1461e241ad40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x146244284e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1462495aa6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14624936a50f in /lib64/libc.so.6)

[rank16]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 16] Timeout at NCCL work: 271, last enqueued NCCL work: 271, last completed NCCL work: 270.
[rank16]:[E ProcessGroupNCCL.cpp:577] [Rank 16] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank16]:[E ProcessGroupNCCL.cpp:583] [Rank 16] To avoid data inconsistency, we are taking the entire process down.
[rank16]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 16] Process group watchdog thread terminated with exception: [Rank 16] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800067 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d07839d6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d02165b4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d021634b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d021635035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d021635e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d083884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d088c586ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d088a1850f in /lib64/libc.so.6)

[rank12]:[E ProcessGroupNCCL.cpp:563] [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800062 milliseconds before timing out.
terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 16] Process group watchdog thread terminated with exception: [Rank 16] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800067 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d07839d6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d02165b4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d021634b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d021635035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d021635e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d083884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d088c586ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d088a1850f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d07839d6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d02165b4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14d0212f5d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14d083884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14d088c586ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14d088a1850f in /lib64/libc.so.6)

[rank12]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 12] Timeout at NCCL work: 271, last enqueued NCCL work: 271, last completed NCCL work: 270.
[rank12]:[E ProcessGroupNCCL.cpp:577] [Rank 12] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank12]:[E ProcessGroupNCCL.cpp:583] [Rank 12] To avoid data inconsistency, we are taking the entire process down.
[rank12]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 12] Process group watchdog thread terminated with exception: [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800062 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14aa092726f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14aa0a32d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14aa0a306b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14aa0a307035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14aa0a307e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14aa6a884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14aa6fd136ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14aa6fad350f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 12] Process group watchdog thread terminated with exception: [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800062 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14aa092726f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14aa0a32d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14aa0a306b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14aa0a307035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14aa0a307e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14aa6a884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14aa6fd136ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14aa6fad350f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14aa092726f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14aa0a32d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14aa09fc7d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14aa6a884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14aa6fd136ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14aa6fad350f in /lib64/libc.so.6)

[rank3]:[E ProcessGroupNCCL.cpp:563] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800080 milliseconds before timing out.
[rank3]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 3] Timeout at NCCL work: 271, last enqueued NCCL work: 271, last completed NCCL work: 270.
[rank3]:[E ProcessGroupNCCL.cpp:577] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E ProcessGroupNCCL.cpp:583] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank3]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800080 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14be60a8d6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14be0f9db4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14be0f9b4b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14be0f9b5035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14be0f9b5e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14be70884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14be75c196ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14be759d950f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800080 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14be60a8d6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14be0f9db4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14be0f9b4b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14be0f9b5035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14be0f9b5e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14be70884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14be75c196ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14be759d950f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14be60a8d6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14be0f9db4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14be0f675d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14be70884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14be75c196ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14be759d950f in /lib64/libc.so.6)

[rank19]:[E ProcessGroupNCCL.cpp:563] [Rank 19] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800083 milliseconds before timing out.
[rank19]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 19] Timeout at NCCL work: 271, last enqueued NCCL work: 271, last completed NCCL work: 270.
[rank19]:[E ProcessGroupNCCL.cpp:577] [Rank 19] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank19]:[E ProcessGroupNCCL.cpp:583] [Rank 19] To avoid data inconsistency, we are taking the entire process down.
[rank19]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 19] Process group watchdog thread terminated with exception: [Rank 19] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800083 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b594ac16f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b57c32d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14b57c306b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14b57c307035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14b57c307e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14b5dca84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14b5e1db56ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14b5e1b7550f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 19] Process group watchdog thread terminated with exception: [Rank 19] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800083 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b594ac16f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b57c32d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14b57c306b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14b57c307035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14b57c307e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14b5dca84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14b5e1db56ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14b5e1b7550f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14b594ac16f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14b57c32d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14b57bfc7d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14b5dca84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14b5e1db56ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14b5e1b7550f in /lib64/libc.so.6)

[rank11]:[E ProcessGroupNCCL.cpp:563] [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800082 milliseconds before timing out.
[rank11]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 11] Timeout at NCCL work: 271, last enqueued NCCL work: 271, last completed NCCL work: 270.
[rank11]:[E ProcessGroupNCCL.cpp:577] [Rank 11] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank11]:[E ProcessGroupNCCL.cpp:583] [Rank 11] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E ProcessGroupNCCL.cpp:563] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800091 milliseconds before timing out.
[rank11]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 11] Process group watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800082 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d3841c36f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d33632d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d336306b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d336307035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d336307e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d396c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d39c06d6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d39be2d50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 11] Process group watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800082 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d3841c36f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d33632d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14d336306b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14d336307035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14d336307e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14d396c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14d39c06d6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14d39be2d50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14d3841c36f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14d33632d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14d335fc7d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14d396c84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14d39c06d6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14d39be2d50f in /lib64/libc.so.6)

[rank2]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 2] Timeout at NCCL work: 271, last enqueued NCCL work: 271, last completed NCCL work: 270.
[rank2]:[E ProcessGroupNCCL.cpp:577] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E ProcessGroupNCCL.cpp:583] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800091 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14df5a39c6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14df5b4f14a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14df5b4cab61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14df5b4cb035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14df5b4cbe6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14dfbba84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14dfc0dc86ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14dfc0b8850f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800091 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14df5a39c6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14df5b4f14a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14df5b4cab61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14df5b4cb035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14df5b4cbe6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14dfbba84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14dfc0dc86ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14dfc0b8850f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14df5a39c6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14df5b4f14a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14df5b18bd40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14dfbba84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14dfc0dc86ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14dfc0b8850f in /lib64/libc.so.6)

[rank4]:[E ProcessGroupNCCL.cpp:563] [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800087 milliseconds before timing out.
[rank4]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 4] Timeout at NCCL work: 271, last enqueued NCCL work: 271, last completed NCCL work: 270.
[rank4]:[E ProcessGroupNCCL.cpp:577] [Rank 4] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank4]:[E ProcessGroupNCCL.cpp:583] [Rank 4] To avoid data inconsistency, we are taking the entire process down.
[rank4]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 4] Process group watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800087 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x148ff20c76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148f9e32d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x148f9e306b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x148f9e307035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x148f9e307e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x148ffec84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x149003f7b6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x149003d3b50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 4] Process group watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800087 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x148ff20c76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148f9e32d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x148f9e306b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x148f9e307035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x148f9e307e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x148ffec84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x149003f7b6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x149003d3b50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x148ff20c76f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x148f9e32d4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x148f9dfc7d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x148ffec84e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x149003f7b6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x149003d3b50f in /lib64/libc.so.6)

[rank13]:[E ProcessGroupNCCL.cpp:563] [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800096 milliseconds before timing out.
[rank13]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 13] Timeout at NCCL work: 271, last enqueued NCCL work: 271, last completed NCCL work: 270.
[rank13]:[E ProcessGroupNCCL.cpp:577] [Rank 13] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank13]:[E ProcessGroupNCCL.cpp:583] [Rank 13] To avoid data inconsistency, we are taking the entire process down.
[rank13]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 13] Process group watchdog thread terminated with exception: [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800096 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1547d079d6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15477f9db4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x15477f9b4b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15477f9b5035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15477f9b5e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1547e0884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1547e5b656ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1547e592550f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 13] Process group watchdog thread terminated with exception: [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800096 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1547d079d6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15477f9db4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x15477f9b4b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x15477f9b5035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x15477f9b5e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x1547e0884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x1547e5b656ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x1547e592550f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x1547d079d6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x15477f9db4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x15477f675d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x1547e0884e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x1547e5b656ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x1547e592550f in /lib64/libc.so.6)

[rank10]:[E ProcessGroupNCCL.cpp:563] [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800099 milliseconds before timing out.
[rank10]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 10] Timeout at NCCL work: 271, last enqueued NCCL work: 271, last completed NCCL work: 270.
[rank10]:[E ProcessGroupNCCL.cpp:577] [Rank 10] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank10]:[E ProcessGroupNCCL.cpp:583] [Rank 10] To avoid data inconsistency, we are taking the entire process down.
[rank10]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 10] Process group watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800099 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c31039d6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c2bc6eb4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14c2bc6c4b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14c2bc6c5035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14c2bc6c5e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14c31d084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14c3224cc6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14c32228c50f in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 10] Process group watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=271, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800099 milliseconds before timing out.
Exception raised from checkTimeout at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c31039d6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c2bc6eb4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d1 (0x14c2bc6c4b61 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1e5 (0x14c2bc6c5035 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0xfd (0x14c2bc6c5e6d in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0xd3e95 (0x14c31d084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #6: <unknown function> + 0xa6ea (0x14c3224cc6ea in /lib64/libpthread.so.0)
frame #7: clone + 0x41 (0x14c32228c50f in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at /soft/applications/conda/2024-04-29/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xa9 (0x14c31039d6f9 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x10584a1 (0x14c2bc6eb4a1 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xcf2d40 (0x14c2bc385d40 in /soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xd3e95 (0x14c31d084e95 in /soft/applications/conda/2024-04-29/mconda3/lib/libstdc++.so.6)
frame #4: <unknown function> + 0xa6ea (0x14c3224cc6ea in /lib64/libpthread.so.0)
frame #5: clone + 0x41 (0x14c32228c50f in /lib64/libc.so.6)

x3002c0s1b1n0.hsn.cm.polaris.alcf.anl.gov: rank 8 died from signal 6 and dumped core
x3002c0s1b0n0.hsn.cm.polaris.alcf.anl.gov: rank 4 died from signal 6 and dumped core
x3002c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 12 died from signal 6 and dumped core
x3002c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 17 died from signal 6 and dumped core
x3002c0s19b0n0.hsn.cm.polaris.alcf.anl.gov: rank 0 died from signal 15
x3002c0s19b0n0.hsn.cm.polaris.alcf.anl.gov: rank 1 died from signal 6 and dumped core
x3002c0s1b0n0.hsn.cm.polaris.alcf.anl.gov: rank 6 died from signal 6 and dumped core
x3002c0s1b0n0.hsn.cm.polaris.alcf.anl.gov: rank 5 died from signal 6 and dumped core
x3002c0s1b1n0.hsn.cm.polaris.alcf.anl.gov: rank 11 died from signal 6 and dumped core
x3002c0s1b1n0.hsn.cm.polaris.alcf.anl.gov: rank 10 died from signal 6 and dumped core
x3002c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 13 died from signal 6 and dumped core
x3002c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 15 died from signal 6 and dumped core
x3002c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 19 died from signal 6 and dumped core








Training completed
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled

Lmod is automatically replacing "nvhpc/23.9" with "gcc-native/12.3".


Lmod is automatically replacing "PrgEnv-nvhpc/8.5.0" with "PrgEnv-gnu/8.5.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.28

declare -x APP2_STATE="23.12.0"
declare -x BASH_ENV="/usr/share/lmod/lmod/init/bash"
declare -x C3_RSH="ssh -oConnectTimeout=10 -oForwardX11=no"
declare -x CFLAGS="-I/soft/applications/conda/2024-04-29/mconda3/include"
declare -x COLORTERM="1"
declare -x COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin"
declare -x CONDA_DEFAULT_ENV="base"
declare -x CONDA_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/conda"
declare -x CONDA_PREFIX="/soft/applications/conda/2024-04-29/mconda3"
declare -x CONDA_PROMPT_MODIFIER="(2024-04-29/base) "
declare -x CONDA_PYTHON_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/python"
declare -x CONDA_SHLVL="1"
declare -x CPU="x86_64"
declare -x CRAYPAT_LD_LIBRARY_PATH="/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAYPAT_OPTS_EXECUTABLE="libexec64/opts"
declare -x CRAYPAT_ROOT="/opt/cray/pe/perftools/23.12.0"
declare -x CRAYPE_DIR="/opt/cray/pe/craype/2.7.30"
declare -x CRAYPE_NETWORK_TARGET="ofi"
declare -x CRAYPE_VERSION="2.7.30"
declare -x CRAY_CPU_TARGET="x86-milan"
declare -x CRAY_DSMML_BASEDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_DIR="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_PREFIX="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_ROOTDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_VER="0.2.2"
declare -x CRAY_DSMML_VERSION="0.2.2"
declare -x CRAY_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x CRAY_HDF5_PARALLEL_PREFIX="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x CRAY_HDF5_PARALLEL_VERSION="1.12.2.9"
declare -x CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:/opt/cray/pe/pmi/6.1.13/lib:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:/opt/cray/pe/mpich/8.1.28/gtl/lib:/opt/cray/pe/dsmml/0.2.2/dsmml/lib:/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAY_LMOD_COMPILER="gnu/12.0"
declare -x CRAY_LMOD_CPU="x86-milan/1.0"
declare -x CRAY_LMOD_MPI="cray-mpich/8.0"
declare -x CRAY_LMOD_NET="ofi/1.0"
declare -x CRAY_MPICH_BASEDIR="/opt/cray/pe/mpich/8.1.28/ofi"
declare -x CRAY_MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_PREFIX="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_ROOTDIR="/opt/cray/pe/mpich/8.1.28"
declare -x CRAY_MPICH_VER="8.1.28"
declare -x CRAY_MPICH_VERSION="8.1.28"
declare -x CRAY_PERFTOOLS_PREFIX="/opt/cray/pe/perftools/23.12.0"
declare -x CRAY_PERFTOOLS_VERSION="23.12.0"
declare -x CRAY_PMI_INCLUDE_OPTS="-I/opt/cray/pe/pmi/6.1.13/include"
declare -x CRAY_PMI_POST_LINK_OPTS="-L/opt/cray/pe/pmi/6.1.13/lib"
declare -x CRAY_PMI_PREFIX="/opt/cray/pe/pmi/6.1.13"
declare -x CRAY_PMI_VERSION="6.1.13"
declare -x CSHEDIT="emacs"
declare -x CUDA_HOME="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_TOOLKIT_BASE="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDNN_HOME="/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/"
declare -x ENVIRONMENT="BATCH"
declare -x ENV_NAME="conda/2024-04-29"
declare -x FROM_HEADER=""
declare -x GCC_PATH="/usr/bin"
declare -x GCC_PREFIX="/usr/lib64/gcc/x86_64-suse-linux/12"
declare -x GCC_VERSION="12.3"
declare -x GNU_VERSION="12.3"
declare -x GPG_TTY="not a tty"
declare -x GSETTINGS_SCHEMA_DIR="/soft/applications/conda/2024-04-29/mconda3/share/glib-2.0/schemas"
declare -x GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=""
declare -x G_BROKEN_FILENAMES="1"
declare -x G_FILENAME_ENCODING="@locale,UTF-8,ISO-8859-15,CP1252"
declare -x HDF5_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HDF5_ROOT="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HISTSIZE="1000"
declare -x HOME="/home/shourya01"
declare -x HOST="x3006c0s19b0n0"
declare -x HOSTNAME="x3006c0s19b0n0"
declare -x HOSTTYPE="x86_64"
declare -x HTTPS_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x HTTP_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x LANG="en_US.UTF-8"
declare -x LANGUAGE="en_US.UTF-8"
declare -x LDFLAGS="-L/soft/applications/conda/2024-04-29/mconda3/lib -Wl,--enable-new-dtags,-rpath,/soft/applications/conda/2024-04-29/mconda3/lib"
declare -x LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:/soft/perftools/darshan/darshan-3.4.4/lib:/opt/cray/pe/papi/7.0.1.2/lib64:/opt/cray/libfabric/1.15.2.0/lib64"
declare -x LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so"
declare -x LESS="-M -I -R"
declare -x LESSCLOSE="lessclose.sh %s %s"
declare -x LESSKEY="/etc/lesskey.bin"
declare -x LESSOPEN="lessopen.sh %s"
declare -x LESS_ADVANCED_PREPROCESSOR="no"
declare -x LMOD_CMD="/usr/share/lmod/lmod/libexec/lmod"
declare -x LMOD_DIR="/usr/share/lmod/lmod/libexec"
declare -x LMOD_FAMILY_COMPILER="gcc-native"
declare -x LMOD_FAMILY_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_CRAYPE="craype"
declare -x LMOD_FAMILY_CRAYPE_CPU="craype-x86-milan"
declare -x LMOD_FAMILY_CRAYPE_CPU_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_NETWORK="craype-network-ofi"
declare -x LMOD_FAMILY_CRAYPE_NETWORK_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_VERSION="2.7.30"
declare -x LMOD_FAMILY_GCC_COMPILER="gcc-native"
declare -x LMOD_FAMILY_GCC_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_HDF5="cray-hdf5-parallel"
declare -x LMOD_FAMILY_HDF5_VERSION="1.12.2.9"
declare -x LMOD_FAMILY_MPI="cray-mpich"
declare -x LMOD_FAMILY_MPI_VERSION="8.1.28"
declare -x LMOD_FAMILY_PRGENV="PrgEnv-gnu"
declare -x LMOD_FAMILY_PRGENV_VERSION="8.5.0"
declare -x LMOD_FAMILY_PYTHON="conda"
declare -x LMOD_FAMILY_PYTHON_VERSION="2024-04-29"
declare -x LMOD_PKG="/usr/share/lmod/lmod"
declare -x LMOD_ROOT="/usr/share/lmod"
declare -x LMOD_SETTARG_FULL_SUPPORT="no"
declare -x LMOD_SYSTEM_DEFAULT_MODULES="PrgEnv-nvhpc:craype-network-ofi:perftools-base:darshan:xalt"
declare -x LMOD_VERSION="8.7.34"
declare -x LMOD_sys="Linux"
declare -x LOADEDMODULES="libfabric/1.15.2.0:craype-network-ofi:perftools-base/23.12.0:darshan/3.4.4:xalt/3.0.2-202408282050:gcc-native/12.3:craype/2.7.30:cray-dsmml/0.2.2:cray-mpich/8.1.28:cray-pmi/6.1.13:cray-pals/1.3.4:cray-libpals/1.3.4:craype-x86-milan:PrgEnv-gnu/8.5.0:cray-hdf5-parallel/1.12.2.9:cudnn/9.1.0:conda/2024-04-29"
declare -x LOGNAME="shourya01"
declare -x MACHTYPE="x86_64-suse-linux"
declare -x MAIL="/var/spool/mail/shourya01"
declare -x MANPATH="/opt/cray/pals/1.3.4/man:/opt/cray/pe/pmi/6.1.13/man:/opt/cray/pe/mpich/8.1.28/ofi/man:/opt/cray/pe/mpich/8.1.28/man/mpich:/opt/cray/pe/dsmml/0.2.2/dsmml/man:/opt/cray/pe/craype/2.7.30/man:/opt/cray/pe/perftools/23.12.0/man:/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:/opt/cray/libfabric/1.15.2.0/share/man:/usr/share/lmod/lmod/share/man:/home/shourya01/.local/man:/usr/local/man:/usr/share/man:/usr/man:/opt/c3/man:/opt/pbs/share/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man"
declare -x MINICOM="-c on"
declare -x MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:/opt/cray/pe/lmod/modulefiles/mix_compilers:/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:/soft/modulefiles:/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:/usr/share/modulefiles/Linux:/usr/share/modulefiles/Core:/usr/share/lmod/lmod/modulefiles/Core:/usr/share/lmod/lmod/modulefiles:/opt/cray/pals/lmod/modulefiles/core:/opt/cray/modulefiles:/opt/cray/pe/lmod/modulefiles/core:/opt/cray/pe/lmod/modulefiles/craype-targets/default:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:/soft/xalt/modulefiles"
declare -x MODULEPATH_ROOT="/usr/share/modulefiles"
declare -x MODULESHOME="/usr/share/lmod/lmod"
declare -x MORE="-sl"
declare -x MPI4JAX_USE_CUDA_MPI="1"
declare -x MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x MPICH_GPU_SUPPORT_ENABLED="1"
declare -x NCCL_IB_DISABLE="1"
declare -x NCCL_SOCKET_IFNAME="hsn"
declare -x NCPUS="64"
declare -x OFFLOAD_INIT="on_start"
declare -x OLDPWD
declare -x OMP_NUM_THREADS="4"
declare -x OSCAR_HOME="/opt/oscar"
declare -x OSTYPE="linux"
declare -x PAGER="less"
declare -x PALS_TRANSFER="0"
declare -x PATH="/soft/applications/conda/2024-04-29/mconda3/bin:/soft/applications/conda/2024-04-29/mconda3/condabin:/soft/xalt/3.0.2-202408282050/bin:/soft/compilers/cudatoolkit/cuda-12.4.1/bin:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:/opt/cray/pe/hdf5/1.12.2.9/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/home/shourya01/.local/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/shourya01/bin:/opt/cray/pe/bin"
declare -x PAT_RT_PERFCTR_DISABLE_COMPONENTS="nvml,rocm_smi"
declare -x PBS_ACCOUNT="ParaLLMs"
declare -x PBS_ENVIRONMENT="PBS_BATCH"
declare -x PBS_JOBCOOKIE="6562FB5932C933DF3C28E79600D3572E"
declare -x PBS_JOBDIR="/home/shourya01"
declare -x PBS_JOBID="5216091.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_JOBNAME="bash"
declare -x PBS_MOMPORT="15003"
declare -x PBS_NODEFILE="/var/spool/pbs/aux/5216091.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_NODENUM="0"
declare -x PBS_O_HOME="/home/shourya01"
declare -x PBS_O_HOST="polaris-login-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_O_INTERACTIVE_AUTH_METHOD="resvport"
declare -x PBS_O_LANG="en_US.UTF-8"
declare -x PBS_O_LOGNAME="shourya01"
declare -x PBS_O_MAIL="/var/spool/mail/shourya01"
declare -x PBS_O_PATH="/home/shourya01/.local/bin:/home/shourya01/.vscode-server/cli/servers/Stable-91fa95bccb027ece6a968589bb1d662fa9c8e170/server/bin/remote-cli:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/soft/xalt/3.0.2-202408282050/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/nvidia/23.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/extras/qd/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/shourya01/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/dbhome/db2cat/sqllib/bin:/dbhome/db2cat/sqllib/adm:/dbhome/db2cat/sqllib/misc:/dbhome/db2cat/sqllib/gskit/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/opt/cray/pe/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.vscode-server/extensions/ms-python.debugpy-2025.8.0/bundled/scripts/noConfigScripts"
declare -x PBS_O_QUEUE="debug"
declare -x PBS_O_SHELL="/bin/bash"
declare -x PBS_O_SYSTEM="Linux"
declare -x PBS_O_WORKDIR="/home/shourya01"
declare -x PBS_QUEUE="debug"
declare -x PBS_TASKNUM="1"
declare -x PELOCAL_PRGENV="true"
declare -x PERFTOOLS_VERSION="23.12.0"
declare -x PE_DSMML_MODULE_NAME="cray-dsmml"
declare -x PE_DSMML_PKGCONFIG_LIBS="dsmml"
declare -x PE_ENV="GNU"
declare -x PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel:mpichf90"
declare -x PE_GCC_EXTERNAL="native"
declare -x PE_GCC_LEVEL="12"
declare -x PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig"
declare -x PE_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x PE_HDF5_PARALLEL_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel"
declare -x PE_HDF5_PARALLEL_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel"
declare -x PE_MPICH_FIXED_PRGENV="GNU"
declare -x PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90"
declare -x PE_MPICH_GENCOMPILERS_GNU="12.3"
declare -x PE_MPICH_GTL_DIR_amd_gfx906="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx908="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx90a="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx940="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx942="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia70="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia80="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia90="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_ponteVecchio="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_LIBS_amd_gfx906="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx908="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx90a="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx940="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx942="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_nvidia70="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia80="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia90="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_ponteVecchio="-lmpi_gtl_ze"
declare -x PE_MPICH_MODULE_NAME="cray-mpich"
declare -x PE_MPICH_PKGCONFIG_LIBS="mpich"
declare -x PE_MPICH_PKGCONFIG_VARIABLES="PE_MPICH_GTL_DIR_@accelerator@:PE_MPICH_GTL_LIBS_@accelerator@"
declare -x PE_PALS_PKGCONFIG_LIBS="libpals"
declare -x PE_PERFTOOLS_MPICH_LIBDIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib"
declare -x PE_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel:mpich:dsmml:darshan-runtime"
declare -x PE_PKGCONFIG_PRODUCTS="PE_PALS:PE_PMI:PE_MPICH:PE_DSMML"
declare -x PE_PMI_PKGCONFIG_LIBS="cray-pmi"
declare -x PE_PRODUCT_LIST="CRAYPE_X86_MILAN"
declare -x PKGCONFIG_ENABLED="1"
declare -x PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:/opt/cray/pals/1.3.4/lib/pkgconfig:/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:/opt/cray/pe/craype/2.7.30/pkg-config:/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig"
declare -x PROFILEREAD="true"
declare -x PWD="/home/shourya01"
declare -x PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages"
declare -x PYTHONUSERBASE="/home/shourya01/.local/polaris/conda/2024-04-29"
declare -x QT_SYSTEM_DIR="/usr/share/desktop-data"
declare -x SHELL="/bin/bash"
declare -x SHLVL="2"
declare -x SLURM_MPI_TYPE="cray_shasta"
declare -x STARSHIP_SESSION_KEY="2428311924149224"
declare -x STARSHIP_SHELL="bash"
declare -x TMPDIR="/var/tmp/pbs.5216091.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x TRITON_DISABLE_AUTOTUNE="1"
declare -x TZ="Etc/UTC"
declare -x USER="shourya01"
declare -x USE_PCM_DB="2"
declare -x WINDOWMANAGER="xterm"
declare -x XALT_DIR="/soft/xalt/3.0.2-202408282050"
declare -x XALT_EXECUTABLE_TRACKING="yes"
declare -x XALT_SAMPLING="no"
declare -x XALT_SCALAR_AND_SPSR_SAMPLING="yes"
declare -x XCURSOR_THEME="DMZ"
declare -x XDG_CONFIG_DIRS="/etc/xdg"
declare -x XDG_DATA_DIRS="/usr/share"
declare -x XKEYSYMDB="/usr/X11R6/lib/X11/XKeysymDB"
declare -x XLA_FLAGS="--xla_gpu_force_compilation_parallelism=1 --xla_gpu_cuda_data_dir=/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x XLA_PYTHON_CLIENT_PREALLOCATE="false"
declare -x XML_CATALOG_FILES="file:///soft/applications/conda/2024-04-29/mconda3/etc/xml/catalog file:///etc/xml/catalog"
declare -x XNLSPATH="/usr/X11R6/lib/X11/nls"
declare -x _CE_CONDA=""
declare -x _CE_M=""
declare -x _LMFILES_="/opt/cray/modulefiles/libfabric/1.15.2.0:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-network-ofi.lua:/opt/cray/pe/lmod/modulefiles/core/perftools-base/23.12.0.lua:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles/darshan/3.4.4:/soft/xalt/modulefiles/xalt/3.0.2-202408282050:/opt/cray/pe/lmod/modulefiles/core/gcc-native/12.3.lua:/opt/cray/pe/lmod/modulefiles/core/craype/2.7.30.lua:/opt/cray/pe/lmod/modulefiles/core/cray-dsmml/0.2.2.lua:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0/cray-mpich/8.1.28.lua:/opt/cray/pe/lmod/modulefiles/core/cray-pmi/6.1.13.lua:/opt/cray/pals/lmod/modulefiles/core/cray-pals/1.3.4.lua:/opt/cray/pals/lmod/modulefiles/core/cray-libpals/1.3.4.lua:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-x86-milan.lua:/opt/cray/pe/lmod/modulefiles/core/PrgEnv-gnu/8.5.0.lua:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2.9.lua:/soft/modulefiles/cudnn/9.1.0.lua:/soft/modulefiles/conda/2024-04-29.lua"
declare -x _ModuleTable001_="X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpQcmdFbnYgPSAiUHJnRW52LWdudSIsCmNvbXBpbGVyID0gImdjYy1uYXRpdmUiLApjcmF5cGUgPSAiY3JheXBlIiwKY3JheXBlX2NwdSA9ICJjcmF5cGUteDg2LW1pbGFuIiwKY3JheXBlX25ldHdvcmsgPSAiY3JheXBlLW5ldHdvcmstb2ZpIiwKZ2NjX2NvbXBpbGVyID0gImdjYy1uYXRpdmUiLApoZGY1ID0gImNyYXktaGRmNS1wYXJhbGxlbCIsCm1waSA9ICJjcmF5LW1waWNoIiwKcHl0aG9uID0gImNvbmRhIiwKfSwKbVQgPSB7ClsiUHJnRW52LWdudSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUv"
declare -x _ModuleTable002_="bG1vZC9tb2R1bGVmaWxlcy9jb3JlL1ByZ0Vudi1nbnUvOC41LjAubHVhIiwKZnVsbE5hbWUgPSAiUHJnRW52LWdudS84LjUuMCIsCmxvYWRPcmRlciA9IDE0LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gIlByZ0Vudi1nbnUiLAp3ViA9ICJeMDAwMDAwMDguMDAwMDAwMDA1Lip6ZmluYWwiLAp9LApjb25kYSA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY29uZGEvMjAyNC0wNC0yOS5sdWEiLApmdWxsTmFtZSA9ICJjb25kYS8yMDI0LTA0LTI5IiwKbG9hZE9yZGVyID0gMTcsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY29uZGEiLAp3ViA9ICJeMDAw"
declare -x _ModuleTable003_="MDIwMjQuKnpmaW5hbC0uMDAwMDAwMDA0Lip6ZmluYWwtLjAwMDAwMDAyOS4qemZpbmFsIiwKfSwKWyJjcmF5LWRzbW1sIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1kc21tbC8wLjIuMi5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWRzbW1sLzAuMi4yIiwKbG9hZE9yZGVyID0gOCwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWRzbW1sIiwKd1YgPSAiXjAwMDAwMDAwLjAwMDAwMDAwMi4wMDAwMDAwMDIuKnpmaW5hbCIsCn0sClsiY3JheS1oZGY1LXBhcmFsbGVsIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9v"
declare -x _ModuleTable004_="ZmkvMS4wL2NyYXktbXBpY2gvOC4wL2NyYXktaGRmNS1wYXJhbGxlbC8xLjEyLjIuOS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLApsb2FkT3JkZXIgPSAxNSwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDEyLjAwMDAwMDAwMi4wMDAwMDAwMDkuKnpmaW5hbCIsCn0sClsiY3JheS1saWJwYWxzIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wYWxzL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5LWxpYnBhbHMvMS4zLjQubHVhIiwKZnVsbE5hbWUgPSAiY3JheS1s"
declare -x _ModuleTable005_="aWJwYWxzLzEuMy40IiwKbG9hZE9yZGVyID0gMTIsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1saWJwYWxzIiwKd1YgPSAiXjAwMDAwMDAxLjAwMDAwMDAwMy4wMDAwMDAwMDQuKnpmaW5hbCIsCn0sClsiY3JheS1tcGljaCJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMC9jcmF5LW1waWNoLzguMS4yOC5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LW1waWNoLzguMS4yOCIsCmxvYWRPcmRlciA9IDksCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1tcGljaCIsCndWID0gIl4w"
declare -x _ModuleTable006_="MDAwMDAwOC4wMDAwMDAwMDEuMDAwMDAwMDI4Lip6ZmluYWwiLAp9LApbImNyYXktcGFscyJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGFscy9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wYWxzLzEuMy40Lmx1YSIsCmZ1bGxOYW1lID0gImNyYXktcGFscy8xLjMuNCIsCmxvYWRPcmRlciA9IDExLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcGFscyIsCndWID0gIl4wMDAwMDAwMS4wMDAwMDAwMDMuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImNyYXktcG1pIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wbWkvNi4xLjEzLmx1YSIsCmZ1bGxOYW1lID0gImNy"
declare -x _ModuleTable007_="YXktcG1pLzYuMS4xMyIsCmxvYWRPcmRlciA9IDEwLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcG1pIiwKd1YgPSAiXjAwMDAwMDA2LjAwMDAwMDAwMS4wMDAwMDAwMTMuKnpmaW5hbCIsCn0sCmNyYXlwZSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5cGUvMi43LjMwLmx1YSIsCmZ1bGxOYW1lID0gImNyYXlwZS8yLjcuMzAiLApsb2FkT3JkZXIgPSA3LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZSIsCndWID0gIl4wMDAwMDAwMi4wMDAwMDAwMDcuMDAwMDAwMDMwLip6ZmluYWwiLAp9LApb"
declare -x _ModuleTable008_="ImNyYXlwZS1uZXR3b3JrLW9maSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS1uZXR3b3JrLW9maS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5cGUtbmV0d29yay1vZmkiLApsb2FkT3JkZXIgPSAyLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS1uZXR3b3JrLW9maSIsCndWID0gIk0uKnpmaW5hbCIsCn0sClsiY3JheXBlLXg4Ni1taWxhbiJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS14ODYtbWlsYW4ubHVhIiwKZnVsbE5hbWUgPSAiY3JheXBl"
declare -x _ModuleTable009_="LXg4Ni1taWxhbiIsCmxvYWRPcmRlciA9IDEzLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS14ODYtbWlsYW4iLAp3ViA9ICJNLip6ZmluYWwiLAp9LApjdWRubiA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY3Vkbm4vOS4xLjAubHVhIiwKZnVsbE5hbWUgPSAiY3Vkbm4vOS4xLjAiLApsb2FkT3JkZXIgPSAxNiwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjdWRubi85LjEuMCIsCndWID0gIjAwMDAwMDAwOS4wMDAwMDAwMDEuKnpmaW5hbCIsCn0sCmRhcnNoYW4gPSB7CmZuID0gIi9zb2Z0L3BlcmZ0b29scy9k"
declare -x _ModuleTable010_="YXJzaGFuL2RhcnNoYW4tMy40LjQvc2hhcmUvY3JheXBlLTIueC9tb2R1bGVmaWxlcy9kYXJzaGFuLzMuNC40IiwKZnVsbE5hbWUgPSAiZGFyc2hhbi8zLjQuNCIsCmxvYWRPcmRlciA9IDQsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiZGFyc2hhbiIsCndWID0gIjAwMDAwMDAwMy4wMDAwMDAwMDQuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImdjYy1uYXRpdmUiXSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9nY2MtbmF0aXZlLzEyLjMubHVhIiwKZnVsbE5hbWUgPSAiZ2NjLW5hdGl2ZS8xMi4zIiwKbG9hZE9yZGVyID0gNiwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0"
declare -x _ModuleTable011_="YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJnY2MtbmF0aXZlIiwKd1YgPSAiXjAwMDAwMDEyLjAwMDAwMDAwMy4qemZpbmFsIiwKfSwKbGliZmFicmljID0gewpmbiA9ICIvb3B0L2NyYXkvbW9kdWxlZmlsZXMvbGliZmFicmljLzEuMTUuMi4wIiwKZnVsbE5hbWUgPSAibGliZmFicmljLzEuMTUuMi4wIiwKbG9hZE9yZGVyID0gMSwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJsaWJmYWJyaWMiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDE1LjAwMDAwMDAwMi4qemZpbmFsIiwKfSwKWyJwZXJmdG9vbHMtYmFzZSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb3JlL3BlcmZ0b29s"
declare -x _ModuleTable012_="cy1iYXNlLzIzLjEyLjAubHVhIiwKZnVsbE5hbWUgPSAicGVyZnRvb2xzLWJhc2UvMjMuMTIuMCIsCmxvYWRPcmRlciA9IDMsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAicGVyZnRvb2xzLWJhc2UiLAp3ViA9ICJeMDAwMDAwMjMuMDAwMDAwMDEyLip6ZmluYWwiLAp9LAp4YWx0ID0gewpmbiA9ICIvc29mdC94YWx0L21vZHVsZWZpbGVzL3hhbHQvMy4wLjItMjAyNDA4MjgyMDUwIiwKZnVsbE5hbWUgPSAieGFsdC8zLjAuMi0yMDI0MDgyODIwNTAiLApsb2FkT3JkZXIgPSA1LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gInhhbHQiLAp3ViA9ICJeMDAwMDAw"
declare -x _ModuleTable013_="MDMuMDAwMDAwMDAwLjAwMDAwMDAwMi4qemZpbmFsLS4yMDI0MDgyODIwNTAuKnpmaW5hbCIsCn0sCn0sCm1wYXRoQSA9IHsKCiIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9oZGY1LXBhcmFsbGVsL2dudS8xMi4wL29maS8xLjAvY3JheS1tcGljaC84LjAvY3JheS1oZGY1LXBhcmFsbGVsLzEuMTIuMiIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3B1L3g4Ni1taWxhbi8xLjAiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9vZmkvMS4wL2NyYXktbXBpY2gvOC4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMCIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxl"
declare -x _ModuleTable014_="ZmlsZXMvbWl4X2NvbXBpbGVycyIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29tcGlsZXIvZ251LzEyLjAiLCAiL3NvZnQvbW9kdWxlZmlsZXMiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL3BlcmZ0b29scy8yMy4xMi4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9uZXQvb2ZpLzEuMCIsICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0xpbnV4IgosICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmUiLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZSIKLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BhbHMvbG1vZC9tb2R1bGVmaWxlcy9jb3JlIgosICIvb3B0L2Ny"
declare -x _ModuleTable015_="YXkvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZSIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdCIKLCAiL3NvZnQvcGVyZnRvb2xzL2RhcnNoYW4vZGFyc2hhbi0zLjQuNC9zaGFyZS9jcmF5cGUtMi54L21vZHVsZWZpbGVzIiwgIi9zb2Z0L3hhbHQvbW9kdWxlZmlsZXMiLAp9LApzeXN0ZW1CYXNlTVBBVEggPSAiL3Vzci9zaGFyZS9tb2R1bGVmaWxlcy9MaW51eDovdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmU6L3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZTovdXNyL3NoYXJlL2xtb2QvbG1vZC9tb2R1bGVmaWxlczovb3B0L2NyYXkvcGFscy9sbW9kL21vZHVs"
declare -x _ModuleTable016_="ZWZpbGVzL2NvcmU6L29wdC9jcmF5L21vZHVsZWZpbGVzOi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmU6L29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdDovc29mdC9wZXJmdG9vbHMvZGFyc2hhbi9kYXJzaGFuLTMuNC40L3NoYXJlL2NyYXlwZS0yLngvbW9kdWxlZmlsZXM6L3NvZnQveGFsdC9tb2R1bGVmaWxlcyIsCn0K"
declare -x _ModuleTable_Sz_="16"
declare -x __LMOD_Priority_PATH="/soft/xalt/3.0.2-202408282050/bin:-100"
declare -x __LMOD_REF_COUNT_COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin:1"
declare -x __LMOD_REF_COUNT_CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:1;/opt/cray/pe/pmi/6.1.13/lib:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:1;/opt/cray/pe/mpich/8.1.28/gtl/lib:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib:1;/opt/cray/pe/perftools/23.12.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:1;/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:1;/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:1;/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:1;/soft/perftools/darshan/darshan-3.4.4/lib:1;/opt/cray/pe/papi/7.0.1.2/lib64:1;/opt/cray/libfabric/1.15.2.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so:1"
declare -x __LMOD_REF_COUNT_MANPATH="/opt/cray/pals/1.3.4/man:2;/opt/cray/pe/pmi/6.1.13/man:1;/opt/cray/pe/mpich/8.1.28/ofi/man:1;/opt/cray/pe/mpich/8.1.28/man/mpich:1;/opt/cray/pe/dsmml/0.2.2/dsmml/man:1;/opt/cray/pe/craype/2.7.30/man:1;/opt/cray/pe/perftools/23.12.0/man:1;/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:1;/opt/cray/libfabric/1.15.2.0/share/man:1;/usr/share/lmod/lmod/share/man:1;/home/shourya01/.local/man:1;/usr/local/man:1;/usr/share/man:1;/usr/man:1;/opt/c3/man:1;/opt/pbs/share/man:1;/opt/clmgr/man:1;/opt/sgi/share/man:1;/opt/clmgr/share/man:1;/opt/clmgr/lib/cm-cli/man:1"
declare -x __LMOD_REF_COUNT_MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:1;/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:1;/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:1;/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:1;/opt/cray/pe/lmod/modulefiles/mix_compilers:1;/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:1;/soft/modulefiles:1;/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:1;/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:1;/usr/share/modulefiles/Linux:1;/usr/share/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles:1;/opt/cray/pals/lmod/modulefiles/core:1;/opt/cray/modulefiles:1;/opt/cray/pe/lmod/modulefiles/core:1;/opt/cray/pe/lmod/modulefiles/craype-targets/default:1;/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:1;/soft/xalt/modulefiles:1"
declare -x __LMOD_REF_COUNT_PATH="/soft/xalt/3.0.2-202408282050/bin:1;/soft/compilers/cudatoolkit/cuda-12.4.1/bin:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:1;/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:1;/opt/cray/pe/hdf5/1.12.2.9/bin:1;/opt/cray/pals/1.3.4/bin:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:1;/opt/cray/pe/mpich/8.1.28/bin:1;/opt/cray/pe/craype/2.7.30/bin:1;/home/shourya01/.local/bin:4;/soft/perftools/darshan/darshan-3.4.4/bin:1;/opt/cray/pe/perftools/23.12.0/bin:1;/opt/cray/pe/papi/7.0.1.2/bin:1;/opt/cray/libfabric/1.15.2.0/bin:1;/opt/clmgr/sbin:1;/opt/clmgr/bin:1;/opt/sgi/sbin:1;/opt/sgi/bin:1;/usr/local/bin:1;/usr/bin:1;/bin:2;/opt/c3/bin:1;/usr/lib/mit/bin:1;/usr/lib/mit/sbin:1;/opt/pbs/bin:1;/sbin:1;/home/shourya01/bin:1;/opt/cray/pe/bin:1"
declare -x __LMOD_REF_COUNT_PE_DSMML_PKGCONFIG_LIBS="dsmml:1"
declare -x __LMOD_REF_COUNT_PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:1;hdf5_fortran_parallel:1;mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FIXED_PRGENV="GNU:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_GENCOMPILERS_GNU="12.3:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_PKGCONFIG_LIBS="mpich:1"
declare -x __LMOD_REF_COUNT_PE_PALS_PKGCONFIG_LIBS="libpals:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_LIBS="hdf5_hl_parallel:1;hdf5_parallel:1;mpich:1;dsmml:1;darshan-runtime:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_PRODUCTS="PE_PALS:1;PE_PMI:1;PE_MPICH:1;PE_DSMML:1"
declare -x __LMOD_REF_COUNT_PE_PMI_PKGCONFIG_LIBS="cray-pmi:1"
declare -x __LMOD_REF_COUNT_PE_PRODUCT_LIST="CRAYPE_X86_MILAN:1;PERFTOOLS:1;CRAYPAT:1"
declare -x __LMOD_REF_COUNT_PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:1;/opt/cray/pals/1.3.4/lib/pkgconfig:1;/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:1;/opt/cray/pe/craype/2.7.30/pkg-config:1;/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:1;/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages:1"
declare -x ftp_proxy="http://proxy.alcf.anl.gov:3128"
declare -x http_proxy="http://proxy.alcf.anl.gov:3128"
declare -x https_proxy="http://proxy.alcf.anl.gov:3128"
declare -x no_proxy="admin,polaris-adminvm-01,localhost,*.cm.polaris.alcf.anl.gov,polaris-*,*.polaris.alcf.anl.gov,*.alcf.anl.gov"
Running on 2 nodes
Total number of GPUs: 8
Connected to tcp://x3006c0s19b0n0.hsn.cm.polaris.alcf.anl.gov:7919
Found executable /soft/applications/conda/2024-04-29/mconda3/bin/python
Launching application 37e2424d-ab99-44b8-903b-d4ffe070ee96
Using PMI port 39500,39501
[2025-06-20 18:31:02,413] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-20 18:31:02,413] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-20 18:31:02,413] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-20 18:31:02,413] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-20 18:31:03,655] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-20 18:31:03,655] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-20 18:31:03,655] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-20 18:31:03,655] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-20 18:31:08,331] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-20 18:31:08,331] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-20 18:31:08,332] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-20 18:31:08,332] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-20 18:31:08,332] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-20 18:31:08,332] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-20 18:31:08,332] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-20 18:31:08,332] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-20 18:31:10,670] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-20 18:31:10,670] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-20 18:31:10,670] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-20 18:31:10,670] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-20 18:31:10,670] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-20 18:31:10,670] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-20 18:31:10,670] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-20 18:31:10,670] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=3, world_size=8, master_addr=10.140.57.105, master_port=29500
[2025-06-20 18:31:10,670] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=8, master_addr=10.140.57.105, master_port=29500
[2025-06-20 18:31:10,670] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-20 18:31:10,670] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=0, world_size=8, master_addr=10.140.57.105, master_port=29500
[2025-06-20 18:31:10,670] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=1, world_size=8, master_addr=10.140.57.105, master_port=29500
[2025-06-20 18:31:10,670] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=2, world_size=8, master_addr=10.140.57.105, master_port=29500
[2025-06-20 18:31:10,670] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=8, master_addr=10.140.57.105, master_port=29500
[2025-06-20 18:31:10,670] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=8, master_addr=10.140.57.105, master_port=29500
[2025-06-20 18:31:10,670] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=8, master_addr=10.140.57.105, master_port=29500
[2025-06-20 18:31:10,670] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Initialized deepspeed on global rank 0, local rank 0 with world size 8.
[2025-06-20 18:35:23,086] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2+5f631abc, git-hash=5f631abc, git-branch=HEAD
[2025-06-20 18:35:26,770] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-06-20 18:35:26,771] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-06-20 18:35:26,772] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-06-20 18:35:26,786] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-06-20 18:35:26,786] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-06-20 18:35:26,786] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-06-20 18:35:26,786] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-06-20 18:35:26,786] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2025-06-20 18:35:26,786] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   amp_params ................... False
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x14a6bb92fd50>
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2025-06-20 18:35:26,787] [INFO] [config.py:1000:print]   dump_state ................... False
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   global_rank .................. 0
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 5e-05, 'weight_decay': 0.01}
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   pld_params ................... False
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   steps_per_print .............. 100000
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   train_batch_size ............. 1024
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2025-06-20 18:35:26,788] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2025-06-20 18:35:26,789] [INFO] [config.py:1000:print]   world_size ................... 8
[2025-06-20 18:35:26,789] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2025-06-20 18:35:26,789] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-06-20 18:35:26,789] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2025-06-20 18:35:26,789] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2025-06-20 18:35:26,789] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2025-06-20 18:35:26,789] [INFO] [config.py:986:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 128, 
    "train_batch_size": 1.024000e+03, 
    "steps_per_print": 1.000000e+05, 
    "gradient_accumulation_steps": 1, 
    "fp16": {
        "enabled": false
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 5e-05, 
            "weight_decay": 0.01
        }
    }, 
    "comms_logger": {
        "enabled": true, 
        "verbose": false
    }, 
    "zero_optimization": {
        "stage": 0
    }
}
Validating lr=5e-05, train epoch 0.:   0%|          | 0/321 [00:00<?, ?it/s]Validating lr=5e-05, train epoch 0.:   0%|          | 1/321 [00:04<26:26,  4.96s/it]Validating lr=5e-05, train epoch 0.:   1%|          | 2/321 [00:08<23:23,  4.40s/it]Validating lr=5e-05, train epoch 0.:   1%|          | 3/321 [00:12<22:19,  4.21s/it]Validating lr=5e-05, train epoch 0.:   1%|          | 4/321 [00:16<21:51,  4.14s/it]Validating lr=5e-05, train epoch 0.:   2%|▏         | 5/321 [00:21<21:34,  4.10s/it]Validating lr=5e-05, train epoch 0.:   2%|▏         | 6/321 [00:25<21:19,  4.06s/it]Validating lr=5e-05, train epoch 0.:   2%|▏         | 7/321 [00:29<21:12,  4.05s/it]Validating lr=5e-05, train epoch 0.:   2%|▏         | 8/321 [00:33<21:00,  4.03s/it]Validating lr=5e-05, train epoch 0.:   3%|▎         | 9/321 [00:37<20:54,  4.02s/it]Validating lr=5e-05, train epoch 0.:   3%|▎         | 10/321 [00:41<20:51,  4.02s/it]Validating lr=5e-05, train epoch 0.:   3%|▎         | 11/321 [00:45<20:45,  4.02s/it]Validating lr=5e-05, train epoch 0.:   4%|▎         | 12/321 [00:49<20:38,  4.01s/it]Validating lr=5e-05, train epoch 0.:   4%|▍         | 13/321 [00:53<20:38,  4.02s/it]Validating lr=5e-05, train epoch 0.:   4%|▍         | 14/321 [00:57<20:31,  4.01s/it]Validating lr=5e-05, train epoch 0.:   5%|▍         | 15/321 [01:01<20:25,  4.01s/it]Validating lr=5e-05, train epoch 0.:   5%|▍         | 16/321 [01:05<20:25,  4.02s/it]Validating lr=5e-05, train epoch 0.:   5%|▌         | 17/321 [01:09<20:21,  4.02s/it]Validating lr=5e-05, train epoch 0.:   6%|▌         | 18/321 [01:13<20:15,  4.01s/it]Validating lr=5e-05, train epoch 0.:   6%|▌         | 19/321 [01:17<20:12,  4.02s/it]Validating lr=5e-05, train epoch 0.:   6%|▌         | 20/321 [01:21<20:09,  4.02s/it]Validating lr=5e-05, train epoch 0.:   7%|▋         | 21/321 [01:25<20:05,  4.02s/it]Validating lr=5e-05, train epoch 0.:   7%|▋         | 22/321 [01:29<19:54,  3.99s/it]Validating lr=5e-05, train epoch 0.:   7%|▋         | 23/321 [01:33<19:49,  3.99s/it]Validating lr=5e-05, train epoch 0.:   7%|▋         | 24/321 [01:37<19:51,  4.01s/it]Validating lr=5e-05, train epoch 0.:   8%|▊         | 25/321 [01:41<19:48,  4.02s/it]Validating lr=5e-05, train epoch 0.:   8%|▊         | 26/321 [01:45<19:43,  4.01s/it]Validating lr=5e-05, train epoch 0.:   8%|▊         | 27/321 [01:49<19:35,  4.00s/it]Validating lr=5e-05, train epoch 0.:   9%|▊         | 28/321 [01:53<19:32,  4.00s/it]Validating lr=5e-05, train epoch 0.:   9%|▉         | 29/321 [01:57<19:28,  4.00s/it]Validating lr=5e-05, train epoch 0.:   9%|▉         | 30/321 [02:01<19:26,  4.01s/it]Validating lr=5e-05, train epoch 0.:  10%|▉         | 31/321 [02:05<19:25,  4.02s/it]Validating lr=5e-05, train epoch 0.:  10%|▉         | 32/321 [02:09<19:21,  4.02s/it]Validating lr=5e-05, train epoch 0.:  10%|█         | 33/321 [02:13<19:12,  4.00s/it]Validating lr=5e-05, train epoch 0.:  11%|█         | 34/321 [02:17<19:08,  4.00s/it]Validating lr=5e-05, train epoch 0.:  11%|█         | 35/321 [02:21<19:06,  4.01s/it]Validating lr=5e-05, train epoch 0.:  11%|█         | 36/321 [02:25<19:00,  4.00s/it]Validating lr=5e-05, train epoch 0.:  12%|█▏        | 37/321 [02:29<18:52,  3.99s/it]Validating lr=5e-05, train epoch 0.:  12%|█▏        | 38/321 [02:33<18:49,  3.99s/it]Validating lr=5e-05, train epoch 0.:  12%|█▏        | 39/321 [02:37<18:48,  4.00s/it]Validating lr=5e-05, train epoch 0.:  12%|█▏        | 40/321 [02:41<18:46,  4.01s/it]Validating lr=5e-05, train epoch 0.:  13%|█▎        | 41/321 [02:45<18:42,  4.01s/it]Validating lr=5e-05, train epoch 0.:  13%|█▎        | 42/321 [02:49<18:39,  4.01s/it]Validating lr=5e-05, train epoch 0.:  13%|█▎        | 43/321 [02:53<18:34,  4.01s/it]Validating lr=5e-05, train epoch 0.:  14%|█▎        | 44/321 [02:57<18:29,  4.00s/it]Validating lr=5e-05, train epoch 0.:  14%|█▍        | 45/321 [03:01<18:29,  4.02s/it]Validating lr=5e-05, train epoch 0.:  14%|█▍        | 46/321 [03:05<18:24,  4.02s/it]Validating lr=5e-05, train epoch 0.:  15%|█▍        | 47/321 [03:09<18:22,  4.02s/it]Validating lr=5e-05, train epoch 0.:  15%|█▍        | 48/321 [03:13<18:24,  4.05s/it]Validating lr=5e-05, train epoch 0.:  15%|█▌        | 49/321 [03:17<18:24,  4.06s/it]Validating lr=5e-05, train epoch 0.:  16%|█▌        | 50/321 [03:21<18:20,  4.06s/it]Validating lr=5e-05, train epoch 0.:  16%|█▌        | 51/321 [03:25<18:15,  4.06s/it]Validating lr=5e-05, train epoch 0.:  16%|█▌        | 52/321 [03:29<18:05,  4.03s/it]Validating lr=5e-05, train epoch 0.:  17%|█▋        | 53/321 [03:33<18:02,  4.04s/it]Validating lr=5e-05, train epoch 0.:  17%|█▋        | 54/321 [03:37<17:54,  4.02s/it]Validating lr=5e-05, train epoch 0.:  17%|█▋        | 55/321 [03:41<17:57,  4.05s/it]Validating lr=5e-05, train epoch 0.:  17%|█▋        | 56/321 [03:45<17:50,  4.04s/it]Validating lr=5e-05, train epoch 0.:  18%|█▊        | 57/321 [03:49<17:42,  4.02s/it]Validating lr=5e-05, train epoch 0.:  18%|█▊        | 58/321 [03:53<17:35,  4.01s/it]Validating lr=5e-05, train epoch 0.:  18%|█▊        | 59/321 [03:57<17:31,  4.01s/it]Validating lr=5e-05, train epoch 0.:  19%|█▊        | 60/321 [04:01<17:32,  4.03s/it]Validating lr=5e-05, train epoch 0.:  19%|█▉        | 61/321 [04:05<17:32,  4.05s/it]Validating lr=5e-05, train epoch 0.:  19%|█▉        | 62/321 [04:09<17:24,  4.03s/it]Validating lr=5e-05, train epoch 0.:  20%|█▉        | 63/321 [04:14<17:23,  4.04s/it]Validating lr=5e-05, train epoch 0.:  20%|█▉        | 64/321 [04:18<17:17,  4.04s/it]Validating lr=5e-05, train epoch 0.:  20%|██        | 65/321 [04:22<17:11,  4.03s/it]Validating lr=5e-05, train epoch 0.:  21%|██        | 66/321 [04:26<17:07,  4.03s/it]Validating lr=5e-05, train epoch 0.:  21%|██        | 67/321 [04:30<17:02,  4.02s/it]Validating lr=5e-05, train epoch 0.:  21%|██        | 68/321 [04:34<17:00,  4.03s/it]Validating lr=5e-05, train epoch 0.:  21%|██▏       | 69/321 [04:38<16:53,  4.02s/it]Validating lr=5e-05, train epoch 0.:  22%|██▏       | 70/321 [04:42<16:44,  4.00s/it]Validating lr=5e-05, train epoch 0.:  22%|██▏       | 71/321 [04:46<16:41,  4.01s/it]Validating lr=5e-05, train epoch 0.:  22%|██▏       | 72/321 [04:50<16:37,  4.01s/it]Validating lr=5e-05, train epoch 0.:  23%|██▎       | 73/321 [04:54<16:35,  4.02s/it]Validating lr=5e-05, train epoch 0.:  23%|██▎       | 74/321 [04:58<16:34,  4.03s/it]Validating lr=5e-05, train epoch 0.:  23%|██▎       | 75/321 [05:02<16:33,  4.04s/it]Validating lr=5e-05, train epoch 0.:  24%|██▎       | 76/321 [05:06<16:24,  4.02s/it]Validating lr=5e-05, train epoch 0.:  24%|██▍       | 77/321 [05:10<16:18,  4.01s/it]Validating lr=5e-05, train epoch 0.:  24%|██▍       | 78/321 [05:14<16:16,  4.02s/it]Validating lr=5e-05, train epoch 0.:  25%|██▍       | 79/321 [05:18<16:17,  4.04s/it]Validating lr=5e-05, train epoch 0.:  25%|██▍       | 80/321 [05:22<16:15,  4.05s/it]Validating lr=5e-05, train epoch 0.:  25%|██▌       | 81/321 [05:26<16:11,  4.05s/it]Validating lr=5e-05, train epoch 0.:  26%|██▌       | 82/321 [05:30<16:08,  4.05s/it]Validating lr=5e-05, train epoch 0.:  26%|██▌       | 83/321 [05:34<16:00,  4.03s/it]Validating lr=5e-05, train epoch 0.:  26%|██▌       | 84/321 [05:38<15:53,  4.02s/it]Validating lr=5e-05, train epoch 0.:  26%|██▋       | 85/321 [05:42<15:50,  4.03s/it]Validating lr=5e-05, train epoch 0.:  27%|██▋       | 86/321 [05:46<15:46,  4.03s/it]Validating lr=5e-05, train epoch 0.:  27%|██▋       | 87/321 [05:50<15:44,  4.04s/it]Validating lr=5e-05, train epoch 0.:  27%|██▋       | 88/321 [05:54<15:38,  4.03s/it]Validating lr=5e-05, train epoch 0.:  28%|██▊       | 89/321 [05:58<15:37,  4.04s/it]Validating lr=5e-05, train epoch 0.:  28%|██▊       | 90/321 [06:02<15:30,  4.03s/it]Validating lr=5e-05, train epoch 0.:  28%|██▊       | 91/321 [06:06<15:30,  4.04s/it]Validating lr=5e-05, train epoch 0.:  29%|██▊       | 92/321 [06:10<15:22,  4.03s/it]Validating lr=5e-05, train epoch 0.:  29%|██▉       | 93/321 [06:14<15:16,  4.02s/it]Validating lr=5e-05, train epoch 0.:  29%|██▉       | 94/321 [06:18<15:17,  4.04s/it]Validating lr=5e-05, train epoch 0.:  30%|██▉       | 95/321 [06:22<15:11,  4.03s/it]Validating lr=5e-05, train epoch 0.:  30%|██▉       | 96/321 [06:26<15:03,  4.02s/it]Validating lr=5e-05, train epoch 0.:  30%|███       | 97/321 [06:30<15:00,  4.02s/it]Validating lr=5e-05, train epoch 0.:  31%|███       | 98/321 [06:34<14:57,  4.02s/it]Validating lr=5e-05, train epoch 0.:  31%|███       | 99/321 [06:39<14:59,  4.05s/it]Validating lr=5e-05, train epoch 0.:  31%|███       | 100/321 [06:43<14:48,  4.02s/it]Validating lr=5e-05, train epoch 0.:  31%|███▏      | 101/321 [06:47<14:42,  4.01s/it]Validating lr=5e-05, train epoch 0.:  32%|███▏      | 102/321 [06:51<14:40,  4.02s/it]Validating lr=5e-05, train epoch 0.:  32%|███▏      | 103/321 [06:55<14:37,  4.02s/it]Validating lr=5e-05, train epoch 0.:  32%|███▏      | 104/321 [06:59<14:31,  4.02s/it]Validating lr=5e-05, train epoch 0.:  33%|███▎      | 105/321 [07:03<14:28,  4.02s/it]Validating lr=5e-05, train epoch 0.:  33%|███▎      | 106/321 [07:07<14:21,  4.01s/it]Validating lr=5e-05, train epoch 0.:  33%|███▎      | 107/321 [07:11<14:19,  4.02s/it]Validating lr=5e-05, train epoch 0.:  34%|███▎      | 108/321 [07:15<14:19,  4.03s/it]Validating lr=5e-05, train epoch 0.:  34%|███▍      | 109/321 [07:19<14:11,  4.02s/it]Validating lr=5e-05, train epoch 0.:  34%|███▍      | 110/321 [07:23<14:08,  4.02s/it]Validating lr=5e-05, train epoch 0.:  35%|███▍      | 111/321 [07:27<14:01,  4.00s/it]Validating lr=5e-05, train epoch 0.:  35%|███▍      | 112/321 [07:31<13:56,  4.00s/it]Validating lr=5e-05, train epoch 0.:  35%|███▌      | 113/321 [07:35<13:52,  4.00s/it]Validating lr=5e-05, train epoch 0.:  36%|███▌      | 114/321 [07:39<13:54,  4.03s/it]Validating lr=5e-05, train epoch 0.:  36%|███▌      | 115/321 [07:43<13:46,  4.01s/it]Validating lr=5e-05, train epoch 0.:  36%|███▌      | 116/321 [07:47<13:46,  4.03s/it]Validating lr=5e-05, train epoch 0.:  36%|███▋      | 117/321 [07:51<13:43,  4.03s/it]Validating lr=5e-05, train epoch 0.:  37%|███▋      | 118/321 [07:55<13:36,  4.02s/it]Validating lr=5e-05, train epoch 0.:  37%|███▋      | 119/321 [07:59<13:32,  4.02s/it]Validating lr=5e-05, train epoch 0.:  37%|███▋      | 120/321 [08:03<13:28,  4.02s/it]Validating lr=5e-05, train epoch 0.:  38%|███▊      | 121/321 [08:07<13:22,  4.01s/it]Validating lr=5e-05, train epoch 0.:  38%|███▊      | 122/321 [08:11<13:17,  4.01s/it]Validating lr=5e-05, train epoch 0.:  38%|███▊      | 123/321 [08:15<13:14,  4.01s/it]Validating lr=5e-05, train epoch 0.:  39%|███▊      | 124/321 [08:19<13:09,  4.01s/it]Validating lr=5e-05, train epoch 0.:  39%|███▉      | 125/321 [08:23<13:04,  4.00s/it]Validating lr=5e-05, train epoch 0.:  39%|███▉      | 126/321 [08:27<13:01,  4.01s/it]Validating lr=5e-05, train epoch 0.:  40%|███▉      | 127/321 [08:31<12:57,  4.01s/it]Validating lr=5e-05, train epoch 0.:  40%|███▉      | 128/321 [08:35<12:50,  3.99s/it]Validating lr=5e-05, train epoch 0.:  40%|████      | 129/321 [08:39<12:45,  3.99s/it]Validating lr=5e-05, train epoch 0.:  40%|████      | 130/321 [08:43<12:39,  3.98s/it]Validating lr=5e-05, train epoch 0.:  41%|████      | 131/321 [08:47<12:36,  3.98s/it]Validating lr=5e-05, train epoch 0.:  41%|████      | 132/321 [08:51<12:34,  3.99s/it]Validating lr=5e-05, train epoch 0.:  41%|████▏     | 133/321 [08:55<12:32,  4.00s/it]Validating lr=5e-05, train epoch 0.:  42%|████▏     | 134/321 [08:59<12:30,  4.01s/it]Validating lr=5e-05, train epoch 0.:  42%|████▏     | 135/321 [09:03<12:30,  4.03s/it]Validating lr=5e-05, train epoch 0.:  42%|████▏     | 136/321 [09:07<12:28,  4.05s/it]Validating lr=5e-05, train epoch 0.:  43%|████▎     | 137/321 [09:11<12:27,  4.06s/it]Validating lr=5e-05, train epoch 0.:  43%|████▎     | 138/321 [09:15<12:21,  4.05s/it]Validating lr=5e-05, train epoch 0.:  43%|████▎     | 139/321 [09:19<12:18,  4.06s/it]Validating lr=5e-05, train epoch 0.:  44%|████▎     | 140/321 [09:23<12:13,  4.05s/it]Validating lr=5e-05, train epoch 0.:  44%|████▍     | 141/321 [09:27<12:04,  4.03s/it]Validating lr=5e-05, train epoch 0.:  44%|████▍     | 142/321 [09:31<12:01,  4.03s/it]Validating lr=5e-05, train epoch 0.:  45%|████▍     | 143/321 [09:35<11:55,  4.02s/it]Validating lr=5e-05, train epoch 0.:  45%|████▍     | 144/321 [09:39<11:51,  4.02s/it]Validating lr=5e-05, train epoch 0.:  45%|████▌     | 145/321 [09:43<11:48,  4.03s/it]Validating lr=5e-05, train epoch 0.:  45%|████▌     | 146/321 [09:47<11:41,  4.01s/it]Validating lr=5e-05, train epoch 0.:  46%|████▌     | 147/321 [09:51<11:39,  4.02s/it]Validating lr=5e-05, train epoch 0.:  46%|████▌     | 148/321 [09:55<11:35,  4.02s/it]Validating lr=5e-05, train epoch 0.:  46%|████▋     | 149/321 [09:59<11:31,  4.02s/it]Validating lr=5e-05, train epoch 0.:  47%|████▋     | 150/321 [10:03<11:25,  4.01s/it]Validating lr=5e-05, train epoch 0.:  47%|████▋     | 151/321 [10:07<11:22,  4.01s/it]Validating lr=5e-05, train epoch 0.:  47%|████▋     | 152/321 [10:11<11:17,  4.01s/it]Validating lr=5e-05, train epoch 0.:  48%|████▊     | 153/321 [10:15<11:13,  4.01s/it]Validating lr=5e-05, train epoch 0.:  48%|████▊     | 154/321 [10:19<11:10,  4.01s/it]Validating lr=5e-05, train epoch 0.:  48%|████▊     | 155/321 [10:24<11:09,  4.03s/it]Validating lr=5e-05, train epoch 0.:  49%|████▊     | 156/321 [10:28<11:08,  4.05s/it]Validating lr=5e-05, train epoch 0.:  49%|████▉     | 157/321 [10:32<11:01,  4.03s/it]Validating lr=5e-05, train epoch 0.:  49%|████▉     | 158/321 [10:36<10:56,  4.03s/it]Validating lr=5e-05, train epoch 0.:  50%|████▉     | 159/321 [10:40<10:49,  4.01s/it]Validating lr=5e-05, train epoch 0.:  50%|████▉     | 160/321 [10:44<10:44,  4.00s/it]Validating lr=5e-05, train epoch 0.:  50%|█████     | 161/321 [10:48<10:39,  4.00s/it]Validating lr=5e-05, train epoch 0.:  50%|█████     | 162/321 [10:52<10:36,  4.00s/it]Validating lr=5e-05, train epoch 0.:  51%|█████     | 163/321 [10:56<10:31,  4.00s/it]Validating lr=5e-05, train epoch 0.:  51%|█████     | 164/321 [11:00<10:25,  3.99s/it]Validating lr=5e-05, train epoch 0.:  51%|█████▏    | 165/321 [11:04<10:25,  4.01s/it]Validating lr=5e-05, train epoch 0.:  52%|█████▏    | 166/321 [11:08<10:21,  4.01s/it]Validating lr=5e-05, train epoch 0.:  52%|█████▏    | 167/321 [11:12<10:21,  4.04s/it]Validating lr=5e-05, train epoch 0.:  52%|█████▏    | 168/321 [11:16<10:18,  4.04s/it]Validating lr=5e-05, train epoch 0.:  53%|█████▎    | 169/321 [11:20<10:12,  4.03s/it]Validating lr=5e-05, train epoch 0.:  53%|█████▎    | 170/321 [11:24<10:09,  4.04s/it]Validating lr=5e-05, train epoch 0.:  53%|█████▎    | 171/321 [11:28<10:03,  4.03s/it]Validating lr=5e-05, train epoch 0.:  54%|█████▎    | 172/321 [11:32<10:03,  4.05s/it]Validating lr=5e-05, train epoch 0.:  54%|█████▍    | 173/321 [11:36<09:56,  4.03s/it]Validating lr=5e-05, train epoch 0.:  54%|█████▍    | 174/321 [11:40<09:50,  4.02s/it]Validating lr=5e-05, train epoch 0.:  55%|█████▍    | 175/321 [11:44<09:46,  4.02s/it]Validating lr=5e-05, train epoch 0.:  55%|█████▍    | 176/321 [11:48<09:43,  4.02s/it]Validating lr=5e-05, train epoch 0.:  55%|█████▌    | 177/321 [11:52<09:37,  4.01s/it]Validating lr=5e-05, train epoch 0.:  55%|█████▌    | 178/321 [11:56<09:34,  4.02s/it]Validating lr=5e-05, train epoch 0.:  56%|█████▌    | 179/321 [12:00<09:28,  4.00s/it]Validating lr=5e-05, train epoch 0.:  56%|█████▌    | 180/321 [12:04<09:25,  4.01s/it]Validating lr=5e-05, train epoch 0.:  56%|█████▋    | 181/321 [12:08<09:21,  4.01s/it]Validating lr=5e-05, train epoch 0.:  57%|█████▋    | 182/321 [12:12<09:17,  4.01s/it]Validating lr=5e-05, train epoch 0.:  57%|█████▋    | 183/321 [12:16<09:15,  4.03s/it]Validating lr=5e-05, train epoch 0.:  57%|█████▋    | 184/321 [12:20<09:09,  4.01s/it]Validating lr=5e-05, train epoch 0.:  58%|█████▊    | 185/321 [12:24<09:04,  4.01s/it]Validating lr=5e-05, train epoch 0.:  58%|█████▊    | 186/321 [12:28<09:02,  4.02s/it]Validating lr=5e-05, train epoch 0.:  58%|█████▊    | 187/321 [12:32<08:56,  4.01s/it]Validating lr=5e-05, train epoch 0.:  59%|█████▊    | 188/321 [12:36<08:52,  4.01s/it]Validating lr=5e-05, train epoch 0.:  59%|█████▉    | 189/321 [12:40<08:47,  4.00s/it]Validating lr=5e-05, train epoch 0.:  59%|█████▉    | 190/321 [12:44<08:43,  4.00s/it]Validating lr=5e-05, train epoch 0.:  60%|█████▉    | 191/321 [12:48<08:39,  4.00s/it]Validating lr=5e-05, train epoch 0.:  60%|█████▉    | 192/321 [12:52<08:36,  4.00s/it]Validating lr=5e-05, train epoch 0.:  60%|██████    | 193/321 [12:56<08:35,  4.03s/it]Validating lr=5e-05, train epoch 0.:  60%|██████    | 194/321 [13:00<08:30,  4.02s/it]Validating lr=5e-05, train epoch 0.:  61%|██████    | 195/321 [13:04<08:26,  4.02s/it]Validating lr=5e-05, train epoch 0.:  61%|██████    | 196/321 [13:08<08:24,  4.04s/it]Validating lr=5e-05, train epoch 0.:  61%|██████▏   | 197/321 [13:12<08:19,  4.03s/it]Validating lr=5e-05, train epoch 0.:  62%|██████▏   | 198/321 [13:16<08:15,  4.03s/it]Validating lr=5e-05, train epoch 0.:  62%|██████▏   | 199/321 [13:20<08:10,  4.02s/it]Validating lr=5e-05, train epoch 0.:  62%|██████▏   | 200/321 [13:24<08:07,  4.03s/it]Validating lr=5e-05, train epoch 0.:  63%|██████▎   | 201/321 [13:28<08:03,  4.03s/it]Validating lr=5e-05, train epoch 0.:  63%|██████▎   | 202/321 [13:32<07:56,  4.01s/it]Validating lr=5e-05, train epoch 0.:  63%|██████▎   | 203/321 [13:36<07:53,  4.02s/it]Validating lr=5e-05, train epoch 0.:  64%|██████▎   | 204/321 [13:40<07:52,  4.04s/it]Validating lr=5e-05, train epoch 0.:  64%|██████▍   | 205/321 [13:44<07:49,  4.05s/it]Validating lr=5e-05, train epoch 0.:  64%|██████▍   | 206/321 [13:48<07:43,  4.03s/it]Validating lr=5e-05, train epoch 0.:  64%|██████▍   | 207/321 [13:52<07:37,  4.01s/it]Validating lr=5e-05, train epoch 0.:  65%|██████▍   | 208/321 [13:56<07:33,  4.01s/it]Validating lr=5e-05, train epoch 0.:  65%|██████▌   | 209/321 [14:00<07:29,  4.01s/it]Validating lr=5e-05, train epoch 0.:  65%|██████▌   | 210/321 [14:04<07:24,  4.00s/it]Validating lr=5e-05, train epoch 0.:  66%|██████▌   | 211/321 [14:08<07:18,  3.99s/it]Validating lr=5e-05, train epoch 0.:  66%|██████▌   | 212/321 [14:12<07:14,  3.99s/it]Validating lr=5e-05, train epoch 0.:  66%|██████▋   | 213/321 [14:16<07:10,  3.98s/it]Validating lr=5e-05, train epoch 0.:  67%|██████▋   | 214/321 [14:20<07:06,  3.99s/it]Validating lr=5e-05, train epoch 0.:  67%|██████▋   | 215/321 [14:24<07:02,  3.99s/it]Validating lr=5e-05, train epoch 0.:  67%|██████▋   | 216/321 [14:28<06:59,  3.99s/it]Validating lr=5e-05, train epoch 0.:  68%|██████▊   | 217/321 [14:32<06:58,  4.02s/it]Validating lr=5e-05, train epoch 0.:  68%|██████▊   | 218/321 [14:36<06:52,  4.01s/it]Validating lr=5e-05, train epoch 0.:  68%|██████▊   | 219/321 [14:40<06:48,  4.00s/it]Validating lr=5e-05, train epoch 0.:  69%|██████▊   | 220/321 [14:44<06:45,  4.01s/it]Validating lr=5e-05, train epoch 0.:  69%|██████▉   | 221/321 [14:48<06:41,  4.01s/it]Validating lr=5e-05, train epoch 0.:  69%|██████▉   | 222/321 [14:52<06:36,  4.00s/it]Validating lr=5e-05, train epoch 0.:  69%|██████▉   | 223/321 [14:56<06:32,  4.01s/it]Validating lr=5e-05, train epoch 0.:  70%|██████▉   | 224/321 [15:00<06:28,  4.01s/it]Validating lr=5e-05, train epoch 0.:  70%|███████   | 225/321 [15:04<06:23,  3.99s/it]Validating lr=5e-05, train epoch 0.:  70%|███████   | 226/321 [15:08<06:21,  4.02s/it]Validating lr=5e-05, train epoch 0.:  71%|███████   | 227/321 [15:12<06:16,  4.01s/it]Validating lr=5e-05, train epoch 0.:  71%|███████   | 228/321 [15:17<06:13,  4.02s/it]Validating lr=5e-05, train epoch 0.:  71%|███████▏  | 229/321 [15:21<06:08,  4.01s/it]Validating lr=5e-05, train epoch 0.:  72%|███████▏  | 230/321 [15:24<06:03,  4.00s/it]Validating lr=5e-05, train epoch 0.:  72%|███████▏  | 231/321 [15:29<06:01,  4.01s/it]Validating lr=5e-05, train epoch 0.:  72%|███████▏  | 232/321 [15:32<05:55,  4.00s/it]Validating lr=5e-05, train epoch 0.:  73%|███████▎  | 233/321 [15:37<05:52,  4.00s/it]Validating lr=5e-05, train epoch 0.:  73%|███████▎  | 234/321 [15:41<05:48,  4.00s/it]Validating lr=5e-05, train epoch 0.:  73%|███████▎  | 235/321 [15:45<05:44,  4.01s/it]Validating lr=5e-05, train epoch 0.:  74%|███████▎  | 236/321 [15:49<05:41,  4.02s/it]Validating lr=5e-05, train epoch 0.:  74%|███████▍  | 237/321 [15:53<05:39,  4.04s/it]Validating lr=5e-05, train epoch 0.:  74%|███████▍  | 238/321 [15:57<05:35,  4.04s/it]Validating lr=5e-05, train epoch 0.:  74%|███████▍  | 239/321 [16:01<05:29,  4.02s/it]Validating lr=5e-05, train epoch 0.:  75%|███████▍  | 240/321 [16:05<05:24,  4.01s/it]Validating lr=5e-05, train epoch 0.:  75%|███████▌  | 241/321 [16:09<05:19,  4.00s/it]Validating lr=5e-05, train epoch 0.:  75%|███████▌  | 242/321 [16:13<05:15,  3.99s/it]Validating lr=5e-05, train epoch 0.:  76%|███████▌  | 243/321 [16:17<05:11,  4.00s/it]Validating lr=5e-05, train epoch 0.:  76%|███████▌  | 244/321 [16:21<05:08,  4.01s/it]Validating lr=5e-05, train epoch 0.:  76%|███████▋  | 245/321 [16:25<05:05,  4.02s/it]Validating lr=5e-05, train epoch 0.:  77%|███████▋  | 246/321 [16:29<05:01,  4.02s/it]Validating lr=5e-05, train epoch 0.:  77%|███████▋  | 247/321 [16:33<04:58,  4.03s/it]Validating lr=5e-05, train epoch 0.:  77%|███████▋  | 248/321 [16:37<04:53,  4.03s/it]Validating lr=5e-05, train epoch 0.:  78%|███████▊  | 249/321 [16:41<04:48,  4.01s/it]Validating lr=5e-05, train epoch 0.:  78%|███████▊  | 250/321 [16:45<04:43,  3.99s/it]Validating lr=5e-05, train epoch 0.:  78%|███████▊  | 251/321 [16:49<04:39,  3.99s/it]Validating lr=5e-05, train epoch 0.:  79%|███████▊  | 252/321 [16:53<04:37,  4.03s/it]Validating lr=5e-05, train epoch 0.:  79%|███████▉  | 253/321 [16:57<04:32,  4.01s/it]Validating lr=5e-05, train epoch 0.:  79%|███████▉  | 254/321 [17:01<04:28,  4.01s/it]Validating lr=5e-05, train epoch 0.:  79%|███████▉  | 255/321 [17:05<04:24,  4.01s/it]Validating lr=5e-05, train epoch 0.:  80%|███████▉  | 256/321 [17:09<04:19,  4.00s/it]Validating lr=5e-05, train epoch 0.:  80%|████████  | 257/321 [17:13<04:16,  4.01s/it]Validating lr=5e-05, train epoch 0.:  80%|████████  | 258/321 [17:17<04:11,  4.00s/it]Validating lr=5e-05, train epoch 0.:  81%|████████  | 259/321 [17:21<04:08,  4.01s/it]Validating lr=5e-05, train epoch 0.:  81%|████████  | 260/321 [17:25<04:05,  4.03s/it]Validating lr=5e-05, train epoch 0.:  81%|████████▏ | 261/321 [17:29<04:01,  4.02s/it]Validating lr=5e-05, train epoch 0.:  82%|████████▏ | 262/321 [17:33<03:57,  4.02s/it]Validating lr=5e-05, train epoch 0.:  82%|████████▏ | 263/321 [17:37<03:53,  4.03s/it]Validating lr=5e-05, train epoch 0.:  82%|████████▏ | 264/321 [17:41<03:49,  4.02s/it]Validating lr=5e-05, train epoch 0.:  83%|████████▎ | 265/321 [17:45<03:45,  4.02s/it]Validating lr=5e-05, train epoch 0.:  83%|████████▎ | 266/321 [17:49<03:41,  4.02s/it]Validating lr=5e-05, train epoch 0.:  83%|████████▎ | 267/321 [17:53<03:37,  4.02s/it]Validating lr=5e-05, train epoch 0.:  83%|████████▎ | 268/321 [17:57<03:32,  4.01s/it]Validating lr=5e-05, train epoch 0.:  84%|████████▍ | 269/321 [18:01<03:29,  4.02s/it]Validating lr=5e-05, train epoch 0.:  84%|████████▍ | 270/321 [18:05<03:25,  4.03s/it]Validating lr=5e-05, train epoch 0.:  84%|████████▍ | 271/321 [18:09<03:22,  4.05s/it]Validating lr=5e-05, train epoch 0.:  85%|████████▍ | 272/321 [18:13<03:18,  4.06s/it]Validating lr=5e-05, train epoch 0.:  85%|████████▌ | 273/321 [18:17<03:13,  4.04s/it]Validating lr=5e-05, train epoch 0.:  85%|████████▌ | 274/321 [18:21<03:10,  4.05s/it]Validating lr=5e-05, train epoch 0.:  86%|████████▌ | 275/321 [18:25<03:05,  4.03s/it]Validating lr=5e-05, train epoch 0.:  86%|████████▌ | 276/321 [18:29<03:01,  4.04s/it]Validating lr=5e-05, train epoch 0.:  86%|████████▋ | 277/321 [18:33<02:56,  4.02s/it]Validating lr=5e-05, train epoch 0.:  87%|████████▋ | 278/321 [18:37<02:53,  4.04s/it]Validating lr=5e-05, train epoch 0.:  87%|████████▋ | 279/321 [18:41<02:48,  4.02s/it]Validating lr=5e-05, train epoch 0.:  87%|████████▋ | 280/321 [18:45<02:44,  4.02s/it]Validating lr=5e-05, train epoch 0.:  88%|████████▊ | 281/321 [18:49<02:40,  4.01s/it]Validating lr=5e-05, train epoch 0.:  88%|████████▊ | 282/321 [18:53<02:36,  4.01s/it]Validating lr=5e-05, train epoch 0.:  88%|████████▊ | 283/321 [18:58<02:32,  4.03s/it]Validating lr=5e-05, train epoch 0.:  88%|████████▊ | 284/321 [19:02<02:29,  4.03s/it]Validating lr=5e-05, train epoch 0.:  89%|████████▉ | 285/321 [19:06<02:24,  4.02s/it]Validating lr=5e-05, train epoch 0.:  89%|████████▉ | 286/321 [19:10<02:21,  4.03s/it]Validating lr=5e-05, train epoch 0.:  89%|████████▉ | 287/321 [19:14<02:16,  4.03s/it]Validating lr=5e-05, train epoch 0.:  90%|████████▉ | 288/321 [19:18<02:12,  4.02s/it]Validating lr=5e-05, train epoch 0.:  90%|█████████ | 289/321 [19:22<02:08,  4.02s/it]Validating lr=5e-05, train epoch 0.:  90%|█████████ | 290/321 [19:26<02:04,  4.03s/it]Validating lr=5e-05, train epoch 0.:  91%|█████████ | 291/321 [19:30<02:00,  4.02s/it]Validating lr=5e-05, train epoch 0.:  91%|█████████ | 292/321 [19:34<01:56,  4.03s/it]Validating lr=5e-05, train epoch 0.:  91%|█████████▏| 293/321 [19:38<01:52,  4.02s/it]Validating lr=5e-05, train epoch 0.:  92%|█████████▏| 294/321 [19:42<01:48,  4.02s/it]Validating lr=5e-05, train epoch 0.:  92%|█████████▏| 295/321 [19:46<01:43,  4.00s/it]Validating lr=5e-05, train epoch 0.:  92%|█████████▏| 296/321 [19:50<01:39,  4.00s/it]Validating lr=5e-05, train epoch 0.:  93%|█████████▎| 297/321 [19:54<01:35,  3.99s/it]Validating lr=5e-05, train epoch 0.:  93%|█████████▎| 298/321 [19:58<01:32,  4.01s/it]Validating lr=5e-05, train epoch 0.:  93%|█████████▎| 299/321 [20:02<01:28,  4.01s/it]Validating lr=5e-05, train epoch 0.:  93%|█████████▎| 300/321 [20:06<01:24,  4.01s/it]Validating lr=5e-05, train epoch 0.:  94%|█████████▍| 301/321 [20:10<01:20,  4.01s/it]Validating lr=5e-05, train epoch 0.:  94%|█████████▍| 302/321 [20:14<01:16,  4.02s/it]Validating lr=5e-05, train epoch 0.:  94%|█████████▍| 303/321 [20:18<01:12,  4.01s/it]Validating lr=5e-05, train epoch 0.:  95%|█████████▍| 304/321 [20:22<01:08,  4.03s/it]Validating lr=5e-05, train epoch 0.:  95%|█████████▌| 305/321 [20:26<01:04,  4.03s/it]Validating lr=5e-05, train epoch 0.:  95%|█████████▌| 306/321 [20:30<01:00,  4.04s/it]Validating lr=5e-05, train epoch 0.:  96%|█████████▌| 307/321 [20:34<00:56,  4.05s/it]Validating lr=5e-05, train epoch 0.:  96%|█████████▌| 308/321 [20:38<00:52,  4.05s/it]Validating lr=5e-05, train epoch 0.:  96%|█████████▋| 309/321 [20:42<00:48,  4.04s/it]Validating lr=5e-05, train epoch 0.:  97%|█████████▋| 310/321 [20:46<00:44,  4.04s/it]Validating lr=5e-05, train epoch 0.:  97%|█████████▋| 311/321 [20:50<00:40,  4.07s/it]Validating lr=5e-05, train epoch 0.:  97%|█████████▋| 312/321 [20:54<00:36,  4.06s/it]Validating lr=5e-05, train epoch 0.:  98%|█████████▊| 313/321 [20:58<00:32,  4.05s/it]Validating lr=5e-05, train epoch 0.:  98%|█████████▊| 314/321 [21:03<00:28,  4.07s/it]Validating lr=5e-05, train epoch 0.:  98%|█████████▊| 315/321 [21:07<00:24,  4.05s/it]Validating lr=5e-05, train epoch 0.:  98%|█████████▊| 316/321 [21:11<00:20,  4.05s/it]Validating lr=5e-05, train epoch 0.:  99%|█████████▉| 317/321 [21:15<00:16,  4.05s/it]Validating lr=5e-05, train epoch 0.:  99%|█████████▉| 318/321 [21:19<00:12,  4.01s/it]Validating lr=5e-05, train epoch 0.:  99%|█████████▉| 319/321 [21:23<00:08,  4.02s/it]Validating lr=5e-05, train epoch 0.: 100%|█████████▉| 320/321 [21:27<00:04,  4.05s/it]Validating lr=5e-05, train epoch 0.: 100%|██████████| 321/321 [21:31<00:00,  4.03s/it]Validating lr=5e-05, train epoch 0.: 100%|██████████| 321/321 [21:31<00:00,  4.02s/it]
Validating lr=5e-05, train epoch 1.:   0%|          | 0/321 [00:00<?, ?it/s]Validating lr=5e-05, train epoch 1.:   0%|          | 1/321 [00:04<21:22,  4.01s/it]Validating lr=5e-05, train epoch 1.:   1%|          | 2/321 [00:08<21:22,  4.02s/it]Validating lr=5e-05, train epoch 1.:   1%|          | 3/321 [00:12<21:12,  4.00s/it]Validating lr=5e-05, train epoch 1.:   1%|          | 4/321 [00:16<21:06,  4.00s/it]Validating lr=5e-05, train epoch 1.:   2%|▏         | 5/321 [00:19<20:56,  3.98s/it]Validating lr=5e-05, train epoch 1.:   2%|▏         | 6/321 [00:23<20:53,  3.98s/it]Validating lr=5e-05, train epoch 1.:   2%|▏         | 7/321 [00:27<20:47,  3.97s/it]Validating lr=5e-05, train epoch 1.:   2%|▏         | 8/321 [00:31<20:48,  3.99s/it]Validating lr=5e-05, train epoch 1.:   3%|▎         | 9/321 [00:35<20:45,  3.99s/it]Validating lr=5e-05, train epoch 1.:   3%|▎         | 10/321 [00:39<20:41,  3.99s/it]Validating lr=5e-05, train epoch 1.:   3%|▎         | 11/321 [00:43<20:44,  4.01s/it]Validating lr=5e-05, train epoch 1.:   4%|▎         | 12/321 [00:48<20:44,  4.03s/it]Validating lr=5e-05, train epoch 1.:   4%|▍         | 13/321 [00:52<20:36,  4.01s/it]Validating lr=5e-05, train epoch 1.:   4%|▍         | 14/321 [00:56<20:30,  4.01s/it]Validating lr=5e-05, train epoch 1.:   5%|▍         | 15/321 [01:00<20:33,  4.03s/it]Validating lr=5e-05, train epoch 1.:   5%|▍         | 16/321 [01:04<20:19,  4.00s/it]Validating lr=5e-05, train epoch 1.:   5%|▌         | 17/321 [01:08<20:15,  4.00s/it]Validating lr=5e-05, train epoch 1.:   6%|▌         | 18/321 [01:12<20:16,  4.01s/it]Validating lr=5e-05, train epoch 1.:   6%|▌         | 19/321 [01:16<20:10,  4.01s/it]Validating lr=5e-05, train epoch 1.:   6%|▌         | 20/321 [01:20<20:03,  4.00s/it]Validating lr=5e-05, train epoch 1.:   7%|▋         | 21/321 [01:24<19:58,  3.99s/it]Validating lr=5e-05, train epoch 1.:   7%|▋         | 22/321 [01:27<19:53,  3.99s/it]Validating lr=5e-05, train epoch 1.:   7%|▋         | 23/321 [01:31<19:49,  3.99s/it]Validating lr=5e-05, train epoch 1.:   7%|▋         | 24/321 [01:35<19:40,  3.97s/it]Validating lr=5e-05, train epoch 1.:   8%|▊         | 25/321 [01:39<19:37,  3.98s/it]Validating lr=5e-05, train epoch 1.:   8%|▊         | 26/321 [01:44<19:43,  4.01s/it]Validating lr=5e-05, train epoch 1.:   8%|▊         | 27/321 [01:48<19:40,  4.01s/it]Validating lr=5e-05, train epoch 1.:   9%|▊         | 28/321 [01:52<19:35,  4.01s/it]Validating lr=5e-05, train epoch 1.:   9%|▉         | 29/321 [01:56<19:35,  4.03s/it]Validating lr=5e-05, train epoch 1.:   9%|▉         | 30/321 [02:00<19:29,  4.02s/it]Validating lr=5e-05, train epoch 1.:  10%|▉         | 31/321 [02:04<19:25,  4.02s/it]Validating lr=5e-05, train epoch 1.:  10%|▉         | 32/321 [02:08<19:21,  4.02s/it]Validating lr=5e-05, train epoch 1.:  10%|█         | 33/321 [02:12<19:14,  4.01s/it]Validating lr=5e-05, train epoch 1.:  11%|█         | 34/321 [02:16<19:11,  4.01s/it]Validating lr=5e-05, train epoch 1.:  11%|█         | 35/321 [02:20<19:09,  4.02s/it]Validating lr=5e-05, train epoch 1.:  11%|█         | 36/321 [02:24<19:12,  4.04s/it]Validating lr=5e-05, train epoch 1.:  12%|█▏        | 37/321 [02:28<19:06,  4.04s/it]Validating lr=5e-05, train epoch 1.:  12%|█▏        | 38/321 [02:32<18:59,  4.03s/it]Validating lr=5e-05, train epoch 1.:  12%|█▏        | 39/321 [02:36<18:54,  4.02s/it]Validating lr=5e-05, train epoch 1.:  12%|█▏        | 40/321 [02:40<18:54,  4.04s/it]Validating lr=5e-05, train epoch 1.:  13%|█▎        | 41/321 [02:44<18:42,  4.01s/it]Validating lr=5e-05, train epoch 1.:  13%|█▎        | 42/321 [02:48<18:37,  4.01s/it]Validating lr=5e-05, train epoch 1.:  13%|█▎        | 43/321 [02:52<18:32,  4.00s/it]Validating lr=5e-05, train epoch 1.:  14%|█▎        | 44/321 [02:56<18:32,  4.02s/it]Validating lr=5e-05, train epoch 1.:  14%|█▍        | 45/321 [03:00<18:26,  4.01s/it]Validating lr=5e-05, train epoch 1.:  14%|█▍        | 46/321 [03:04<18:21,  4.01s/it]Validating lr=5e-05, train epoch 1.:  15%|█▍        | 47/321 [03:08<18:16,  4.00s/it]Validating lr=5e-05, train epoch 1.:  15%|█▍        | 48/321 [03:12<18:19,  4.03s/it]Validating lr=5e-05, train epoch 1.:  15%|█▌        | 49/321 [03:16<18:13,  4.02s/it]Validating lr=5e-05, train epoch 1.:  16%|█▌        | 50/321 [03:20<18:08,  4.02s/it]Validating lr=5e-05, train epoch 1.:  16%|█▌        | 51/321 [03:24<18:02,  4.01s/it]Validating lr=5e-05, train epoch 1.:  16%|█▌        | 52/321 [03:28<18:02,  4.02s/it]Validating lr=5e-05, train epoch 1.:  17%|█▋        | 53/321 [03:32<17:58,  4.03s/it]Validating lr=5e-05, train epoch 1.:  17%|█▋        | 54/321 [03:36<17:57,  4.03s/it]Validating lr=5e-05, train epoch 1.:  17%|█▋        | 55/321 [03:40<17:50,  4.02s/it]Validating lr=5e-05, train epoch 1.:  17%|█▋        | 56/321 [03:44<17:42,  4.01s/it]Validating lr=5e-05, train epoch 1.:  18%|█▊        | 57/321 [03:48<17:50,  4.05s/it]Validating lr=5e-05, train epoch 1.:  18%|█▊        | 58/321 [03:52<17:42,  4.04s/it]Validating lr=5e-05, train epoch 1.:  18%|█▊        | 59/321 [03:56<17:33,  4.02s/it]Validating lr=5e-05, train epoch 1.:  19%|█▊        | 60/321 [04:00<17:27,  4.01s/it]Validating lr=5e-05, train epoch 1.:  19%|█▉        | 61/321 [04:04<17:25,  4.02s/it]Validating lr=5e-05, train epoch 1.:  19%|█▉        | 62/321 [04:08<17:26,  4.04s/it]Validating lr=5e-05, train epoch 1.:  20%|█▉        | 63/321 [04:12<17:23,  4.04s/it]Validating lr=5e-05, train epoch 1.:  20%|█▉        | 64/321 [04:16<17:14,  4.02s/it]Validating lr=5e-05, train epoch 1.:  20%|██        | 65/321 [04:20<17:06,  4.01s/it]Validating lr=5e-05, train epoch 1.:  21%|██        | 66/321 [04:24<17:02,  4.01s/it]Validating lr=5e-05, train epoch 1.:  21%|██        | 67/321 [04:28<17:04,  4.04s/it]Validating lr=5e-05, train epoch 1.:  21%|██        | 68/321 [04:32<16:57,  4.02s/it]Validating lr=5e-05, train epoch 1.:  21%|██▏       | 69/321 [04:36<16:49,  4.01s/it]Validating lr=5e-05, train epoch 1.:  22%|██▏       | 70/321 [04:40<16:47,  4.02s/it]Validating lr=5e-05, train epoch 1.:  22%|██▏       | 71/321 [04:44<16:42,  4.01s/it]Validating lr=5e-05, train epoch 1.:  22%|██▏       | 72/321 [04:48<16:35,  4.00s/it]Validating lr=5e-05, train epoch 1.:  23%|██▎       | 73/321 [04:52<16:35,  4.01s/it]Validating lr=5e-05, train epoch 1.:  23%|██▎       | 74/321 [04:56<16:27,  4.00s/it]Validating lr=5e-05, train epoch 1.:  23%|██▎       | 75/321 [05:00<16:23,  4.00s/it]Validating lr=5e-05, train epoch 1.:  24%|██▎       | 76/321 [05:04<16:15,  3.98s/it]Validating lr=5e-05, train epoch 1.:  24%|██▍       | 77/321 [05:08<16:12,  3.99s/it]Validating lr=5e-05, train epoch 1.:  24%|██▍       | 78/321 [05:12<16:05,  3.97s/it]Validating lr=5e-05, train epoch 1.:  25%|██▍       | 79/321 [05:16<16:03,  3.98s/it]Validating lr=5e-05, train epoch 1.:  25%|██▍       | 80/321 [05:20<15:59,  3.98s/it]Validating lr=5e-05, train epoch 1.:  25%|██▌       | 81/321 [05:24<15:55,  3.98s/it]Validating lr=5e-05, train epoch 1.:  26%|██▌       | 82/321 [05:28<15:52,  3.98s/it]Validating lr=5e-05, train epoch 1.:  26%|██▌       | 83/321 [05:32<15:51,  4.00s/it]Validating lr=5e-05, train epoch 1.:  26%|██▌       | 84/321 [05:36<15:47,  4.00s/it]Validating lr=5e-05, train epoch 1.:  26%|██▋       | 85/321 [05:40<15:44,  4.00s/it]Validating lr=5e-05, train epoch 1.:  27%|██▋       | 86/321 [05:44<15:46,  4.03s/it]Validating lr=5e-05, train epoch 1.:  27%|██▋       | 87/321 [05:48<15:42,  4.03s/it]Validating lr=5e-05, train epoch 1.:  27%|██▋       | 88/321 [05:53<15:45,  4.06s/it]Validating lr=5e-05, train epoch 1.:  28%|██▊       | 89/321 [05:57<15:36,  4.04s/it]Validating lr=5e-05, train epoch 1.:  28%|██▊       | 90/321 [06:01<15:40,  4.07s/it]Validating lr=5e-05, train epoch 1.:  28%|██▊       | 91/321 [06:05<15:34,  4.06s/it]Validating lr=5e-05, train epoch 1.:  29%|██▊       | 92/321 [06:09<15:25,  4.04s/it]Validating lr=5e-05, train epoch 1.:  29%|██▉       | 93/321 [06:13<15:18,  4.03s/it]Validating lr=5e-05, train epoch 1.:  29%|██▉       | 94/321 [06:17<15:10,  4.01s/it]Validating lr=5e-05, train epoch 1.:  30%|██▉       | 95/321 [06:21<15:07,  4.02s/it]Validating lr=5e-05, train epoch 1.:  30%|██▉       | 96/321 [06:25<15:02,  4.01s/it]Validating lr=5e-05, train epoch 1.:  30%|███       | 97/321 [06:29<14:57,  4.01s/it]Validating lr=5e-05, train epoch 1.:  31%|███       | 98/321 [06:33<15:01,  4.04s/it]Validating lr=5e-05, train epoch 1.:  31%|███       | 99/321 [06:37<14:55,  4.03s/it]Validating lr=5e-05, train epoch 1.:  31%|███       | 100/321 [06:41<14:52,  4.04s/it]Validating lr=5e-05, train epoch 1.:  31%|███▏      | 101/321 [06:45<14:45,  4.03s/it]Validating lr=5e-05, train epoch 1.:  32%|███▏      | 102/321 [06:49<14:42,  4.03s/it]Validating lr=5e-05, train epoch 1.:  32%|███▏      | 103/321 [06:53<14:36,  4.02s/it]Validating lr=5e-05, train epoch 1.:  32%|███▏      | 104/321 [06:57<14:31,  4.02s/it]Validating lr=5e-05, train epoch 1.:  33%|███▎      | 105/321 [07:01<14:28,  4.02s/it]Validating lr=5e-05, train epoch 1.:  33%|███▎      | 106/321 [07:05<14:21,  4.01s/it]Validating lr=5e-05, train epoch 1.:  33%|███▎      | 107/321 [07:09<14:20,  4.02s/it]Validating lr=5e-05, train epoch 1.:  34%|███▎      | 108/321 [07:13<14:18,  4.03s/it]Validating lr=5e-05, train epoch 1.:  34%|███▍      | 109/321 [07:17<14:15,  4.03s/it]Validating lr=5e-05, train epoch 1.:  34%|███▍      | 110/321 [07:21<14:10,  4.03s/it]Validating lr=5e-05, train epoch 1.:  35%|███▍      | 111/321 [07:25<14:12,  4.06s/it]Validating lr=5e-05, train epoch 1.:  35%|███▍      | 112/321 [07:29<14:04,  4.04s/it]Validating lr=5e-05, train epoch 1.:  35%|███▌      | 113/321 [07:33<14:01,  4.04s/it]Validating lr=5e-05, train epoch 1.:  36%|███▌      | 114/321 [07:37<13:54,  4.03s/it]Validating lr=5e-05, train epoch 1.:  36%|███▌      | 115/321 [07:41<13:46,  4.01s/it]Validating lr=5e-05, train epoch 1.:  36%|███▌      | 116/321 [07:45<13:43,  4.02s/it]Validating lr=5e-05, train epoch 1.:  36%|███▋      | 117/321 [07:49<13:38,  4.01s/it]Validating lr=5e-05, train epoch 1.:  37%|███▋      | 118/321 [07:53<13:33,  4.01s/it]Validating lr=5e-05, train epoch 1.:  37%|███▋      | 119/321 [07:57<13:31,  4.02s/it]Validating lr=5e-05, train epoch 1.:  37%|███▋      | 120/321 [08:01<13:27,  4.02s/it]Validating lr=5e-05, train epoch 1.:  38%|███▊      | 121/321 [08:05<13:22,  4.01s/it]Validating lr=5e-05, train epoch 1.:  38%|███▊      | 122/321 [08:09<13:21,  4.03s/it]Validating lr=5e-05, train epoch 1.:  38%|███▊      | 123/321 [08:13<13:19,  4.04s/it]Validating lr=5e-05, train epoch 1.:  39%|███▊      | 124/321 [08:17<13:13,  4.03s/it]Validating lr=5e-05, train epoch 1.:  39%|███▉      | 125/321 [08:21<13:07,  4.02s/it]Validating lr=5e-05, train epoch 1.:  39%|███▉      | 126/321 [08:25<13:02,  4.01s/it]Validating lr=5e-05, train epoch 1.:  40%|███▉      | 127/321 [08:29<12:56,  4.00s/it]Validating lr=5e-05, train epoch 1.:  40%|███▉      | 128/321 [08:33<12:51,  4.00s/it]Validating lr=5e-05, train epoch 1.:  40%|████      | 129/321 [08:37<12:49,  4.01s/it]Validating lr=5e-05, train epoch 1.:  40%|████      | 130/321 [08:41<12:44,  4.00s/it]Validating lr=5e-05, train epoch 1.:  41%|████      | 131/321 [08:45<12:40,  4.00s/it]Validating lr=5e-05, train epoch 1.:  41%|████      | 132/321 [08:49<12:37,  4.01s/it]Validating lr=5e-05, train epoch 1.:  41%|████▏     | 133/321 [08:53<12:32,  4.01s/it]Validating lr=5e-05, train epoch 1.:  42%|████▏     | 134/321 [08:57<12:30,  4.01s/it]Validating lr=5e-05, train epoch 1.:  42%|████▏     | 135/321 [09:01<12:24,  4.00s/it]Validating lr=5e-05, train epoch 1.:  42%|████▏     | 136/321 [09:05<12:21,  4.01s/it]Validating lr=5e-05, train epoch 1.:  43%|████▎     | 137/321 [09:10<12:24,  4.05s/it]Validating lr=5e-05, train epoch 1.:  43%|████▎     | 138/321 [09:14<12:19,  4.04s/it]Validating lr=5e-05, train epoch 1.:  43%|████▎     | 139/321 [09:18<12:17,  4.05s/it]Validating lr=5e-05, train epoch 1.:  44%|████▎     | 140/321 [09:22<12:16,  4.07s/it]Validating lr=5e-05, train epoch 1.:  44%|████▍     | 141/321 [09:26<12:10,  4.06s/it]Validating lr=5e-05, train epoch 1.:  44%|████▍     | 142/321 [09:30<12:04,  4.05s/it]Validating lr=5e-05, train epoch 1.:  45%|████▍     | 143/321 [09:34<12:00,  4.05s/it]Validating lr=5e-05, train epoch 1.:  45%|████▍     | 144/321 [09:38<11:55,  4.04s/it]Validating lr=5e-05, train epoch 1.:  45%|████▌     | 145/321 [09:42<11:50,  4.04s/it]Validating lr=5e-05, train epoch 1.:  45%|████▌     | 146/321 [09:46<11:45,  4.03s/it]Validating lr=5e-05, train epoch 1.:  46%|████▌     | 147/321 [09:50<11:41,  4.03s/it]Validating lr=5e-05, train epoch 1.:  46%|████▌     | 148/321 [09:54<11:33,  4.01s/it]Validating lr=5e-05, train epoch 1.:  46%|████▋     | 149/321 [09:58<11:28,  4.00s/it]Validating lr=5e-05, train epoch 1.:  47%|████▋     | 150/321 [10:02<11:26,  4.01s/it]Validating lr=5e-05, train epoch 1.:  47%|████▋     | 151/321 [10:06<11:22,  4.01s/it]Validating lr=5e-05, train epoch 1.:  47%|████▋     | 152/321 [10:10<11:17,  4.01s/it]Validating lr=5e-05, train epoch 1.:  48%|████▊     | 153/321 [10:14<11:12,  4.00s/it]Validating lr=5e-05, train epoch 1.:  48%|████▊     | 154/321 [10:18<11:08,  4.01s/it]Validating lr=5e-05, train epoch 1.:  48%|████▊     | 155/321 [10:22<11:04,  4.00s/it]Validating lr=5e-05, train epoch 1.:  49%|████▊     | 156/321 [10:26<11:01,  4.01s/it]Validating lr=5e-05, train epoch 1.:  49%|████▉     | 157/321 [10:30<11:00,  4.02s/it]Validating lr=5e-05, train epoch 1.:  49%|████▉     | 158/321 [10:34<10:54,  4.01s/it]Validating lr=5e-05, train epoch 1.:  50%|████▉     | 159/321 [10:38<10:50,  4.01s/it]Validating lr=5e-05, train epoch 1.:  50%|████▉     | 160/321 [10:42<10:44,  4.00s/it]Validating lr=5e-05, train epoch 1.:  50%|█████     | 161/321 [10:46<10:39,  3.99s/it]Validating lr=5e-05, train epoch 1.:  50%|█████     | 162/321 [10:50<10:33,  3.99s/it]Validating lr=5e-05, train epoch 1.:  51%|█████     | 163/321 [10:54<10:30,  3.99s/it]Validating lr=5e-05, train epoch 1.:  51%|█████     | 164/321 [10:58<10:27,  4.00s/it]Validating lr=5e-05, train epoch 1.:  51%|█████▏    | 165/321 [11:02<10:24,  4.00s/it]Validating lr=5e-05, train epoch 1.:  52%|█████▏    | 166/321 [11:06<10:20,  4.00s/it]Validating lr=5e-05, train epoch 1.:  52%|█████▏    | 167/321 [11:10<10:16,  4.00s/it]Validating lr=5e-05, train epoch 1.:  52%|█████▏    | 168/321 [11:14<10:13,  4.01s/it]Validating lr=5e-05, train epoch 1.:  53%|█████▎    | 169/321 [11:18<10:09,  4.01s/it]Validating lr=5e-05, train epoch 1.:  53%|█████▎    | 170/321 [11:22<10:05,  4.01s/it]Validating lr=5e-05, train epoch 1.:  53%|█████▎    | 171/321 [11:26<10:05,  4.03s/it]Validating lr=5e-05, train epoch 1.:  54%|█████▎    | 172/321 [11:30<09:59,  4.02s/it]Validating lr=5e-05, train epoch 1.:  54%|█████▍    | 173/321 [11:34<09:55,  4.03s/it]Validating lr=5e-05, train epoch 1.:  54%|█████▍    | 174/321 [11:38<09:55,  4.05s/it]Validating lr=5e-05, train epoch 1.:  55%|█████▍    | 175/321 [11:42<09:49,  4.04s/it]Validating lr=5e-05, train epoch 1.:  55%|█████▍    | 176/321 [11:46<09:46,  4.04s/it]Validating lr=5e-05, train epoch 1.:  55%|█████▌    | 177/321 [11:51<09:43,  4.05s/it]Validating lr=5e-05, train epoch 1.:  55%|█████▌    | 178/321 [11:55<09:37,  4.04s/it]Validating lr=5e-05, train epoch 1.:  56%|█████▌    | 179/321 [11:58<09:31,  4.02s/it]Validating lr=5e-05, train epoch 1.:  56%|█████▌    | 180/321 [12:03<09:28,  4.03s/it]Validating lr=5e-05, train epoch 1.:  56%|█████▋    | 181/321 [12:07<09:24,  4.03s/it]Validating lr=5e-05, train epoch 1.:  57%|█████▋    | 182/321 [12:11<09:18,  4.02s/it]Validating lr=5e-05, train epoch 1.:  57%|█████▋    | 183/321 [12:15<09:13,  4.01s/it]Validating lr=5e-05, train epoch 1.:  57%|█████▋    | 184/321 [12:19<09:09,  4.01s/it]Validating lr=5e-05, train epoch 1.:  58%|█████▊    | 185/321 [12:23<09:05,  4.01s/it]Validating lr=5e-05, train epoch 1.:  58%|█████▊    | 186/321 [12:27<09:01,  4.01s/it]Validating lr=5e-05, train epoch 1.:  58%|█████▊    | 187/321 [12:31<08:58,  4.02s/it]Validating lr=5e-05, train epoch 1.:  59%|█████▊    | 188/321 [12:35<08:53,  4.01s/it]Validating lr=5e-05, train epoch 1.:  59%|█████▉    | 189/321 [12:39<08:50,  4.02s/it]Validating lr=5e-05, train epoch 1.:  59%|█████▉    | 190/321 [12:43<08:45,  4.01s/it]Validating lr=5e-05, train epoch 1.:  60%|█████▉    | 191/321 [12:47<08:43,  4.03s/it]Validating lr=5e-05, train epoch 1.:  60%|█████▉    | 192/321 [12:51<08:39,  4.03s/it]Validating lr=5e-05, train epoch 1.:  60%|██████    | 193/321 [12:55<08:34,  4.02s/it]Validating lr=5e-05, train epoch 1.:  60%|██████    | 194/321 [12:59<08:36,  4.07s/it]Validating lr=5e-05, train epoch 1.:  61%|██████    | 195/321 [13:03<08:31,  4.06s/it]Validating lr=5e-05, train epoch 1.:  61%|██████    | 196/321 [13:07<08:27,  4.06s/it]Validating lr=5e-05, train epoch 1.:  61%|██████▏   | 197/321 [13:11<08:22,  4.05s/it]Validating lr=5e-05, train epoch 1.:  62%|██████▏   | 198/321 [13:15<08:14,  4.02s/it]Validating lr=5e-05, train epoch 1.:  62%|██████▏   | 199/321 [13:19<08:12,  4.04s/it]Validating lr=5e-05, train epoch 1.:  62%|██████▏   | 200/321 [13:23<08:08,  4.04s/it]Validating lr=5e-05, train epoch 1.:  63%|██████▎   | 201/321 [13:27<08:04,  4.04s/it]Validating lr=5e-05, train epoch 1.:  63%|██████▎   | 202/321 [13:31<08:02,  4.05s/it]Validating lr=5e-05, train epoch 1.:  63%|██████▎   | 203/321 [13:35<07:57,  4.04s/it]Validating lr=5e-05, train epoch 1.:  64%|██████▎   | 204/321 [13:39<07:50,  4.02s/it]Validating lr=5e-05, train epoch 1.:  64%|██████▍   | 205/321 [13:43<07:45,  4.02s/it]Validating lr=5e-05, train epoch 1.:  64%|██████▍   | 206/321 [13:47<07:42,  4.02s/it]Validating lr=5e-05, train epoch 1.:  64%|██████▍   | 207/321 [13:51<07:38,  4.02s/it]Validating lr=5e-05, train epoch 1.:  65%|██████▍   | 208/321 [13:55<07:32,  4.01s/it]Validating lr=5e-05, train epoch 1.:  65%|██████▌   | 209/321 [13:59<07:30,  4.02s/it]Validating lr=5e-05, train epoch 1.:  65%|██████▌   | 210/321 [14:03<07:25,  4.02s/it]Validating lr=5e-05, train epoch 1.:  66%|██████▌   | 211/321 [14:07<07:24,  4.04s/it]Validating lr=5e-05, train epoch 1.:  66%|██████▌   | 212/321 [14:11<07:20,  4.05s/it]Validating lr=5e-05, train epoch 1.:  66%|██████▋   | 213/321 [14:15<07:15,  4.04s/it]Validating lr=5e-05, train epoch 1.:  67%|██████▋   | 214/321 [14:20<07:10,  4.03s/it]Validating lr=5e-05, train epoch 1.:  67%|██████▋   | 215/321 [14:24<07:08,  4.04s/it]Validating lr=5e-05, train epoch 1.:  67%|██████▋   | 216/321 [14:28<07:05,  4.05s/it]Validating lr=5e-05, train epoch 1.:  68%|██████▊   | 217/321 [14:32<06:57,  4.02s/it]Validating lr=5e-05, train epoch 1.:  68%|██████▊   | 218/321 [14:36<06:52,  4.00s/it]Validating lr=5e-05, train epoch 1.:  68%|██████▊   | 219/321 [14:40<06:49,  4.02s/it]Validating lr=5e-05, train epoch 1.:  69%|██████▊   | 220/321 [14:44<06:46,  4.03s/it]Validating lr=5e-05, train epoch 1.:  69%|██████▉   | 221/321 [14:48<06:43,  4.03s/it]Validating lr=5e-05, train epoch 1.:  69%|██████▉   | 222/321 [14:52<06:39,  4.03s/it]Validating lr=5e-05, train epoch 1.:  69%|██████▉   | 223/321 [14:56<06:35,  4.04s/it]Validating lr=5e-05, train epoch 1.:  70%|██████▉   | 224/321 [15:00<06:30,  4.03s/it]Validating lr=5e-05, train epoch 1.:  70%|███████   | 225/321 [15:04<06:28,  4.05s/it]Validating lr=5e-05, train epoch 1.:  70%|███████   | 226/321 [15:08<06:22,  4.03s/it]Validating lr=5e-05, train epoch 1.:  71%|███████   | 227/321 [15:12<06:18,  4.03s/it]Validating lr=5e-05, train epoch 1.:  71%|███████   | 228/321 [15:16<06:13,  4.01s/it]Validating lr=5e-05, train epoch 1.:  71%|███████▏  | 229/321 [15:20<06:09,  4.01s/it]Validating lr=5e-05, train epoch 1.:  72%|███████▏  | 230/321 [15:24<06:05,  4.02s/it]Validating lr=5e-05, train epoch 1.:  72%|███████▏  | 231/321 [15:28<05:59,  4.00s/it]Validating lr=5e-05, train epoch 1.:  72%|███████▏  | 232/321 [15:32<05:57,  4.02s/it]Validating lr=5e-05, train epoch 1.:  73%|███████▎  | 233/321 [15:36<05:53,  4.02s/it]Validating lr=5e-05, train epoch 1.:  73%|███████▎  | 234/321 [15:40<05:48,  4.01s/it]Validating lr=5e-05, train epoch 1.:  73%|███████▎  | 235/321 [15:44<05:44,  4.00s/it]Validating lr=5e-05, train epoch 1.:  74%|███████▎  | 236/321 [15:48<05:41,  4.02s/it]Validating lr=5e-05, train epoch 1.:  74%|███████▍  | 237/321 [15:52<05:37,  4.01s/it]Validating lr=5e-05, train epoch 1.:  74%|███████▍  | 238/321 [15:56<05:31,  3.99s/it]Validating lr=5e-05, train epoch 1.:  74%|███████▍  | 239/321 [16:00<05:29,  4.01s/it]Validating lr=5e-05, train epoch 1.:  75%|███████▍  | 240/321 [16:04<05:24,  4.00s/it]Validating lr=5e-05, train epoch 1.:  75%|███████▌  | 241/321 [16:08<05:20,  4.00s/it]Validating lr=5e-05, train epoch 1.:  75%|███████▌  | 242/321 [16:12<05:16,  4.01s/it]Validating lr=5e-05, train epoch 1.:  76%|███████▌  | 243/321 [16:16<05:12,  4.01s/it]Validating lr=5e-05, train epoch 1.:  76%|███████▌  | 244/321 [16:20<05:10,  4.03s/it]Validating lr=5e-05, train epoch 1.:  76%|███████▋  | 245/321 [16:24<05:06,  4.03s/it]Validating lr=5e-05, train epoch 1.:  77%|███████▋  | 246/321 [16:28<05:03,  4.04s/it]Validating lr=5e-05, train epoch 1.:  77%|███████▋  | 247/321 [16:32<04:58,  4.03s/it]Validating lr=5e-05, train epoch 1.:  77%|███████▋  | 248/321 [16:36<04:53,  4.02s/it]Validating lr=5e-05, train epoch 1.:  78%|███████▊  | 249/321 [16:40<04:48,  4.00s/it]Validating lr=5e-05, train epoch 1.:  78%|███████▊  | 250/321 [16:44<04:43,  4.00s/it]Validating lr=5e-05, train epoch 1.:  78%|███████▊  | 251/321 [16:48<04:41,  4.02s/it]Validating lr=5e-05, train epoch 1.:  79%|███████▊  | 252/321 [16:52<04:37,  4.03s/it]Validating lr=5e-05, train epoch 1.:  79%|███████▉  | 253/321 [16:56<04:33,  4.03s/it]Validating lr=5e-05, train epoch 1.:  79%|███████▉  | 254/321 [17:00<04:31,  4.05s/it]Validating lr=5e-05, train epoch 1.:  79%|███████▉  | 255/321 [17:04<04:28,  4.06s/it]Validating lr=5e-05, train epoch 1.:  80%|███████▉  | 256/321 [17:08<04:22,  4.04s/it]Validating lr=5e-05, train epoch 1.:  80%|████████  | 257/321 [17:13<04:19,  4.05s/it]Validating lr=5e-05, train epoch 1.:  80%|████████  | 258/321 [17:17<04:15,  4.06s/it]Validating lr=5e-05, train epoch 1.:  81%|████████  | 259/321 [17:21<04:12,  4.07s/it]Validating lr=5e-05, train epoch 1.:  81%|████████  | 260/321 [17:25<04:07,  4.05s/it]Validating lr=5e-05, train epoch 1.:  81%|████████▏ | 261/321 [17:29<04:03,  4.05s/it]Validating lr=5e-05, train epoch 1.:  82%|████████▏ | 262/321 [17:33<03:59,  4.06s/it]Validating lr=5e-05, train epoch 1.:  82%|████████▏ | 263/321 [17:37<03:55,  4.05s/it]Validating lr=5e-05, train epoch 1.:  82%|████████▏ | 264/321 [17:41<03:50,  4.05s/it]Validating lr=5e-05, train epoch 1.:  83%|████████▎ | 265/321 [17:45<03:46,  4.04s/it]Validating lr=5e-05, train epoch 1.:  83%|████████▎ | 266/321 [17:49<03:41,  4.02s/it]Validating lr=5e-05, train epoch 1.:  83%|████████▎ | 267/321 [17:53<03:36,  4.01s/it]Validating lr=5e-05, train epoch 1.:  83%|████████▎ | 268/321 [17:57<03:32,  4.02s/it]Validating lr=5e-05, train epoch 1.:  84%|████████▍ | 269/321 [18:01<03:28,  4.01s/it]Validating lr=5e-05, train epoch 1.:  84%|████████▍ | 270/321 [18:05<03:23,  3.99s/it]Validating lr=5e-05, train epoch 1.:  84%|████████▍ | 271/321 [18:09<03:19,  3.99s/it]Validating lr=5e-05, train epoch 1.:  85%|████████▍ | 272/321 [18:13<03:16,  4.01s/it]Validating lr=5e-05, train epoch 1.:  85%|████████▌ | 273/321 [18:17<03:13,  4.04s/it]Validating lr=5e-05, train epoch 1.:  85%|████████▌ | 274/321 [18:21<03:09,  4.03s/it]Validating lr=5e-05, train epoch 1.:  86%|████████▌ | 275/321 [18:25<03:05,  4.04s/it]Validating lr=5e-05, train epoch 1.:  86%|████████▌ | 276/321 [18:29<03:01,  4.03s/it]Validating lr=5e-05, train epoch 1.:  86%|████████▋ | 277/321 [18:33<02:57,  4.03s/it]Validating lr=5e-05, train epoch 1.:  87%|████████▋ | 278/321 [18:37<02:52,  4.02s/it]Validating lr=5e-05, train epoch 1.:  87%|████████▋ | 279/321 [18:41<02:48,  4.00s/it]Validating lr=5e-05, train epoch 1.:  87%|████████▋ | 280/321 [18:45<02:44,  4.01s/it]Validating lr=5e-05, train epoch 1.:  88%|████████▊ | 281/321 [18:49<02:41,  4.03s/it]Validating lr=5e-05, train epoch 1.:  88%|████████▊ | 282/321 [18:53<02:37,  4.04s/it]Validating lr=5e-05, train epoch 1.:  88%|████████▊ | 283/321 [18:57<02:32,  4.02s/it]Validating lr=5e-05, train epoch 1.:  88%|████████▊ | 284/321 [19:01<02:29,  4.03s/it]Validating lr=5e-05, train epoch 1.:  89%|████████▉ | 285/321 [19:05<02:25,  4.03s/it]Validating lr=5e-05, train epoch 1.:  89%|████████▉ | 286/321 [19:09<02:20,  4.02s/it]Validating lr=5e-05, train epoch 1.:  89%|████████▉ | 287/321 [19:13<02:16,  4.01s/it]Validating lr=5e-05, train epoch 1.:  90%|████████▉ | 288/321 [19:17<02:13,  4.03s/it]Validating lr=5e-05, train epoch 1.:  90%|█████████ | 289/321 [19:21<02:08,  4.03s/it]Validating lr=5e-05, train epoch 1.:  90%|█████████ | 290/321 [19:25<02:04,  4.02s/it]Validating lr=5e-05, train epoch 1.:  91%|█████████ | 291/321 [19:29<02:00,  4.02s/it]Validating lr=5e-05, train epoch 1.:  91%|█████████ | 292/321 [19:33<01:56,  4.01s/it]Validating lr=5e-05, train epoch 1.:  91%|█████████▏| 293/321 [19:37<01:51,  4.00s/it]Validating lr=5e-05, train epoch 1.:  92%|█████████▏| 294/321 [19:41<01:47,  4.00s/it]Validating lr=5e-05, train epoch 1.:  92%|█████████▏| 295/321 [19:45<01:44,  4.01s/it]Validating lr=5e-05, train epoch 1.:  92%|█████████▏| 296/321 [19:49<01:39,  3.99s/it]Validating lr=5e-05, train epoch 1.:  93%|█████████▎| 297/321 [19:53<01:36,  4.00s/it]Validating lr=5e-05, train epoch 1.:  93%|█████████▎| 298/321 [19:57<01:31,  3.99s/it]Validating lr=5e-05, train epoch 1.:  93%|█████████▎| 299/321 [20:01<01:27,  3.99s/it]Validating lr=5e-05, train epoch 1.:  93%|█████████▎| 300/321 [20:05<01:24,  4.01s/it]Validating lr=5e-05, train epoch 1.:  94%|█████████▍| 301/321 [20:09<01:20,  4.01s/it]Validating lr=5e-05, train epoch 1.:  94%|█████████▍| 302/321 [20:14<01:16,  4.04s/it]Validating lr=5e-05, train epoch 1.:  94%|█████████▍| 303/321 [20:18<01:12,  4.04s/it]Validating lr=5e-05, train epoch 1.:  95%|█████████▍| 304/321 [20:22<01:08,  4.03s/it]Validating lr=5e-05, train epoch 1.:  95%|█████████▌| 305/321 [20:26<01:04,  4.02s/it]Validating lr=5e-05, train epoch 1.:  95%|█████████▌| 306/321 [20:30<01:00,  4.02s/it]Validating lr=5e-05, train epoch 1.:  96%|█████████▌| 307/321 [20:34<00:56,  4.02s/it]Validating lr=5e-05, train epoch 1.:  96%|█████████▌| 308/321 [20:38<00:52,  4.00s/it]Validating lr=5e-05, train epoch 1.:  96%|█████████▋| 309/321 [20:42<00:48,  4.00s/it]Validating lr=5e-05, train epoch 1.:  97%|█████████▋| 310/321 [20:46<00:43,  4.00s/it]Validating lr=5e-05, train epoch 1.:  97%|█████████▋| 311/321 [20:50<00:39,  4.00s/it]Validating lr=5e-05, train epoch 1.:  97%|█████████▋| 312/321 [20:54<00:35,  4.00s/it]Validating lr=5e-05, train epoch 1.:  98%|█████████▊| 313/321 [20:58<00:31,  4.00s/it]Validating lr=5e-05, train epoch 1.:  98%|█████████▊| 314/321 [21:02<00:27,  3.99s/it]Validating lr=5e-05, train epoch 1.:  98%|█████████▊| 315/321 [21:06<00:24,  4.01s/it]Validating lr=5e-05, train epoch 1.:  98%|█████████▊| 316/321 [21:10<00:19,  4.00s/it]Validating lr=5e-05, train epoch 1.:  99%|█████████▉| 317/321 [21:14<00:16,  4.00s/it]Validating lr=5e-05, train epoch 1.:  99%|█████████▉| 318/321 [21:18<00:12,  4.01s/it]Validating lr=5e-05, train epoch 1.:  99%|█████████▉| 319/321 [21:22<00:08,  4.02s/it]Validating lr=5e-05, train epoch 1.: 100%|█████████▉| 320/321 [21:26<00:04,  4.04s/it]Validating lr=5e-05, train epoch 1.: 100%|██████████| 321/321 [21:30<00:00,  4.03s/it]Validating lr=5e-05, train epoch 1.: 100%|██████████| 321/321 [21:30<00:00,  4.02s/it]
Evaluating for lr=5e-05:   0%|          | 0/33 [00:00<?, ?it/s]Evaluating for lr=5e-05:   3%|▎         | 1/33 [00:01<00:55,  1.72s/it]Evaluating for lr=5e-05:   6%|▌         | 2/33 [00:03<00:53,  1.72s/it]Evaluating for lr=5e-05:   9%|▉         | 3/33 [00:05<00:51,  1.72s/it]Evaluating for lr=5e-05:  12%|█▏        | 4/33 [00:06<00:49,  1.72s/it]Evaluating for lr=5e-05:  15%|█▌        | 5/33 [00:08<00:48,  1.74s/it]Evaluating for lr=5e-05:  18%|█▊        | 6/33 [00:10<00:46,  1.73s/it]Evaluating for lr=5e-05:  21%|██        | 7/33 [00:12<00:45,  1.76s/it]Evaluating for lr=5e-05:  24%|██▍       | 8/33 [00:13<00:43,  1.75s/it]Evaluating for lr=5e-05:  27%|██▋       | 9/33 [00:15<00:41,  1.75s/it]Evaluating for lr=5e-05:  30%|███       | 10/33 [00:17<00:39,  1.73s/it]Evaluating for lr=5e-05:  33%|███▎      | 11/33 [00:19<00:38,  1.73s/it]Evaluating for lr=5e-05:  36%|███▋      | 12/33 [00:20<00:36,  1.72s/it]Evaluating for lr=5e-05:  39%|███▉      | 13/33 [00:22<00:34,  1.72s/it]Evaluating for lr=5e-05:  42%|████▏     | 14/33 [00:24<00:32,  1.72s/it]Evaluating for lr=5e-05:  45%|████▌     | 15/33 [00:25<00:30,  1.72s/it]Evaluating for lr=5e-05:  48%|████▊     | 16/33 [00:27<00:29,  1.73s/it]Evaluating for lr=5e-05:  52%|█████▏    | 17/33 [00:29<00:27,  1.73s/it]Evaluating for lr=5e-05:  55%|█████▍    | 18/33 [00:31<00:25,  1.73s/it]Evaluating for lr=5e-05:  58%|█████▊    | 19/33 [00:32<00:24,  1.74s/it]Evaluating for lr=5e-05:  61%|██████    | 20/33 [00:34<00:22,  1.74s/it]Evaluating for lr=5e-05:  64%|██████▎   | 21/33 [00:36<00:20,  1.72s/it]Evaluating for lr=5e-05:  67%|██████▋   | 22/33 [00:38<00:19,  1.73s/it]Evaluating for lr=5e-05:  70%|██████▉   | 23/33 [00:39<00:17,  1.73s/it]Evaluating for lr=5e-05:  73%|███████▎  | 24/33 [00:41<00:15,  1.72s/it]Evaluating for lr=5e-05:  76%|███████▌  | 25/33 [00:43<00:13,  1.73s/it]Evaluating for lr=5e-05:  79%|███████▉  | 26/33 [00:44<00:12,  1.72s/it]Evaluating for lr=5e-05:  82%|████████▏ | 27/33 [00:46<00:10,  1.72s/it]Evaluating for lr=5e-05:  85%|████████▍ | 28/33 [00:48<00:08,  1.73s/it]Evaluating for lr=5e-05:  88%|████████▊ | 29/33 [00:50<00:06,  1.73s/it]Evaluating for lr=5e-05:  91%|█████████ | 30/33 [00:51<00:05,  1.73s/it]Evaluating for lr=5e-05:  94%|█████████▍| 31/33 [00:53<00:03,  1.74s/it]Evaluating for lr=5e-05:  97%|█████████▋| 32/33 [00:55<00:01,  1.73s/it]Evaluating for lr=5e-05: 100%|██████████| 33/33 [00:57<00:00,  1.72s/it]Evaluating for lr=5e-05: 100%|██████████| 33/33 [00:57<00:00,  1.73s/it]
Hyperparameter tuning process completed, using lr 5e-05!
[2025-06-20 19:19:25,239] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2+5f631abc, git-hash=5f631abc, git-branch=HEAD
[2025-06-20 19:19:27,899] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-06-20 19:19:27,902] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-06-20 19:19:27,902] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-06-20 19:19:27,916] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-06-20 19:19:27,916] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-06-20 19:19:27,916] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-06-20 19:19:27,916] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-06-20 19:19:27,916] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2025-06-20 19:19:27,917] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2025-06-20 19:19:27,917] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-06-20 19:19:27,917] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-06-20 19:19:27,917] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2025-06-20 19:19:27,917] [INFO] [config.py:1000:print]   amp_params ................... False
[2025-06-20 19:19:27,917] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-06-20 19:19:27,917] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2025-06-20 19:19:27,917] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2025-06-20 19:19:27,917] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2025-06-20 19:19:27,917] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2025-06-20 19:19:27,917] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2025-06-20 19:19:27,917] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x14a6bb8db010>
[2025-06-20 19:19:27,917] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2025-06-20 19:19:27,917] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   dump_state ................... False
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   global_rank .................. 0
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-06-20 19:19:27,918] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 5e-05, 'weight_decay': 0.01}
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   pld_params ................... False
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   steps_per_print .............. 100000
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   train_batch_size ............. 1024
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   world_size ................... 8
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2025-06-20 19:19:27,919] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2025-06-20 19:19:27,919] [INFO] [config.py:986:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 128, 
    "train_batch_size": 1.024000e+03, 
    "steps_per_print": 1.000000e+05, 
    "gradient_accumulation_steps": 1, 
    "fp16": {
        "enabled": false
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 5e-05, 
            "weight_decay": 0.01
        }
    }, 
    "comms_logger": {
        "enabled": true, 
        "verbose": false
    }, 
    "zero_optimization": {
        "stage": 0
    }
}
Main training loop, train epoch 0.:   0%|          | 0/321 [00:00<?, ?it/s]Main training loop, train epoch 0.:   0%|          | 1/321 [00:04<21:32,  4.04s/it]Main training loop, train epoch 0.:   1%|          | 2/321 [00:08<21:31,  4.05s/it]Main training loop, train epoch 0.:   1%|          | 3/321 [00:12<21:31,  4.06s/it]Main training loop, train epoch 0.:   1%|          | 4/321 [00:16<21:20,  4.04s/it]Main training loop, train epoch 0.:   2%|▏         | 5/321 [00:20<21:15,  4.04s/it]Main training loop, train epoch 0.:   2%|▏         | 6/321 [00:24<21:10,  4.03s/it]Main training loop, train epoch 0.:   2%|▏         | 7/321 [00:28<21:04,  4.03s/it]Main training loop, train epoch 0.:   2%|▏         | 8/321 [00:32<21:00,  4.03s/it]Main training loop, train epoch 0.:   3%|▎         | 9/321 [00:36<20:54,  4.02s/it]Main training loop, train epoch 0.:   3%|▎         | 10/321 [00:40<20:49,  4.02s/it]Main training loop, train epoch 0.:   3%|▎         | 11/321 [00:44<20:44,  4.01s/it]Main training loop, train epoch 0.:   4%|▎         | 12/321 [00:48<20:46,  4.03s/it]Main training loop, train epoch 0.:   4%|▍         | 13/321 [00:52<20:54,  4.07s/it]Main training loop, train epoch 0.:   4%|▍         | 14/321 [00:56<20:47,  4.06s/it]Main training loop, train epoch 0.:   5%|▍         | 15/321 [01:00<20:44,  4.07s/it]Main training loop, train epoch 0.:   5%|▍         | 16/321 [01:04<20:35,  4.05s/it]Main training loop, train epoch 0.:   5%|▌         | 17/321 [01:08<20:27,  4.04s/it]Main training loop, train epoch 0.:   6%|▌         | 18/321 [01:12<20:21,  4.03s/it]Main training loop, train epoch 0.:   6%|▌         | 19/321 [01:16<20:23,  4.05s/it]Main training loop, train epoch 0.:   6%|▌         | 20/321 [01:20<20:15,  4.04s/it]Main training loop, train epoch 0.:   7%|▋         | 21/321 [01:24<20:10,  4.03s/it]Main training loop, train epoch 0.:   7%|▋         | 22/321 [01:28<20:08,  4.04s/it]Main training loop, train epoch 0.:   7%|▋         | 23/321 [01:32<20:06,  4.05s/it]Main training loop, train epoch 0.:   7%|▋         | 24/321 [01:36<20:00,  4.04s/it]Main training loop, train epoch 0.:   8%|▊         | 25/321 [01:41<19:58,  4.05s/it]Main training loop, train epoch 0.:   8%|▊         | 26/321 [01:45<19:50,  4.04s/it]Main training loop, train epoch 0.:   8%|▊         | 27/321 [01:49<19:53,  4.06s/it]Main training loop, train epoch 0.:   9%|▊         | 28/321 [01:53<19:45,  4.05s/it]Main training loop, train epoch 0.:   9%|▉         | 29/321 [01:57<19:45,  4.06s/it]Main training loop, train epoch 0.:   9%|▉         | 30/321 [02:01<19:37,  4.04s/it]Main training loop, train epoch 0.:  10%|▉         | 31/321 [02:05<19:32,  4.04s/it]Main training loop, train epoch 0.:  10%|▉         | 32/321 [02:09<19:30,  4.05s/it]Main training loop, train epoch 0.:  10%|█         | 33/321 [02:13<19:23,  4.04s/it]Main training loop, train epoch 0.:  11%|█         | 34/321 [02:17<19:18,  4.04s/it]Main training loop, train epoch 0.:  11%|█         | 35/321 [02:21<19:07,  4.01s/it]Main training loop, train epoch 0.:  11%|█         | 36/321 [02:25<19:03,  4.01s/it]Main training loop, train epoch 0.:  12%|█▏        | 37/321 [02:29<19:00,  4.01s/it]Main training loop, train epoch 0.:  12%|█▏        | 38/321 [02:33<19:03,  4.04s/it]Main training loop, train epoch 0.:  12%|█▏        | 39/321 [02:37<18:53,  4.02s/it]Main training loop, train epoch 0.:  12%|█▏        | 40/321 [02:41<18:45,  4.01s/it]Main training loop, train epoch 0.:  13%|█▎        | 41/321 [02:45<18:39,  4.00s/it]Main training loop, train epoch 0.:  13%|█▎        | 42/321 [02:49<18:38,  4.01s/it]Main training loop, train epoch 0.:  13%|█▎        | 43/321 [02:53<18:41,  4.03s/it]Main training loop, train epoch 0.:  14%|█▎        | 44/321 [02:57<18:34,  4.02s/it]Main training loop, train epoch 0.:  14%|█▍        | 45/321 [03:01<18:26,  4.01s/it]Main training loop, train epoch 0.:  14%|█▍        | 46/321 [03:05<18:20,  4.00s/it]Main training loop, train epoch 0.:  15%|█▍        | 47/321 [03:09<18:17,  4.01s/it]Main training loop, train epoch 0.:  15%|█▍        | 48/321 [03:13<18:13,  4.00s/it]Main training loop, train epoch 0.:  15%|█▌        | 49/321 [03:17<18:10,  4.01s/it]Main training loop, train epoch 0.:  16%|█▌        | 50/321 [03:21<18:08,  4.02s/it]Main training loop, train epoch 0.:  16%|█▌        | 51/321 [03:25<18:05,  4.02s/it]Main training loop, train epoch 0.:  16%|█▌        | 52/321 [03:29<18:01,  4.02s/it]Main training loop, train epoch 0.:  17%|█▋        | 53/321 [03:33<18:00,  4.03s/it]Main training loop, train epoch 0.:  17%|█▋        | 54/321 [03:37<18:01,  4.05s/it]Main training loop, train epoch 0.:  17%|█▋        | 55/321 [03:41<17:58,  4.05s/it]Main training loop, train epoch 0.:  17%|█▋        | 56/321 [03:45<17:48,  4.03s/it]Main training loop, train epoch 0.:  18%|█▊        | 57/321 [03:49<17:45,  4.04s/it]Main training loop, train epoch 0.:  18%|█▊        | 58/321 [03:53<17:38,  4.02s/it]Main training loop, train epoch 0.:  18%|█▊        | 59/321 [03:57<17:33,  4.02s/it]Main training loop, train epoch 0.:  19%|█▊        | 60/321 [04:01<17:27,  4.01s/it]Main training loop, train epoch 0.:  19%|█▉        | 61/321 [04:05<17:21,  4.00s/it]Main training loop, train epoch 0.:  19%|█▉        | 62/321 [04:09<17:23,  4.03s/it]Main training loop, train epoch 0.:  20%|█▉        | 63/321 [04:13<17:16,  4.02s/it]Main training loop, train epoch 0.:  20%|█▉        | 64/321 [04:17<17:11,  4.02s/it]Main training loop, train epoch 0.:  20%|██        | 65/321 [04:21<17:07,  4.01s/it]Main training loop, train epoch 0.:  21%|██        | 66/321 [04:25<17:03,  4.01s/it]Main training loop, train epoch 0.:  21%|██        | 67/321 [04:30<17:00,  4.02s/it]Main training loop, train epoch 0.:  21%|██        | 68/321 [04:34<16:58,  4.03s/it]Main training loop, train epoch 0.:  21%|██▏       | 69/321 [04:38<16:53,  4.02s/it]Main training loop, train epoch 0.:  22%|██▏       | 70/321 [04:42<16:49,  4.02s/it]Main training loop, train epoch 0.:  22%|██▏       | 71/321 [04:46<16:46,  4.03s/it]Main training loop, train epoch 0.:  22%|██▏       | 72/321 [04:50<16:41,  4.02s/it]Main training loop, train epoch 0.:  23%|██▎       | 73/321 [04:54<16:39,  4.03s/it]Main training loop, train epoch 0.:  23%|██▎       | 74/321 [04:58<16:36,  4.04s/it]Main training loop, train epoch 0.:  23%|██▎       | 75/321 [05:02<16:30,  4.03s/it]Main training loop, train epoch 0.:  24%|██▎       | 76/321 [05:06<16:29,  4.04s/it]Main training loop, train epoch 0.:  24%|██▍       | 77/321 [05:10<16:21,  4.02s/it]Main training loop, train epoch 0.:  24%|██▍       | 78/321 [05:14<16:16,  4.02s/it]Main training loop, train epoch 0.:  25%|██▍       | 79/321 [05:18<16:08,  4.00s/it]Main training loop, train epoch 0.:  25%|██▍       | 80/321 [05:22<16:09,  4.02s/it]Main training loop, train epoch 0.:  25%|██▌       | 81/321 [05:26<16:02,  4.01s/it]Main training loop, train epoch 0.:  26%|██▌       | 82/321 [05:30<15:56,  4.00s/it]Main training loop, train epoch 0.:  26%|██▌       | 83/321 [05:34<16:00,  4.04s/it]Main training loop, train epoch 0.:  26%|██▌       | 84/321 [05:38<15:54,  4.03s/it]Main training loop, train epoch 0.:  26%|██▋       | 85/321 [05:42<15:49,  4.02s/it]Main training loop, train epoch 0.:  27%|██▋       | 86/321 [05:46<15:45,  4.02s/it]Main training loop, train epoch 0.:  27%|██▋       | 87/321 [05:50<15:39,  4.02s/it]Main training loop, train epoch 0.:  27%|██▋       | 88/321 [05:54<15:43,  4.05s/it]Main training loop, train epoch 0.:  28%|██▊       | 89/321 [05:58<15:39,  4.05s/it]Main training loop, train epoch 0.:  28%|██▊       | 90/321 [06:02<15:31,  4.03s/it]Main training loop, train epoch 0.:  28%|██▊       | 91/321 [06:06<15:26,  4.03s/it]Main training loop, train epoch 0.:  29%|██▊       | 92/321 [06:10<15:20,  4.02s/it]Main training loop, train epoch 0.:  29%|██▉       | 93/321 [06:14<15:21,  4.04s/it]Main training loop, train epoch 0.:  29%|██▉       | 94/321 [06:18<15:12,  4.02s/it]Main training loop, train epoch 0.:  30%|██▉       | 95/321 [06:22<15:06,  4.01s/it]Main training loop, train epoch 0.:  30%|██▉       | 96/321 [06:26<15:05,  4.03s/it]Main training loop, train epoch 0.:  30%|███       | 97/321 [06:30<15:01,  4.02s/it]Main training loop, train epoch 0.:  31%|███       | 98/321 [06:34<14:59,  4.03s/it]Main training loop, train epoch 0.:  31%|███       | 99/321 [06:38<14:57,  4.04s/it]Main training loop, train epoch 0.:  31%|███       | 100/321 [06:42<14:53,  4.04s/it]Main training loop, train epoch 0.:  31%|███▏      | 101/321 [06:46<14:48,  4.04s/it]Main training loop, train epoch 0.:  32%|███▏      | 102/321 [06:51<14:44,  4.04s/it]Main training loop, train epoch 0.:  32%|███▏      | 103/321 [06:55<14:44,  4.06s/it]Main training loop, train epoch 0.:  32%|███▏      | 104/321 [06:59<14:37,  4.04s/it]Main training loop, train epoch 0.:  33%|███▎      | 105/321 [07:03<14:33,  4.04s/it]Main training loop, train epoch 0.:  33%|███▎      | 106/321 [07:07<14:28,  4.04s/it]Main training loop, train epoch 0.:  33%|███▎      | 107/321 [07:11<14:21,  4.03s/it]Main training loop, train epoch 0.:  34%|███▎      | 108/321 [07:15<14:18,  4.03s/it]Main training loop, train epoch 0.:  34%|███▍      | 109/321 [07:19<14:14,  4.03s/it]Main training loop, train epoch 0.:  34%|███▍      | 110/321 [07:23<14:09,  4.02s/it]Main training loop, train epoch 0.:  35%|███▍      | 111/321 [07:27<14:04,  4.02s/it]Main training loop, train epoch 0.:  35%|███▍      | 112/321 [07:31<14:01,  4.02s/it]Main training loop, train epoch 0.:  35%|███▌      | 113/321 [07:35<13:56,  4.02s/it]Main training loop, train epoch 0.:  36%|███▌      | 114/321 [07:39<13:51,  4.02s/it]Main training loop, train epoch 0.:  36%|███▌      | 115/321 [07:43<13:46,  4.01s/it]Main training loop, train epoch 0.:  36%|███▌      | 116/321 [07:47<13:43,  4.02s/it]Main training loop, train epoch 0.:  36%|███▋      | 117/321 [07:51<13:37,  4.01s/it]Main training loop, train epoch 0.:  37%|███▋      | 118/321 [07:55<13:34,  4.01s/it]Main training loop, train epoch 0.:  37%|███▋      | 119/321 [07:59<13:29,  4.01s/it]Main training loop, train epoch 0.:  37%|███▋      | 120/321 [08:03<13:23,  4.00s/it]Main training loop, train epoch 0.:  38%|███▊      | 121/321 [08:07<13:23,  4.02s/it]Main training loop, train epoch 0.:  38%|███▊      | 122/321 [08:11<13:21,  4.03s/it]Main training loop, train epoch 0.:  38%|███▊      | 123/321 [08:15<13:20,  4.04s/it]Main training loop, train epoch 0.:  39%|███▊      | 124/321 [08:19<13:21,  4.07s/it]Main training loop, train epoch 0.:  39%|███▉      | 125/321 [08:23<13:15,  4.06s/it]Main training loop, train epoch 0.:  39%|███▉      | 126/321 [08:27<13:08,  4.04s/it]Main training loop, train epoch 0.:  40%|███▉      | 127/321 [08:31<13:03,  4.04s/it]Main training loop, train epoch 0.:  40%|███▉      | 128/321 [08:35<13:00,  4.04s/it]Main training loop, train epoch 0.:  40%|████      | 129/321 [08:39<12:58,  4.05s/it]Main training loop, train epoch 0.:  40%|████      | 130/321 [08:43<12:54,  4.05s/it]Main training loop, train epoch 0.:  41%|████      | 131/321 [08:47<12:48,  4.04s/it]Main training loop, train epoch 0.:  41%|████      | 132/321 [08:51<12:42,  4.03s/it]Main training loop, train epoch 0.:  41%|████▏     | 133/321 [08:55<12:34,  4.02s/it]Main training loop, train epoch 0.:  42%|████▏     | 134/321 [08:59<12:30,  4.01s/it]Main training loop, train epoch 0.:  42%|████▏     | 135/321 [09:03<12:28,  4.03s/it]Main training loop, train epoch 0.:  42%|████▏     | 136/321 [09:08<12:25,  4.03s/it]Main training loop, train epoch 0.:  43%|████▎     | 137/321 [09:12<12:22,  4.03s/it]Main training loop, train epoch 0.:  43%|████▎     | 138/321 [09:16<12:22,  4.06s/it]Main training loop, train epoch 0.:  43%|████▎     | 139/321 [09:20<12:19,  4.07s/it]Main training loop, train epoch 0.:  44%|████▎     | 140/321 [09:24<12:14,  4.06s/it]Main training loop, train epoch 0.:  44%|████▍     | 141/321 [09:28<12:09,  4.05s/it]Main training loop, train epoch 0.:  44%|████▍     | 142/321 [09:32<12:01,  4.03s/it]Main training loop, train epoch 0.:  45%|████▍     | 143/321 [09:36<11:56,  4.03s/it]Main training loop, train epoch 0.:  45%|████▍     | 144/321 [09:40<11:54,  4.04s/it]Main training loop, train epoch 0.:  45%|████▌     | 145/321 [09:44<11:49,  4.03s/it]Main training loop, train epoch 0.:  45%|████▌     | 146/321 [09:48<11:44,  4.03s/it]Main training loop, train epoch 0.:  46%|████▌     | 147/321 [09:52<11:39,  4.02s/it]Main training loop, train epoch 0.:  46%|████▌     | 148/321 [09:56<11:41,  4.05s/it]Main training loop, train epoch 0.:  46%|████▋     | 149/321 [10:00<11:36,  4.05s/it]Main training loop, train epoch 0.:  47%|████▋     | 150/321 [10:04<11:30,  4.04s/it]Main training loop, train epoch 0.:  47%|████▋     | 151/321 [10:08<11:24,  4.02s/it]Main training loop, train epoch 0.:  47%|████▋     | 152/321 [10:12<11:18,  4.01s/it]Main training loop, train epoch 0.:  48%|████▊     | 153/321 [10:16<11:15,  4.02s/it]Main training loop, train epoch 0.:  48%|████▊     | 154/321 [10:20<11:11,  4.02s/it]Main training loop, train epoch 0.:  48%|████▊     | 155/321 [10:24<11:06,  4.01s/it]Main training loop, train epoch 0.:  49%|████▊     | 156/321 [10:28<11:03,  4.02s/it]Main training loop, train epoch 0.:  49%|████▉     | 157/321 [10:32<11:01,  4.03s/it]Main training loop, train epoch 0.:  49%|████▉     | 158/321 [10:36<10:58,  4.04s/it]Main training loop, train epoch 0.:  50%|████▉     | 159/321 [10:40<10:54,  4.04s/it]Main training loop, train epoch 0.:  50%|████▉     | 160/321 [10:44<10:51,  4.04s/it]Main training loop, train epoch 0.:  50%|█████     | 161/321 [10:48<10:45,  4.03s/it]Main training loop, train epoch 0.:  50%|█████     | 162/321 [10:52<10:42,  4.04s/it]Main training loop, train epoch 0.:  51%|█████     | 163/321 [10:56<10:36,  4.03s/it]Main training loop, train epoch 0.:  51%|█████     | 164/321 [11:00<10:31,  4.03s/it]Main training loop, train epoch 0.:  51%|█████▏    | 165/321 [11:05<10:28,  4.03s/it]Main training loop, train epoch 0.:  52%|█████▏    | 166/321 [11:09<10:24,  4.03s/it]Main training loop, train epoch 0.:  52%|█████▏    | 167/321 [11:13<10:17,  4.01s/it]Main training loop, train epoch 0.:  52%|█████▏    | 168/321 [11:17<10:12,  4.01s/it]Main training loop, train epoch 0.:  53%|█████▎    | 169/321 [11:21<10:08,  4.01s/it]Main training loop, train epoch 0.:  53%|█████▎    | 170/321 [11:25<10:04,  4.00s/it]Main training loop, train epoch 0.:  53%|█████▎    | 171/321 [11:29<10:01,  4.01s/it]Main training loop, train epoch 0.:  54%|█████▎    | 172/321 [11:33<09:56,  4.00s/it]Main training loop, train epoch 0.:  54%|█████▍    | 173/321 [11:37<09:51,  4.00s/it]Main training loop, train epoch 0.:  54%|█████▍    | 174/321 [11:41<09:47,  4.00s/it]Main training loop, train epoch 0.:  55%|█████▍    | 175/321 [11:45<09:45,  4.01s/it]Main training loop, train epoch 0.:  55%|█████▍    | 176/321 [11:49<09:41,  4.01s/it]=>> PBS: job killed: walltime 3628 exceeded limit 3600
read_signalfd: got signal 15 (Terminated)
Forwarding signal 15
x3006c0s31b0n0.hsn.cm.polaris.alcf.anl.gov: rank 4 died from signal 15
x3006c0s19b0n0.hsn.cm.polaris.alcf.anl.gov: rank 0 died from signal 15
x3006c0s31b0n0.hsn.cm.polaris.alcf.anl.gov: rank 6 died from signal 15
x3006c0s31b0n0.hsn.cm.polaris.alcf.anl.gov: rank 5 died from signal 15
x3006c0s19b0n0.hsn.cm.polaris.alcf.anl.gov: rank 2 died from signal 15
x3006c0s19b0n0.hsn.cm.polaris.alcf.anl.gov: rank 1 died from signal 15
x3006c0s31b0n0.hsn.cm.polaris.alcf.anl.gov: rank 7 died from signal 15
x3006c0s19b0n0.hsn.cm.polaris.alcf.anl.gov: rank 3 died from signal 15
Application 37e2424d resources: utime=21277s stime=6041s maxrss=35865852KB inblock=116825540 oublock=24 minflt=353352867 majflt=20573 nvcsw=42557575 nivcsw=13142222
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled

Lmod is automatically replacing "nvhpc/23.9" with "gcc-native/12.3".


Lmod is automatically replacing "PrgEnv-nvhpc/8.5.0" with "PrgEnv-gnu/8.5.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.28

declare -x APP2_STATE="23.12.0"
declare -x BASH_ENV="/usr/share/lmod/lmod/init/bash"
declare -x C3_RSH="ssh -oConnectTimeout=10 -oForwardX11=no"
declare -x CFLAGS="-I/soft/applications/conda/2024-04-29/mconda3/include"
declare -x COLORTERM="1"
declare -x COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin"
declare -x CONDA_DEFAULT_ENV="base"
declare -x CONDA_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/conda"
declare -x CONDA_PREFIX="/soft/applications/conda/2024-04-29/mconda3"
declare -x CONDA_PROMPT_MODIFIER="(2024-04-29/base) "
declare -x CONDA_PYTHON_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/python"
declare -x CONDA_SHLVL="1"
declare -x CPU="x86_64"
declare -x CRAYPAT_LD_LIBRARY_PATH="/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAYPAT_OPTS_EXECUTABLE="libexec64/opts"
declare -x CRAYPAT_ROOT="/opt/cray/pe/perftools/23.12.0"
declare -x CRAYPE_DIR="/opt/cray/pe/craype/2.7.30"
declare -x CRAYPE_NETWORK_TARGET="ofi"
declare -x CRAYPE_VERSION="2.7.30"
declare -x CRAY_CPU_TARGET="x86-milan"
declare -x CRAY_DSMML_BASEDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_DIR="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_PREFIX="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_ROOTDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_VER="0.2.2"
declare -x CRAY_DSMML_VERSION="0.2.2"
declare -x CRAY_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x CRAY_HDF5_PARALLEL_PREFIX="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x CRAY_HDF5_PARALLEL_VERSION="1.12.2.9"
declare -x CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:/opt/cray/pe/pmi/6.1.13/lib:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:/opt/cray/pe/mpich/8.1.28/gtl/lib:/opt/cray/pe/dsmml/0.2.2/dsmml/lib:/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAY_LMOD_COMPILER="gnu/12.0"
declare -x CRAY_LMOD_CPU="x86-milan/1.0"
declare -x CRAY_LMOD_MPI="cray-mpich/8.0"
declare -x CRAY_LMOD_NET="ofi/1.0"
declare -x CRAY_MPICH_BASEDIR="/opt/cray/pe/mpich/8.1.28/ofi"
declare -x CRAY_MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_PREFIX="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_ROOTDIR="/opt/cray/pe/mpich/8.1.28"
declare -x CRAY_MPICH_VER="8.1.28"
declare -x CRAY_MPICH_VERSION="8.1.28"
declare -x CRAY_PERFTOOLS_PREFIX="/opt/cray/pe/perftools/23.12.0"
declare -x CRAY_PERFTOOLS_VERSION="23.12.0"
declare -x CRAY_PMI_INCLUDE_OPTS="-I/opt/cray/pe/pmi/6.1.13/include"
declare -x CRAY_PMI_POST_LINK_OPTS="-L/opt/cray/pe/pmi/6.1.13/lib"
declare -x CRAY_PMI_PREFIX="/opt/cray/pe/pmi/6.1.13"
declare -x CRAY_PMI_VERSION="6.1.13"
declare -x CSHEDIT="emacs"
declare -x CUDA_HOME="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_TOOLKIT_BASE="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDNN_HOME="/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/"
declare -x ENVIRONMENT="BATCH"
declare -x ENV_NAME="conda/2024-04-29"
declare -x FROM_HEADER=""
declare -x GCC_PATH="/usr/bin"
declare -x GCC_PREFIX="/usr/lib64/gcc/x86_64-suse-linux/12"
declare -x GCC_VERSION="12.3"
declare -x GNU_VERSION="12.3"
declare -x GPG_TTY="not a tty"
declare -x GSETTINGS_SCHEMA_DIR="/soft/applications/conda/2024-04-29/mconda3/share/glib-2.0/schemas"
declare -x GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=""
declare -x G_BROKEN_FILENAMES="1"
declare -x G_FILENAME_ENCODING="@locale,UTF-8,ISO-8859-15,CP1252"
declare -x HDF5_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HDF5_ROOT="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HISTSIZE="1000"
declare -x HOME="/home/shourya01"
declare -x HOST="x3003c0s37b1n0"
declare -x HOSTNAME="x3003c0s37b1n0"
declare -x HOSTTYPE="x86_64"
declare -x HTTPS_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x HTTP_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x LANG="en_US.UTF-8"
declare -x LANGUAGE="en_US.UTF-8"
declare -x LDFLAGS="-L/soft/applications/conda/2024-04-29/mconda3/lib -Wl,--enable-new-dtags,-rpath,/soft/applications/conda/2024-04-29/mconda3/lib"
declare -x LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:/soft/perftools/darshan/darshan-3.4.4/lib:/opt/cray/pe/papi/7.0.1.2/lib64:/opt/cray/libfabric/1.15.2.0/lib64"
declare -x LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so"
declare -x LESS="-M -I -R"
declare -x LESSCLOSE="lessclose.sh %s %s"
declare -x LESSKEY="/etc/lesskey.bin"
declare -x LESSOPEN="lessopen.sh %s"
declare -x LESS_ADVANCED_PREPROCESSOR="no"
declare -x LMOD_CMD="/usr/share/lmod/lmod/libexec/lmod"
declare -x LMOD_DIR="/usr/share/lmod/lmod/libexec"
declare -x LMOD_FAMILY_COMPILER="gcc-native"
declare -x LMOD_FAMILY_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_CRAYPE="craype"
declare -x LMOD_FAMILY_CRAYPE_CPU="craype-x86-milan"
declare -x LMOD_FAMILY_CRAYPE_CPU_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_NETWORK="craype-network-ofi"
declare -x LMOD_FAMILY_CRAYPE_NETWORK_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_VERSION="2.7.30"
declare -x LMOD_FAMILY_GCC_COMPILER="gcc-native"
declare -x LMOD_FAMILY_GCC_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_HDF5="cray-hdf5-parallel"
declare -x LMOD_FAMILY_HDF5_VERSION="1.12.2.9"
declare -x LMOD_FAMILY_MPI="cray-mpich"
declare -x LMOD_FAMILY_MPI_VERSION="8.1.28"
declare -x LMOD_FAMILY_PRGENV="PrgEnv-gnu"
declare -x LMOD_FAMILY_PRGENV_VERSION="8.5.0"
declare -x LMOD_FAMILY_PYTHON="conda"
declare -x LMOD_FAMILY_PYTHON_VERSION="2024-04-29"
declare -x LMOD_PKG="/usr/share/lmod/lmod"
declare -x LMOD_ROOT="/usr/share/lmod"
declare -x LMOD_SETTARG_FULL_SUPPORT="no"
declare -x LMOD_SYSTEM_DEFAULT_MODULES="PrgEnv-nvhpc:craype-network-ofi:perftools-base:darshan:xalt"
declare -x LMOD_VERSION="8.7.34"
declare -x LMOD_sys="Linux"
declare -x LOADEDMODULES="libfabric/1.15.2.0:craype-network-ofi:perftools-base/23.12.0:darshan/3.4.4:xalt/3.0.2-202408282050:gcc-native/12.3:craype/2.7.30:cray-dsmml/0.2.2:cray-mpich/8.1.28:cray-pmi/6.1.13:cray-pals/1.3.4:cray-libpals/1.3.4:craype-x86-milan:PrgEnv-gnu/8.5.0:cray-hdf5-parallel/1.12.2.9:cudnn/9.1.0:conda/2024-04-29"
declare -x LOGNAME="shourya01"
declare -x MACHTYPE="x86_64-suse-linux"
declare -x MAIL="/var/spool/mail/shourya01"
declare -x MANPATH="/opt/cray/pals/1.3.4/man:/opt/cray/pe/pmi/6.1.13/man:/opt/cray/pe/mpich/8.1.28/ofi/man:/opt/cray/pe/mpich/8.1.28/man/mpich:/opt/cray/pe/dsmml/0.2.2/dsmml/man:/opt/cray/pe/craype/2.7.30/man:/opt/cray/pe/perftools/23.12.0/man:/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:/opt/cray/libfabric/1.15.2.0/share/man:/usr/share/lmod/lmod/share/man:/home/shourya01/.local/man:/usr/local/man:/usr/share/man:/usr/man:/opt/c3/man:/opt/pbs/share/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man"
declare -x MINICOM="-c on"
declare -x MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:/opt/cray/pe/lmod/modulefiles/mix_compilers:/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:/soft/modulefiles:/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:/usr/share/modulefiles/Linux:/usr/share/modulefiles/Core:/usr/share/lmod/lmod/modulefiles/Core:/usr/share/lmod/lmod/modulefiles:/opt/cray/pals/lmod/modulefiles/core:/opt/cray/modulefiles:/opt/cray/pe/lmod/modulefiles/core:/opt/cray/pe/lmod/modulefiles/craype-targets/default:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:/soft/xalt/modulefiles"
declare -x MODULEPATH_ROOT="/usr/share/modulefiles"
declare -x MODULESHOME="/usr/share/lmod/lmod"
declare -x MORE="-sl"
declare -x MPI4JAX_USE_CUDA_MPI="1"
declare -x MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x MPICH_GPU_SUPPORT_ENABLED="1"
declare -x NCCL_IB_DISABLE="1"
declare -x NCCL_SOCKET_IFNAME="hsn"
declare -x NCPUS="64"
declare -x OFFLOAD_INIT="on_start"
declare -x OLDPWD
declare -x OMP_NUM_THREADS="4"
declare -x OSCAR_HOME="/opt/oscar"
declare -x OSTYPE="linux"
declare -x PAGER="less"
declare -x PALS_TRANSFER="0"
declare -x PATH="/soft/applications/conda/2024-04-29/mconda3/bin:/soft/applications/conda/2024-04-29/mconda3/condabin:/soft/xalt/3.0.2-202408282050/bin:/soft/compilers/cudatoolkit/cuda-12.4.1/bin:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:/opt/cray/pe/hdf5/1.12.2.9/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/home/shourya01/.local/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/shourya01/bin:/opt/cray/pe/bin"
declare -x PAT_RT_PERFCTR_DISABLE_COMPONENTS="nvml,rocm_smi"
declare -x PBS_ACCOUNT="ParaLLMs"
declare -x PBS_ENVIRONMENT="PBS_BATCH"
declare -x PBS_JOBCOOKIE="497B99EF735D9B520926B19918402DBD"
declare -x PBS_JOBDIR="/home/shourya01"
declare -x PBS_JOBID="5235951.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_JOBNAME="bash"
declare -x PBS_MOMPORT="15003"
declare -x PBS_NODEFILE="/var/spool/pbs/aux/5235951.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_NODENUM="0"
declare -x PBS_O_HOME="/home/shourya01"
declare -x PBS_O_HOST="polaris-login-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_O_INTERACTIVE_AUTH_METHOD="resvport"
declare -x PBS_O_LANG="en_US.UTF-8"
declare -x PBS_O_LOGNAME="shourya01"
declare -x PBS_O_MAIL="/var/spool/mail/shourya01"
declare -x PBS_O_PATH="/home/shourya01/.local/bin:/home/shourya01/.vscode-server/cli/servers/Stable-dfaf44141ea9deb3b4096f7cd6d24e00c147a4b1/server/bin/remote-cli:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/soft/xalt/3.0.2-202408282050/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/nvidia/23.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/extras/qd/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/shourya01/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/dbhome/db2cat/sqllib/bin:/dbhome/db2cat/sqllib/adm:/dbhome/db2cat/sqllib/misc:/dbhome/db2cat/sqllib/gskit/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/opt/cray/pe/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.vscode-server/extensions/ms-python.debugpy-2025.8.0/bundled/scripts/noConfigScripts"
declare -x PBS_O_QUEUE="debug-scaling"
declare -x PBS_O_SHELL="/bin/bash"
declare -x PBS_O_SYSTEM="Linux"
declare -x PBS_O_WORKDIR="/home/shourya01"
declare -x PBS_QUEUE="debug-scaling"
declare -x PBS_TASKNUM="1"
declare -x PELOCAL_PRGENV="true"
declare -x PERFTOOLS_VERSION="23.12.0"
declare -x PE_DSMML_MODULE_NAME="cray-dsmml"
declare -x PE_DSMML_PKGCONFIG_LIBS="dsmml"
declare -x PE_ENV="GNU"
declare -x PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel:mpichf90"
declare -x PE_GCC_EXTERNAL="native"
declare -x PE_GCC_LEVEL="12"
declare -x PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig"
declare -x PE_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x PE_HDF5_PARALLEL_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel"
declare -x PE_HDF5_PARALLEL_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel"
declare -x PE_MPICH_FIXED_PRGENV="GNU"
declare -x PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90"
declare -x PE_MPICH_GENCOMPILERS_GNU="12.3"
declare -x PE_MPICH_GTL_DIR_amd_gfx906="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx908="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx90a="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx940="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx942="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia70="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia80="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia90="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_ponteVecchio="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_LIBS_amd_gfx906="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx908="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx90a="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx940="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx942="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_nvidia70="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia80="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia90="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_ponteVecchio="-lmpi_gtl_ze"
declare -x PE_MPICH_MODULE_NAME="cray-mpich"
declare -x PE_MPICH_PKGCONFIG_LIBS="mpich"
declare -x PE_MPICH_PKGCONFIG_VARIABLES="PE_MPICH_GTL_DIR_@accelerator@:PE_MPICH_GTL_LIBS_@accelerator@"
declare -x PE_PALS_PKGCONFIG_LIBS="libpals"
declare -x PE_PERFTOOLS_MPICH_LIBDIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib"
declare -x PE_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel:mpich:dsmml:darshan-runtime"
declare -x PE_PKGCONFIG_PRODUCTS="PE_PALS:PE_PMI:PE_MPICH:PE_DSMML"
declare -x PE_PMI_PKGCONFIG_LIBS="cray-pmi"
declare -x PE_PRODUCT_LIST="CRAYPE_X86_MILAN"
declare -x PKGCONFIG_ENABLED="1"
declare -x PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:/opt/cray/pals/1.3.4/lib/pkgconfig:/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:/opt/cray/pe/craype/2.7.30/pkg-config:/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig"
declare -x PROFILEREAD="true"
declare -x PWD="/home/shourya01"
declare -x PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages"
declare -x PYTHONUSERBASE="/home/shourya01/.local/polaris/conda/2024-04-29"
declare -x QT_SYSTEM_DIR="/usr/share/desktop-data"
declare -x SHELL="/bin/bash"
declare -x SHLVL="2"
declare -x SLURM_MPI_TYPE="cray_shasta"
declare -x STARSHIP_SESSION_KEY="1173716827358825"
declare -x STARSHIP_SHELL="bash"
declare -x TMPDIR="/var/tmp/pbs.5235951.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x TRITON_DISABLE_AUTOTUNE="1"
declare -x TZ="Etc/UTC"
declare -x USER="shourya01"
declare -x USE_PCM_DB="2"
declare -x WINDOWMANAGER="xterm"
declare -x XALT_DIR="/soft/xalt/3.0.2-202408282050"
declare -x XALT_EXECUTABLE_TRACKING="yes"
declare -x XALT_SAMPLING="no"
declare -x XALT_SCALAR_AND_SPSR_SAMPLING="yes"
declare -x XCURSOR_THEME="DMZ"
declare -x XDG_CONFIG_DIRS="/etc/xdg"
declare -x XDG_DATA_DIRS="/usr/share"
declare -x XKEYSYMDB="/usr/X11R6/lib/X11/XKeysymDB"
declare -x XLA_FLAGS="--xla_gpu_force_compilation_parallelism=1 --xla_gpu_cuda_data_dir=/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x XLA_PYTHON_CLIENT_PREALLOCATE="false"
declare -x XML_CATALOG_FILES="file:///soft/applications/conda/2024-04-29/mconda3/etc/xml/catalog file:///etc/xml/catalog"
declare -x XNLSPATH="/usr/X11R6/lib/X11/nls"
declare -x _CE_CONDA=""
declare -x _CE_M=""
declare -x _LMFILES_="/opt/cray/modulefiles/libfabric/1.15.2.0:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-network-ofi.lua:/opt/cray/pe/lmod/modulefiles/core/perftools-base/23.12.0.lua:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles/darshan/3.4.4:/soft/xalt/modulefiles/xalt/3.0.2-202408282050:/opt/cray/pe/lmod/modulefiles/core/gcc-native/12.3.lua:/opt/cray/pe/lmod/modulefiles/core/craype/2.7.30.lua:/opt/cray/pe/lmod/modulefiles/core/cray-dsmml/0.2.2.lua:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0/cray-mpich/8.1.28.lua:/opt/cray/pe/lmod/modulefiles/core/cray-pmi/6.1.13.lua:/opt/cray/pals/lmod/modulefiles/core/cray-pals/1.3.4.lua:/opt/cray/pals/lmod/modulefiles/core/cray-libpals/1.3.4.lua:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-x86-milan.lua:/opt/cray/pe/lmod/modulefiles/core/PrgEnv-gnu/8.5.0.lua:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2.9.lua:/soft/modulefiles/cudnn/9.1.0.lua:/soft/modulefiles/conda/2024-04-29.lua"
declare -x _ModuleTable001_="X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpQcmdFbnYgPSAiUHJnRW52LWdudSIsCmNvbXBpbGVyID0gImdjYy1uYXRpdmUiLApjcmF5cGUgPSAiY3JheXBlIiwKY3JheXBlX2NwdSA9ICJjcmF5cGUteDg2LW1pbGFuIiwKY3JheXBlX25ldHdvcmsgPSAiY3JheXBlLW5ldHdvcmstb2ZpIiwKZ2NjX2NvbXBpbGVyID0gImdjYy1uYXRpdmUiLApoZGY1ID0gImNyYXktaGRmNS1wYXJhbGxlbCIsCm1waSA9ICJjcmF5LW1waWNoIiwKcHl0aG9uID0gImNvbmRhIiwKfSwKbVQgPSB7ClsiUHJnRW52LWdudSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUv"
declare -x _ModuleTable002_="bG1vZC9tb2R1bGVmaWxlcy9jb3JlL1ByZ0Vudi1nbnUvOC41LjAubHVhIiwKZnVsbE5hbWUgPSAiUHJnRW52LWdudS84LjUuMCIsCmxvYWRPcmRlciA9IDE0LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gIlByZ0Vudi1nbnUiLAp3ViA9ICJeMDAwMDAwMDguMDAwMDAwMDA1Lip6ZmluYWwiLAp9LApjb25kYSA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY29uZGEvMjAyNC0wNC0yOS5sdWEiLApmdWxsTmFtZSA9ICJjb25kYS8yMDI0LTA0LTI5IiwKbG9hZE9yZGVyID0gMTcsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY29uZGEiLAp3ViA9ICJeMDAw"
declare -x _ModuleTable003_="MDIwMjQuKnpmaW5hbC0uMDAwMDAwMDA0Lip6ZmluYWwtLjAwMDAwMDAyOS4qemZpbmFsIiwKfSwKWyJjcmF5LWRzbW1sIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1kc21tbC8wLjIuMi5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWRzbW1sLzAuMi4yIiwKbG9hZE9yZGVyID0gOCwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWRzbW1sIiwKd1YgPSAiXjAwMDAwMDAwLjAwMDAwMDAwMi4wMDAwMDAwMDIuKnpmaW5hbCIsCn0sClsiY3JheS1oZGY1LXBhcmFsbGVsIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9v"
declare -x _ModuleTable004_="ZmkvMS4wL2NyYXktbXBpY2gvOC4wL2NyYXktaGRmNS1wYXJhbGxlbC8xLjEyLjIuOS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLApsb2FkT3JkZXIgPSAxNSwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDEyLjAwMDAwMDAwMi4wMDAwMDAwMDkuKnpmaW5hbCIsCn0sClsiY3JheS1saWJwYWxzIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wYWxzL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5LWxpYnBhbHMvMS4zLjQubHVhIiwKZnVsbE5hbWUgPSAiY3JheS1s"
declare -x _ModuleTable005_="aWJwYWxzLzEuMy40IiwKbG9hZE9yZGVyID0gMTIsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1saWJwYWxzIiwKd1YgPSAiXjAwMDAwMDAxLjAwMDAwMDAwMy4wMDAwMDAwMDQuKnpmaW5hbCIsCn0sClsiY3JheS1tcGljaCJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMC9jcmF5LW1waWNoLzguMS4yOC5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LW1waWNoLzguMS4yOCIsCmxvYWRPcmRlciA9IDksCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1tcGljaCIsCndWID0gIl4w"
declare -x _ModuleTable006_="MDAwMDAwOC4wMDAwMDAwMDEuMDAwMDAwMDI4Lip6ZmluYWwiLAp9LApbImNyYXktcGFscyJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGFscy9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wYWxzLzEuMy40Lmx1YSIsCmZ1bGxOYW1lID0gImNyYXktcGFscy8xLjMuNCIsCmxvYWRPcmRlciA9IDExLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcGFscyIsCndWID0gIl4wMDAwMDAwMS4wMDAwMDAwMDMuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImNyYXktcG1pIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wbWkvNi4xLjEzLmx1YSIsCmZ1bGxOYW1lID0gImNy"
declare -x _ModuleTable007_="YXktcG1pLzYuMS4xMyIsCmxvYWRPcmRlciA9IDEwLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcG1pIiwKd1YgPSAiXjAwMDAwMDA2LjAwMDAwMDAwMS4wMDAwMDAwMTMuKnpmaW5hbCIsCn0sCmNyYXlwZSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5cGUvMi43LjMwLmx1YSIsCmZ1bGxOYW1lID0gImNyYXlwZS8yLjcuMzAiLApsb2FkT3JkZXIgPSA3LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZSIsCndWID0gIl4wMDAwMDAwMi4wMDAwMDAwMDcuMDAwMDAwMDMwLip6ZmluYWwiLAp9LApb"
declare -x _ModuleTable008_="ImNyYXlwZS1uZXR3b3JrLW9maSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS1uZXR3b3JrLW9maS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5cGUtbmV0d29yay1vZmkiLApsb2FkT3JkZXIgPSAyLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS1uZXR3b3JrLW9maSIsCndWID0gIk0uKnpmaW5hbCIsCn0sClsiY3JheXBlLXg4Ni1taWxhbiJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS14ODYtbWlsYW4ubHVhIiwKZnVsbE5hbWUgPSAiY3JheXBl"
declare -x _ModuleTable009_="LXg4Ni1taWxhbiIsCmxvYWRPcmRlciA9IDEzLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS14ODYtbWlsYW4iLAp3ViA9ICJNLip6ZmluYWwiLAp9LApjdWRubiA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY3Vkbm4vOS4xLjAubHVhIiwKZnVsbE5hbWUgPSAiY3Vkbm4vOS4xLjAiLApsb2FkT3JkZXIgPSAxNiwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjdWRubi85LjEuMCIsCndWID0gIjAwMDAwMDAwOS4wMDAwMDAwMDEuKnpmaW5hbCIsCn0sCmRhcnNoYW4gPSB7CmZuID0gIi9zb2Z0L3BlcmZ0b29scy9k"
declare -x _ModuleTable010_="YXJzaGFuL2RhcnNoYW4tMy40LjQvc2hhcmUvY3JheXBlLTIueC9tb2R1bGVmaWxlcy9kYXJzaGFuLzMuNC40IiwKZnVsbE5hbWUgPSAiZGFyc2hhbi8zLjQuNCIsCmxvYWRPcmRlciA9IDQsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiZGFyc2hhbiIsCndWID0gIjAwMDAwMDAwMy4wMDAwMDAwMDQuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImdjYy1uYXRpdmUiXSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9nY2MtbmF0aXZlLzEyLjMubHVhIiwKZnVsbE5hbWUgPSAiZ2NjLW5hdGl2ZS8xMi4zIiwKbG9hZE9yZGVyID0gNiwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0"
declare -x _ModuleTable011_="YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJnY2MtbmF0aXZlIiwKd1YgPSAiXjAwMDAwMDEyLjAwMDAwMDAwMy4qemZpbmFsIiwKfSwKbGliZmFicmljID0gewpmbiA9ICIvb3B0L2NyYXkvbW9kdWxlZmlsZXMvbGliZmFicmljLzEuMTUuMi4wIiwKZnVsbE5hbWUgPSAibGliZmFicmljLzEuMTUuMi4wIiwKbG9hZE9yZGVyID0gMSwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJsaWJmYWJyaWMiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDE1LjAwMDAwMDAwMi4qemZpbmFsIiwKfSwKWyJwZXJmdG9vbHMtYmFzZSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb3JlL3BlcmZ0b29s"
declare -x _ModuleTable012_="cy1iYXNlLzIzLjEyLjAubHVhIiwKZnVsbE5hbWUgPSAicGVyZnRvb2xzLWJhc2UvMjMuMTIuMCIsCmxvYWRPcmRlciA9IDMsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAicGVyZnRvb2xzLWJhc2UiLAp3ViA9ICJeMDAwMDAwMjMuMDAwMDAwMDEyLip6ZmluYWwiLAp9LAp4YWx0ID0gewpmbiA9ICIvc29mdC94YWx0L21vZHVsZWZpbGVzL3hhbHQvMy4wLjItMjAyNDA4MjgyMDUwIiwKZnVsbE5hbWUgPSAieGFsdC8zLjAuMi0yMDI0MDgyODIwNTAiLApsb2FkT3JkZXIgPSA1LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gInhhbHQiLAp3ViA9ICJeMDAwMDAw"
declare -x _ModuleTable013_="MDMuMDAwMDAwMDAwLjAwMDAwMDAwMi4qemZpbmFsLS4yMDI0MDgyODIwNTAuKnpmaW5hbCIsCn0sCn0sCm1wYXRoQSA9IHsKCiIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9oZGY1LXBhcmFsbGVsL2dudS8xMi4wL29maS8xLjAvY3JheS1tcGljaC84LjAvY3JheS1oZGY1LXBhcmFsbGVsLzEuMTIuMiIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3B1L3g4Ni1taWxhbi8xLjAiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9vZmkvMS4wL2NyYXktbXBpY2gvOC4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMCIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxl"
declare -x _ModuleTable014_="ZmlsZXMvbWl4X2NvbXBpbGVycyIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29tcGlsZXIvZ251LzEyLjAiLCAiL3NvZnQvbW9kdWxlZmlsZXMiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL3BlcmZ0b29scy8yMy4xMi4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9uZXQvb2ZpLzEuMCIsICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0xpbnV4IgosICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmUiLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZSIKLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BhbHMvbG1vZC9tb2R1bGVmaWxlcy9jb3JlIgosICIvb3B0L2Ny"
declare -x _ModuleTable015_="YXkvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZSIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdCIKLCAiL3NvZnQvcGVyZnRvb2xzL2RhcnNoYW4vZGFyc2hhbi0zLjQuNC9zaGFyZS9jcmF5cGUtMi54L21vZHVsZWZpbGVzIiwgIi9zb2Z0L3hhbHQvbW9kdWxlZmlsZXMiLAp9LApzeXN0ZW1CYXNlTVBBVEggPSAiL3Vzci9zaGFyZS9tb2R1bGVmaWxlcy9MaW51eDovdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmU6L3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZTovdXNyL3NoYXJlL2xtb2QvbG1vZC9tb2R1bGVmaWxlczovb3B0L2NyYXkvcGFscy9sbW9kL21vZHVs"
declare -x _ModuleTable016_="ZWZpbGVzL2NvcmU6L29wdC9jcmF5L21vZHVsZWZpbGVzOi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmU6L29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdDovc29mdC9wZXJmdG9vbHMvZGFyc2hhbi9kYXJzaGFuLTMuNC40L3NoYXJlL2NyYXlwZS0yLngvbW9kdWxlZmlsZXM6L3NvZnQveGFsdC9tb2R1bGVmaWxlcyIsCn0K"
declare -x _ModuleTable_Sz_="16"
declare -x __LMOD_Priority_PATH="/soft/xalt/3.0.2-202408282050/bin:-100"
declare -x __LMOD_REF_COUNT_COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin:1"
declare -x __LMOD_REF_COUNT_CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:1;/opt/cray/pe/pmi/6.1.13/lib:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:1;/opt/cray/pe/mpich/8.1.28/gtl/lib:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib:1;/opt/cray/pe/perftools/23.12.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:1;/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:1;/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:1;/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:1;/soft/perftools/darshan/darshan-3.4.4/lib:1;/opt/cray/pe/papi/7.0.1.2/lib64:1;/opt/cray/libfabric/1.15.2.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so:1"
declare -x __LMOD_REF_COUNT_MANPATH="/opt/cray/pals/1.3.4/man:2;/opt/cray/pe/pmi/6.1.13/man:1;/opt/cray/pe/mpich/8.1.28/ofi/man:1;/opt/cray/pe/mpich/8.1.28/man/mpich:1;/opt/cray/pe/dsmml/0.2.2/dsmml/man:1;/opt/cray/pe/craype/2.7.30/man:1;/opt/cray/pe/perftools/23.12.0/man:1;/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:1;/opt/cray/libfabric/1.15.2.0/share/man:1;/usr/share/lmod/lmod/share/man:1;/home/shourya01/.local/man:1;/usr/local/man:1;/usr/share/man:1;/usr/man:1;/opt/c3/man:1;/opt/pbs/share/man:1;/opt/clmgr/man:1;/opt/sgi/share/man:1;/opt/clmgr/share/man:1;/opt/clmgr/lib/cm-cli/man:1"
declare -x __LMOD_REF_COUNT_MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:1;/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:1;/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:1;/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:1;/opt/cray/pe/lmod/modulefiles/mix_compilers:1;/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:1;/soft/modulefiles:1;/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:1;/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:1;/usr/share/modulefiles/Linux:1;/usr/share/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles:1;/opt/cray/pals/lmod/modulefiles/core:1;/opt/cray/modulefiles:1;/opt/cray/pe/lmod/modulefiles/core:1;/opt/cray/pe/lmod/modulefiles/craype-targets/default:1;/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:1;/soft/xalt/modulefiles:1"
declare -x __LMOD_REF_COUNT_PATH="/soft/xalt/3.0.2-202408282050/bin:1;/soft/compilers/cudatoolkit/cuda-12.4.1/bin:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:1;/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:1;/opt/cray/pe/hdf5/1.12.2.9/bin:1;/opt/cray/pals/1.3.4/bin:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:1;/opt/cray/pe/mpich/8.1.28/bin:1;/opt/cray/pe/craype/2.7.30/bin:1;/home/shourya01/.local/bin:4;/soft/perftools/darshan/darshan-3.4.4/bin:1;/opt/cray/pe/perftools/23.12.0/bin:1;/opt/cray/pe/papi/7.0.1.2/bin:1;/opt/cray/libfabric/1.15.2.0/bin:1;/opt/clmgr/sbin:1;/opt/clmgr/bin:1;/opt/sgi/sbin:1;/opt/sgi/bin:1;/usr/local/bin:1;/usr/bin:1;/bin:2;/opt/c3/bin:1;/usr/lib/mit/bin:1;/usr/lib/mit/sbin:1;/opt/pbs/bin:1;/sbin:1;/home/shourya01/bin:1;/opt/cray/pe/bin:1"
declare -x __LMOD_REF_COUNT_PE_DSMML_PKGCONFIG_LIBS="dsmml:1"
declare -x __LMOD_REF_COUNT_PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:1;hdf5_fortran_parallel:1;mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FIXED_PRGENV="GNU:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_GENCOMPILERS_GNU="12.3:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_PKGCONFIG_LIBS="mpich:1"
declare -x __LMOD_REF_COUNT_PE_PALS_PKGCONFIG_LIBS="libpals:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_LIBS="hdf5_hl_parallel:1;hdf5_parallel:1;mpich:1;dsmml:1;darshan-runtime:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_PRODUCTS="PE_PALS:1;PE_PMI:1;PE_MPICH:1;PE_DSMML:1"
declare -x __LMOD_REF_COUNT_PE_PMI_PKGCONFIG_LIBS="cray-pmi:1"
declare -x __LMOD_REF_COUNT_PE_PRODUCT_LIST="CRAYPE_X86_MILAN:1;PERFTOOLS:1;CRAYPAT:1"
declare -x __LMOD_REF_COUNT_PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:1;/opt/cray/pals/1.3.4/lib/pkgconfig:1;/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:1;/opt/cray/pe/craype/2.7.30/pkg-config:1;/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:1;/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages:1"
declare -x ftp_proxy="http://proxy.alcf.anl.gov:3128"
declare -x http_proxy="http://proxy.alcf.anl.gov:3128"
declare -x https_proxy="http://proxy.alcf.anl.gov:3128"
declare -x no_proxy="admin,polaris-adminvm-01,localhost,*.cm.polaris.alcf.anl.gov,polaris-*,*.polaris.alcf.anl.gov,*.alcf.anl.gov"
Running on 5 nodes
Total number of GPUs: 20
Connected to tcp://x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov:7919
Found executable /soft/applications/conda/2024-04-29/mconda3/bin/python
Launching application 104aa459-600a-4e1f-88c2-216b2747128e
Using PMI port 42977,42978
[2025-06-23 08:07:35,716] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 08:07:35,716] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 08:07:35,716] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 08:07:35,716] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 08:07:35,716] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 08:07:35,716] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 08:07:35,716] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 08:07:35,716] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 08:07:35,718] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 08:07:35,718] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 08:07:35,718] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 08:07:35,718] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 08:07:35,747] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 08:07:35,747] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 08:07:35,747] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 08:07:35,747] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 08:07:35,747] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 08:07:35,747] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 08:07:35,747] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 08:07:35,747] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:44,999] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 08:07:44,999] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:45,000] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:45,000] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 08:07:45,000] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:45,000] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:45,000] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 08:07:45,000] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:45,000] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 08:07:45,000] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 08:07:45,000] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:45,000] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 08:07:45,000] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:45,000] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 08:07:45,000] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 08:07:45,000] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:45,000] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:45,000] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:45,000] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 08:07:45,000] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:45,001] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:45,000] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 08:07:45,000] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:45,001] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 08:07:45,001] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:45,000] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 08:07:45,000] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:45,001] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 08:07:45,001] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:45,000] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 08:07:45,000] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:45,000] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 08:07:45,000] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:45,001] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 08:07:45,001] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 08:07:45,000] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 08:07:45,000] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 08:07:45,000] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 08:07:45,001] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 08:07:45,000] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=20, master_addr=10.140.57.41, master_port=29500
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=12, local_rank=0, world_size=20, master_addr=10.140.57.41, master_port=29500
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=20, master_addr=10.140.57.41, master_port=29500
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=13, local_rank=1, world_size=20, master_addr=10.140.57.41, master_port=29500
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=16, local_rank=0, world_size=20, master_addr=10.140.57.41, master_port=29500
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=8, local_rank=0, world_size=20, master_addr=10.140.57.41, master_port=29500
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=20, master_addr=10.140.57.41, master_port=29500
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=14, local_rank=2, world_size=20, master_addr=10.140.57.41, master_port=29500
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=17, local_rank=1, world_size=20, master_addr=10.140.57.41, master_port=29500
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=15, local_rank=3, world_size=20, master_addr=10.140.57.41, master_port=29500
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=20, master_addr=10.140.57.41, master_port=29500
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=18, local_rank=2, world_size=20, master_addr=10.140.57.41, master_port=29500
[2025-06-23 08:07:45,011] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=0, world_size=20, master_addr=10.140.57.41, master_port=29500
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=9, local_rank=1, world_size=20, master_addr=10.140.57.41, master_port=29500
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=19, local_rank=3, world_size=20, master_addr=10.140.57.41, master_port=29500
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=10, local_rank=2, world_size=20, master_addr=10.140.57.41, master_port=29500
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=1, world_size=20, master_addr=10.140.57.41, master_port=29500
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=3, world_size=20, master_addr=10.140.57.41, master_port=29500
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=11, local_rank=3, world_size=20, master_addr=10.140.57.41, master_port=29500
[2025-06-23 08:07:45,011] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=2, world_size=20, master_addr=10.140.57.41, master_port=29500
Initialized deepspeed on global rank 18, local rank 2 with world size 20.
Initialized deepspeed on global rank 16, local rank 0 with world size 20.
Initialized deepspeed on global rank 17, local rank 1 with world size 20.
[rank16]: Traceback (most recent call last):
[rank16]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank16]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank16]:                                 ^^^^^^^^^^^^^^^^^
[rank16]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank16]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank16]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank16]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank16]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
[rank18]: Traceback (most recent call last):
[rank18]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank18]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank18]:                                 ^^^^^^^^^^^^^^^^^
[rank18]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank18]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank18]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank18]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank18]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
[rank17]: Traceback (most recent call last):
[rank17]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank17]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank17]:                                 ^^^^^^^^^^^^^^^^^
[rank17]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank17]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank17]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank17]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank17]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
Initialized deepspeed on global rank 8, local rank 0 with world size 20.
Initialized deepspeed on global rank 10, local rank 2 with world size 20.
[rank8]: Traceback (most recent call last):
[rank8]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank8]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank8]:                                 ^^^^^^^^^^^^^^^^^
[rank8]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank8]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank8]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank8]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank8]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
[rank10]: Traceback (most recent call last):
[rank10]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank10]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank10]:                                 ^^^^^^^^^^^^^^^^^
[rank10]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank10]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank10]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank10]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank10]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
Initialized deepspeed on global rank 11, local rank 3 with world size 20.
[rank11]: Traceback (most recent call last):
[rank11]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank11]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank11]:                                 ^^^^^^^^^^^^^^^^^
[rank11]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank11]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank11]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank11]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank11]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
Initialized deepspeed on global rank 4, local rank 0 with world size 20.
Initialized deepspeed on global rank 9, local rank 1 with world size 20.
Initialized deepspeed on global rank 5, local rank 1 with world size 20.
[rank9]: Traceback (most recent call last):
[rank9]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank9]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank9]:                                 ^^^^^^^^^^^^^^^^^
[rank9]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank9]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank9]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank9]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank9]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
Initialized deepspeed on global rank 19, local rank 3 with world size 20.
Initialized deepspeed on global rank 6, local rank 2 with world size 20.
Initialized deepspeed on global rank 7, local rank 3 with world size 20.
[rank19]: Traceback (most recent call last):
[rank19]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank19]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank19]:                                 ^^^^^^^^^^^^^^^^^
[rank19]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank19]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank19]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank19]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank19]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
[rank4]: Traceback (most recent call last):
[rank4]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank4]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank4]:                                 ^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank4]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank4]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank4]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank4]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
[rank5]: Traceback (most recent call last):
[rank5]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank5]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank5]:                                 ^^^^^^^^^^^^^^^^^
[rank5]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank5]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank5]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank5]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank5]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
[rank6]: Traceback (most recent call last):
[rank6]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank6]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank6]:                                 ^^^^^^^^^^^^^^^^^
[rank6]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank6]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank6]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank6]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank6]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
[rank7]: Traceback (most recent call last):
[rank7]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank7]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank7]:                                 ^^^^^^^^^^^^^^^^^
[rank7]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank7]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank7]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank7]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank7]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
Initialized deepspeed on global rank 14, local rank 2 with world size 20.
[rank14]: Traceback (most recent call last):
[rank14]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank14]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank14]:                                 ^^^^^^^^^^^^^^^^^
[rank14]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank14]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank14]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank14]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank14]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
Initialized deepspeed on global rank 12, local rank 0 with world size 20.
Initialized deepspeed on global rank 15, local rank 3 with world size 20.
[rank12]: Traceback (most recent call last):
[rank12]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank12]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank12]:                                 ^^^^^^^^^^^^^^^^^
[rank12]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank12]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank12]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank12]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank12]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
[rank15]: Traceback (most recent call last):
[rank15]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank15]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank15]:                                 ^^^^^^^^^^^^^^^^^
[rank15]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank15]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank15]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank15]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank15]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
Initialized deepspeed on global rank 13, local rank 1 with world size 20.
[rank13]: Traceback (most recent call last):
[rank13]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank13]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank13]:                                 ^^^^^^^^^^^^^^^^^
[rank13]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank13]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank13]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank13]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank13]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
Initialized deepspeed on global rank 0, local rank 0 with world size 20.
Initialized deepspeed on global rank 1, local rank 1 with world size 20.
Initialized deepspeed on global rank 3, local rank 3 with world size 20.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank0]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank0]:                                 ^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank0]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank0]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank0]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank0]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank1]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank1]:                                 ^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank1]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank1]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank1]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank1]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank3]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank3]:                                 ^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank3]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank3]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank3]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank3]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
Initialized deepspeed on global rank 2, local rank 2 with world size 20.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank2]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank2]:                                 ^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank2]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+"nc")), mode="r")
[rank2]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank2]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank2]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/californianc'
x3004c0s1b1n0.hsn.cm.polaris.alcf.anl.gov: rank 8 exited with code 1
x3004c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 16 exited with code 1
x3004c0s13b1n0.hsn.cm.polaris.alcf.anl.gov: rank 4 exited with code 1
x3004c0s25b1n0.hsn.cm.polaris.alcf.anl.gov: rank 12 exited with code 1
x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 0 exited with code 1
x3004c0s1b1n0.hsn.cm.polaris.alcf.anl.gov: rank 9 exited with code 1
x3004c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 18 exited with code 1
x3004c0s1b1n0.hsn.cm.polaris.alcf.anl.gov: rank 10 exited with code 1
x3004c0s13b1n0.hsn.cm.polaris.alcf.anl.gov: rank 6 exited with code 1
x3004c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 19 exited with code 1
x3004c0s13b1n0.hsn.cm.polaris.alcf.anl.gov: rank 5 exited with code 1
x3004c0s25b1n0.hsn.cm.polaris.alcf.anl.gov: rank 15 exited with code 1
x3004c0s25b1n0.hsn.cm.polaris.alcf.anl.gov: rank 13 exited with code 1
x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 2 exited with code 1
x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 1 exited with code 1
x3004c0s25b1n0.hsn.cm.polaris.alcf.anl.gov: rank 14 exited with code 1
x3004c0s1b1n0.hsn.cm.polaris.alcf.anl.gov: rank 11 exited with code 1
x3004c0s13b1n0.hsn.cm.polaris.alcf.anl.gov: rank 7 exited with code 1
x3003c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 3 exited with code 1
x3004c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 17 exited with code 1
Application 104aa459 resources: utime=109s stime=87s maxrss=2729004KB inblock=15855084 oublock=336 minflt=4442147 majflt=75741 nvcsw=835400 nivcsw=9372
Training completed
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled

Lmod is automatically replacing "nvhpc/23.9" with "gcc-native/12.3".


Lmod is automatically replacing "PrgEnv-nvhpc/8.5.0" with "PrgEnv-gnu/8.5.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.28

declare -x APP2_STATE="23.12.0"
declare -x BASH_ENV="/usr/share/lmod/lmod/init/bash"
declare -x C3_RSH="ssh -oConnectTimeout=10 -oForwardX11=no"
declare -x CFLAGS="-I/soft/applications/conda/2024-04-29/mconda3/include"
declare -x COLORTERM="1"
declare -x COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin"
declare -x CONDA_DEFAULT_ENV="base"
declare -x CONDA_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/conda"
declare -x CONDA_PREFIX="/soft/applications/conda/2024-04-29/mconda3"
declare -x CONDA_PROMPT_MODIFIER="(2024-04-29/base) "
declare -x CONDA_PYTHON_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/python"
declare -x CONDA_SHLVL="1"
declare -x CPU="x86_64"
declare -x CRAYPAT_LD_LIBRARY_PATH="/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAYPAT_OPTS_EXECUTABLE="libexec64/opts"
declare -x CRAYPAT_ROOT="/opt/cray/pe/perftools/23.12.0"
declare -x CRAYPE_DIR="/opt/cray/pe/craype/2.7.30"
declare -x CRAYPE_NETWORK_TARGET="ofi"
declare -x CRAYPE_VERSION="2.7.30"
declare -x CRAY_CPU_TARGET="x86-milan"
declare -x CRAY_DSMML_BASEDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_DIR="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_PREFIX="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_ROOTDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_VER="0.2.2"
declare -x CRAY_DSMML_VERSION="0.2.2"
declare -x CRAY_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x CRAY_HDF5_PARALLEL_PREFIX="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x CRAY_HDF5_PARALLEL_VERSION="1.12.2.9"
declare -x CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:/opt/cray/pe/pmi/6.1.13/lib:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:/opt/cray/pe/mpich/8.1.28/gtl/lib:/opt/cray/pe/dsmml/0.2.2/dsmml/lib:/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAY_LMOD_COMPILER="gnu/12.0"
declare -x CRAY_LMOD_CPU="x86-milan/1.0"
declare -x CRAY_LMOD_MPI="cray-mpich/8.0"
declare -x CRAY_LMOD_NET="ofi/1.0"
declare -x CRAY_MPICH_BASEDIR="/opt/cray/pe/mpich/8.1.28/ofi"
declare -x CRAY_MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_PREFIX="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_ROOTDIR="/opt/cray/pe/mpich/8.1.28"
declare -x CRAY_MPICH_VER="8.1.28"
declare -x CRAY_MPICH_VERSION="8.1.28"
declare -x CRAY_PERFTOOLS_PREFIX="/opt/cray/pe/perftools/23.12.0"
declare -x CRAY_PERFTOOLS_VERSION="23.12.0"
declare -x CRAY_PMI_INCLUDE_OPTS="-I/opt/cray/pe/pmi/6.1.13/include"
declare -x CRAY_PMI_POST_LINK_OPTS="-L/opt/cray/pe/pmi/6.1.13/lib"
declare -x CRAY_PMI_PREFIX="/opt/cray/pe/pmi/6.1.13"
declare -x CRAY_PMI_VERSION="6.1.13"
declare -x CSHEDIT="emacs"
declare -x CUDA_HOME="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_TOOLKIT_BASE="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDNN_HOME="/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/"
declare -x ENVIRONMENT="BATCH"
declare -x ENV_NAME="conda/2024-04-29"
declare -x FROM_HEADER=""
declare -x GCC_PATH="/usr/bin"
declare -x GCC_PREFIX="/usr/lib64/gcc/x86_64-suse-linux/12"
declare -x GCC_VERSION="12.3"
declare -x GNU_VERSION="12.3"
declare -x GPG_TTY="not a tty"
declare -x GSETTINGS_SCHEMA_DIR="/soft/applications/conda/2024-04-29/mconda3/share/glib-2.0/schemas"
declare -x GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=""
declare -x G_BROKEN_FILENAMES="1"
declare -x G_FILENAME_ENCODING="@locale,UTF-8,ISO-8859-15,CP1252"
declare -x HDF5_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HDF5_ROOT="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HISTSIZE="1000"
declare -x HOME="/home/shourya01"
declare -x HOST="x3005c0s37b0n0"
declare -x HOSTNAME="x3005c0s37b0n0"
declare -x HOSTTYPE="x86_64"
declare -x HTTPS_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x HTTP_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x LANG="en_US.UTF-8"
declare -x LANGUAGE="en_US.UTF-8"
declare -x LDFLAGS="-L/soft/applications/conda/2024-04-29/mconda3/lib -Wl,--enable-new-dtags,-rpath,/soft/applications/conda/2024-04-29/mconda3/lib"
declare -x LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:/soft/perftools/darshan/darshan-3.4.4/lib:/opt/cray/pe/papi/7.0.1.2/lib64:/opt/cray/libfabric/1.15.2.0/lib64"
declare -x LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so"
declare -x LESS="-M -I -R"
declare -x LESSCLOSE="lessclose.sh %s %s"
declare -x LESSKEY="/etc/lesskey.bin"
declare -x LESSOPEN="lessopen.sh %s"
declare -x LESS_ADVANCED_PREPROCESSOR="no"
declare -x LMOD_CMD="/usr/share/lmod/lmod/libexec/lmod"
declare -x LMOD_DIR="/usr/share/lmod/lmod/libexec"
declare -x LMOD_FAMILY_COMPILER="gcc-native"
declare -x LMOD_FAMILY_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_CRAYPE="craype"
declare -x LMOD_FAMILY_CRAYPE_CPU="craype-x86-milan"
declare -x LMOD_FAMILY_CRAYPE_CPU_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_NETWORK="craype-network-ofi"
declare -x LMOD_FAMILY_CRAYPE_NETWORK_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_VERSION="2.7.30"
declare -x LMOD_FAMILY_GCC_COMPILER="gcc-native"
declare -x LMOD_FAMILY_GCC_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_HDF5="cray-hdf5-parallel"
declare -x LMOD_FAMILY_HDF5_VERSION="1.12.2.9"
declare -x LMOD_FAMILY_MPI="cray-mpich"
declare -x LMOD_FAMILY_MPI_VERSION="8.1.28"
declare -x LMOD_FAMILY_PRGENV="PrgEnv-gnu"
declare -x LMOD_FAMILY_PRGENV_VERSION="8.5.0"
declare -x LMOD_FAMILY_PYTHON="conda"
declare -x LMOD_FAMILY_PYTHON_VERSION="2024-04-29"
declare -x LMOD_PKG="/usr/share/lmod/lmod"
declare -x LMOD_ROOT="/usr/share/lmod"
declare -x LMOD_SETTARG_FULL_SUPPORT="no"
declare -x LMOD_SYSTEM_DEFAULT_MODULES="PrgEnv-nvhpc:craype-network-ofi:perftools-base:darshan:xalt"
declare -x LMOD_VERSION="8.7.34"
declare -x LMOD_sys="Linux"
declare -x LOADEDMODULES="libfabric/1.15.2.0:craype-network-ofi:perftools-base/23.12.0:darshan/3.4.4:xalt/3.0.2-202408282050:gcc-native/12.3:craype/2.7.30:cray-dsmml/0.2.2:cray-mpich/8.1.28:cray-pmi/6.1.13:cray-pals/1.3.4:cray-libpals/1.3.4:craype-x86-milan:PrgEnv-gnu/8.5.0:cray-hdf5-parallel/1.12.2.9:cudnn/9.1.0:conda/2024-04-29"
declare -x LOGNAME="shourya01"
declare -x MACHTYPE="x86_64-suse-linux"
declare -x MAIL="/var/spool/mail/shourya01"
declare -x MANPATH="/opt/cray/pals/1.3.4/man:/opt/cray/pe/pmi/6.1.13/man:/opt/cray/pe/mpich/8.1.28/ofi/man:/opt/cray/pe/mpich/8.1.28/man/mpich:/opt/cray/pe/dsmml/0.2.2/dsmml/man:/opt/cray/pe/craype/2.7.30/man:/opt/cray/pe/perftools/23.12.0/man:/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:/opt/cray/libfabric/1.15.2.0/share/man:/usr/share/lmod/lmod/share/man:/home/shourya01/.local/man:/usr/local/man:/usr/share/man:/usr/man:/opt/c3/man:/opt/pbs/share/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man"
declare -x MINICOM="-c on"
declare -x MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:/opt/cray/pe/lmod/modulefiles/mix_compilers:/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:/soft/modulefiles:/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:/usr/share/modulefiles/Linux:/usr/share/modulefiles/Core:/usr/share/lmod/lmod/modulefiles/Core:/usr/share/lmod/lmod/modulefiles:/opt/cray/pals/lmod/modulefiles/core:/opt/cray/modulefiles:/opt/cray/pe/lmod/modulefiles/core:/opt/cray/pe/lmod/modulefiles/craype-targets/default:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:/soft/xalt/modulefiles"
declare -x MODULEPATH_ROOT="/usr/share/modulefiles"
declare -x MODULESHOME="/usr/share/lmod/lmod"
declare -x MORE="-sl"
declare -x MPI4JAX_USE_CUDA_MPI="1"
declare -x MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x MPICH_GPU_SUPPORT_ENABLED="1"
declare -x NCCL_IB_DISABLE="1"
declare -x NCCL_SOCKET_IFNAME="hsn"
declare -x NCPUS="64"
declare -x OFFLOAD_INIT="on_start"
declare -x OLDPWD
declare -x OMP_NUM_THREADS="4"
declare -x OSCAR_HOME="/opt/oscar"
declare -x OSTYPE="linux"
declare -x PAGER="less"
declare -x PALS_TRANSFER="0"
declare -x PATH="/soft/applications/conda/2024-04-29/mconda3/bin:/soft/applications/conda/2024-04-29/mconda3/condabin:/soft/xalt/3.0.2-202408282050/bin:/soft/compilers/cudatoolkit/cuda-12.4.1/bin:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:/opt/cray/pe/hdf5/1.12.2.9/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/home/shourya01/.local/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/shourya01/bin:/opt/cray/pe/bin"
declare -x PAT_RT_PERFCTR_DISABLE_COMPONENTS="nvml,rocm_smi"
declare -x PBS_ACCOUNT="ParaLLMs"
declare -x PBS_ENVIRONMENT="PBS_BATCH"
declare -x PBS_JOBCOOKIE="402AD5180475375F5C3029B65D7A531A"
declare -x PBS_JOBDIR="/home/shourya01"
declare -x PBS_JOBID="5236102.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_JOBNAME="bash"
declare -x PBS_MOMPORT="15003"
declare -x PBS_NODEFILE="/var/spool/pbs/aux/5236102.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_NODENUM="0"
declare -x PBS_O_HOME="/home/shourya01"
declare -x PBS_O_HOST="polaris-login-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_O_INTERACTIVE_AUTH_METHOD="resvport"
declare -x PBS_O_LANG="en_US.UTF-8"
declare -x PBS_O_LOGNAME="shourya01"
declare -x PBS_O_MAIL="/var/spool/mail/shourya01"
declare -x PBS_O_PATH="/home/shourya01/.local/bin:/home/shourya01/.vscode-server/cli/servers/Stable-dfaf44141ea9deb3b4096f7cd6d24e00c147a4b1/server/bin/remote-cli:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/soft/xalt/3.0.2-202408282050/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/nvidia/23.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/extras/qd/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/shourya01/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/dbhome/db2cat/sqllib/bin:/dbhome/db2cat/sqllib/adm:/dbhome/db2cat/sqllib/misc:/dbhome/db2cat/sqllib/gskit/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/opt/cray/pe/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.vscode-server/extensions/ms-python.debugpy-2025.8.0/bundled/scripts/noConfigScripts"
declare -x PBS_O_QUEUE="debug-scaling"
declare -x PBS_O_SHELL="/bin/bash"
declare -x PBS_O_SYSTEM="Linux"
declare -x PBS_O_WORKDIR="/home/shourya01"
declare -x PBS_QUEUE="debug-scaling"
declare -x PBS_TASKNUM="1"
declare -x PELOCAL_PRGENV="true"
declare -x PERFTOOLS_VERSION="23.12.0"
declare -x PE_DSMML_MODULE_NAME="cray-dsmml"
declare -x PE_DSMML_PKGCONFIG_LIBS="dsmml"
declare -x PE_ENV="GNU"
declare -x PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel:mpichf90"
declare -x PE_GCC_EXTERNAL="native"
declare -x PE_GCC_LEVEL="12"
declare -x PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig"
declare -x PE_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x PE_HDF5_PARALLEL_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel"
declare -x PE_HDF5_PARALLEL_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel"
declare -x PE_MPICH_FIXED_PRGENV="GNU"
declare -x PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90"
declare -x PE_MPICH_GENCOMPILERS_GNU="12.3"
declare -x PE_MPICH_GTL_DIR_amd_gfx906="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx908="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx90a="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx940="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx942="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia70="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia80="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia90="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_ponteVecchio="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_LIBS_amd_gfx906="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx908="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx90a="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx940="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx942="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_nvidia70="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia80="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia90="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_ponteVecchio="-lmpi_gtl_ze"
declare -x PE_MPICH_MODULE_NAME="cray-mpich"
declare -x PE_MPICH_PKGCONFIG_LIBS="mpich"
declare -x PE_MPICH_PKGCONFIG_VARIABLES="PE_MPICH_GTL_DIR_@accelerator@:PE_MPICH_GTL_LIBS_@accelerator@"
declare -x PE_PALS_PKGCONFIG_LIBS="libpals"
declare -x PE_PERFTOOLS_MPICH_LIBDIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib"
declare -x PE_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel:mpich:dsmml:darshan-runtime"
declare -x PE_PKGCONFIG_PRODUCTS="PE_PALS:PE_PMI:PE_MPICH:PE_DSMML"
declare -x PE_PMI_PKGCONFIG_LIBS="cray-pmi"
declare -x PE_PRODUCT_LIST="CRAYPE_X86_MILAN"
declare -x PKGCONFIG_ENABLED="1"
declare -x PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:/opt/cray/pals/1.3.4/lib/pkgconfig:/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:/opt/cray/pe/craype/2.7.30/pkg-config:/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig"
declare -x PROFILEREAD="true"
declare -x PWD="/home/shourya01"
declare -x PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages"
declare -x PYTHONUSERBASE="/home/shourya01/.local/polaris/conda/2024-04-29"
declare -x QT_SYSTEM_DIR="/usr/share/desktop-data"
declare -x SHELL="/bin/bash"
declare -x SHLVL="2"
declare -x SLURM_MPI_TYPE="cray_shasta"
declare -x STARSHIP_SESSION_KEY="2509179022193917"
declare -x STARSHIP_SHELL="bash"
declare -x TMPDIR="/var/tmp/pbs.5236102.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x TRITON_DISABLE_AUTOTUNE="1"
declare -x TZ="Etc/UTC"
declare -x USER="shourya01"
declare -x USE_PCM_DB="2"
declare -x WINDOWMANAGER="xterm"
declare -x XALT_DIR="/soft/xalt/3.0.2-202408282050"
declare -x XALT_EXECUTABLE_TRACKING="yes"
declare -x XALT_SAMPLING="no"
declare -x XALT_SCALAR_AND_SPSR_SAMPLING="yes"
declare -x XCURSOR_THEME="DMZ"
declare -x XDG_CONFIG_DIRS="/etc/xdg"
declare -x XDG_DATA_DIRS="/usr/share"
declare -x XKEYSYMDB="/usr/X11R6/lib/X11/XKeysymDB"
declare -x XLA_FLAGS="--xla_gpu_force_compilation_parallelism=1 --xla_gpu_cuda_data_dir=/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x XLA_PYTHON_CLIENT_PREALLOCATE="false"
declare -x XML_CATALOG_FILES="file:///soft/applications/conda/2024-04-29/mconda3/etc/xml/catalog file:///etc/xml/catalog"
declare -x XNLSPATH="/usr/X11R6/lib/X11/nls"
declare -x _CE_CONDA=""
declare -x _CE_M=""
declare -x _LMFILES_="/opt/cray/modulefiles/libfabric/1.15.2.0:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-network-ofi.lua:/opt/cray/pe/lmod/modulefiles/core/perftools-base/23.12.0.lua:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles/darshan/3.4.4:/soft/xalt/modulefiles/xalt/3.0.2-202408282050:/opt/cray/pe/lmod/modulefiles/core/gcc-native/12.3.lua:/opt/cray/pe/lmod/modulefiles/core/craype/2.7.30.lua:/opt/cray/pe/lmod/modulefiles/core/cray-dsmml/0.2.2.lua:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0/cray-mpich/8.1.28.lua:/opt/cray/pe/lmod/modulefiles/core/cray-pmi/6.1.13.lua:/opt/cray/pals/lmod/modulefiles/core/cray-pals/1.3.4.lua:/opt/cray/pals/lmod/modulefiles/core/cray-libpals/1.3.4.lua:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-x86-milan.lua:/opt/cray/pe/lmod/modulefiles/core/PrgEnv-gnu/8.5.0.lua:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2.9.lua:/soft/modulefiles/cudnn/9.1.0.lua:/soft/modulefiles/conda/2024-04-29.lua"
declare -x _ModuleTable001_="X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpQcmdFbnYgPSAiUHJnRW52LWdudSIsCmNvbXBpbGVyID0gImdjYy1uYXRpdmUiLApjcmF5cGUgPSAiY3JheXBlIiwKY3JheXBlX2NwdSA9ICJjcmF5cGUteDg2LW1pbGFuIiwKY3JheXBlX25ldHdvcmsgPSAiY3JheXBlLW5ldHdvcmstb2ZpIiwKZ2NjX2NvbXBpbGVyID0gImdjYy1uYXRpdmUiLApoZGY1ID0gImNyYXktaGRmNS1wYXJhbGxlbCIsCm1waSA9ICJjcmF5LW1waWNoIiwKcHl0aG9uID0gImNvbmRhIiwKfSwKbVQgPSB7ClsiUHJnRW52LWdudSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUv"
declare -x _ModuleTable002_="bG1vZC9tb2R1bGVmaWxlcy9jb3JlL1ByZ0Vudi1nbnUvOC41LjAubHVhIiwKZnVsbE5hbWUgPSAiUHJnRW52LWdudS84LjUuMCIsCmxvYWRPcmRlciA9IDE0LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gIlByZ0Vudi1nbnUiLAp3ViA9ICJeMDAwMDAwMDguMDAwMDAwMDA1Lip6ZmluYWwiLAp9LApjb25kYSA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY29uZGEvMjAyNC0wNC0yOS5sdWEiLApmdWxsTmFtZSA9ICJjb25kYS8yMDI0LTA0LTI5IiwKbG9hZE9yZGVyID0gMTcsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY29uZGEiLAp3ViA9ICJeMDAw"
declare -x _ModuleTable003_="MDIwMjQuKnpmaW5hbC0uMDAwMDAwMDA0Lip6ZmluYWwtLjAwMDAwMDAyOS4qemZpbmFsIiwKfSwKWyJjcmF5LWRzbW1sIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1kc21tbC8wLjIuMi5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWRzbW1sLzAuMi4yIiwKbG9hZE9yZGVyID0gOCwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWRzbW1sIiwKd1YgPSAiXjAwMDAwMDAwLjAwMDAwMDAwMi4wMDAwMDAwMDIuKnpmaW5hbCIsCn0sClsiY3JheS1oZGY1LXBhcmFsbGVsIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9v"
declare -x _ModuleTable004_="ZmkvMS4wL2NyYXktbXBpY2gvOC4wL2NyYXktaGRmNS1wYXJhbGxlbC8xLjEyLjIuOS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLApsb2FkT3JkZXIgPSAxNSwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDEyLjAwMDAwMDAwMi4wMDAwMDAwMDkuKnpmaW5hbCIsCn0sClsiY3JheS1saWJwYWxzIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wYWxzL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5LWxpYnBhbHMvMS4zLjQubHVhIiwKZnVsbE5hbWUgPSAiY3JheS1s"
declare -x _ModuleTable005_="aWJwYWxzLzEuMy40IiwKbG9hZE9yZGVyID0gMTIsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1saWJwYWxzIiwKd1YgPSAiXjAwMDAwMDAxLjAwMDAwMDAwMy4wMDAwMDAwMDQuKnpmaW5hbCIsCn0sClsiY3JheS1tcGljaCJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMC9jcmF5LW1waWNoLzguMS4yOC5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LW1waWNoLzguMS4yOCIsCmxvYWRPcmRlciA9IDksCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1tcGljaCIsCndWID0gIl4w"
declare -x _ModuleTable006_="MDAwMDAwOC4wMDAwMDAwMDEuMDAwMDAwMDI4Lip6ZmluYWwiLAp9LApbImNyYXktcGFscyJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGFscy9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wYWxzLzEuMy40Lmx1YSIsCmZ1bGxOYW1lID0gImNyYXktcGFscy8xLjMuNCIsCmxvYWRPcmRlciA9IDExLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcGFscyIsCndWID0gIl4wMDAwMDAwMS4wMDAwMDAwMDMuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImNyYXktcG1pIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wbWkvNi4xLjEzLmx1YSIsCmZ1bGxOYW1lID0gImNy"
declare -x _ModuleTable007_="YXktcG1pLzYuMS4xMyIsCmxvYWRPcmRlciA9IDEwLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcG1pIiwKd1YgPSAiXjAwMDAwMDA2LjAwMDAwMDAwMS4wMDAwMDAwMTMuKnpmaW5hbCIsCn0sCmNyYXlwZSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5cGUvMi43LjMwLmx1YSIsCmZ1bGxOYW1lID0gImNyYXlwZS8yLjcuMzAiLApsb2FkT3JkZXIgPSA3LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZSIsCndWID0gIl4wMDAwMDAwMi4wMDAwMDAwMDcuMDAwMDAwMDMwLip6ZmluYWwiLAp9LApb"
declare -x _ModuleTable008_="ImNyYXlwZS1uZXR3b3JrLW9maSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS1uZXR3b3JrLW9maS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5cGUtbmV0d29yay1vZmkiLApsb2FkT3JkZXIgPSAyLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS1uZXR3b3JrLW9maSIsCndWID0gIk0uKnpmaW5hbCIsCn0sClsiY3JheXBlLXg4Ni1taWxhbiJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS14ODYtbWlsYW4ubHVhIiwKZnVsbE5hbWUgPSAiY3JheXBl"
declare -x _ModuleTable009_="LXg4Ni1taWxhbiIsCmxvYWRPcmRlciA9IDEzLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS14ODYtbWlsYW4iLAp3ViA9ICJNLip6ZmluYWwiLAp9LApjdWRubiA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY3Vkbm4vOS4xLjAubHVhIiwKZnVsbE5hbWUgPSAiY3Vkbm4vOS4xLjAiLApsb2FkT3JkZXIgPSAxNiwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjdWRubi85LjEuMCIsCndWID0gIjAwMDAwMDAwOS4wMDAwMDAwMDEuKnpmaW5hbCIsCn0sCmRhcnNoYW4gPSB7CmZuID0gIi9zb2Z0L3BlcmZ0b29scy9k"
declare -x _ModuleTable010_="YXJzaGFuL2RhcnNoYW4tMy40LjQvc2hhcmUvY3JheXBlLTIueC9tb2R1bGVmaWxlcy9kYXJzaGFuLzMuNC40IiwKZnVsbE5hbWUgPSAiZGFyc2hhbi8zLjQuNCIsCmxvYWRPcmRlciA9IDQsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiZGFyc2hhbiIsCndWID0gIjAwMDAwMDAwMy4wMDAwMDAwMDQuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImdjYy1uYXRpdmUiXSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9nY2MtbmF0aXZlLzEyLjMubHVhIiwKZnVsbE5hbWUgPSAiZ2NjLW5hdGl2ZS8xMi4zIiwKbG9hZE9yZGVyID0gNiwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0"
declare -x _ModuleTable011_="YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJnY2MtbmF0aXZlIiwKd1YgPSAiXjAwMDAwMDEyLjAwMDAwMDAwMy4qemZpbmFsIiwKfSwKbGliZmFicmljID0gewpmbiA9ICIvb3B0L2NyYXkvbW9kdWxlZmlsZXMvbGliZmFicmljLzEuMTUuMi4wIiwKZnVsbE5hbWUgPSAibGliZmFicmljLzEuMTUuMi4wIiwKbG9hZE9yZGVyID0gMSwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJsaWJmYWJyaWMiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDE1LjAwMDAwMDAwMi4qemZpbmFsIiwKfSwKWyJwZXJmdG9vbHMtYmFzZSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb3JlL3BlcmZ0b29s"
declare -x _ModuleTable012_="cy1iYXNlLzIzLjEyLjAubHVhIiwKZnVsbE5hbWUgPSAicGVyZnRvb2xzLWJhc2UvMjMuMTIuMCIsCmxvYWRPcmRlciA9IDMsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAicGVyZnRvb2xzLWJhc2UiLAp3ViA9ICJeMDAwMDAwMjMuMDAwMDAwMDEyLip6ZmluYWwiLAp9LAp4YWx0ID0gewpmbiA9ICIvc29mdC94YWx0L21vZHVsZWZpbGVzL3hhbHQvMy4wLjItMjAyNDA4MjgyMDUwIiwKZnVsbE5hbWUgPSAieGFsdC8zLjAuMi0yMDI0MDgyODIwNTAiLApsb2FkT3JkZXIgPSA1LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gInhhbHQiLAp3ViA9ICJeMDAwMDAw"
declare -x _ModuleTable013_="MDMuMDAwMDAwMDAwLjAwMDAwMDAwMi4qemZpbmFsLS4yMDI0MDgyODIwNTAuKnpmaW5hbCIsCn0sCn0sCm1wYXRoQSA9IHsKCiIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9oZGY1LXBhcmFsbGVsL2dudS8xMi4wL29maS8xLjAvY3JheS1tcGljaC84LjAvY3JheS1oZGY1LXBhcmFsbGVsLzEuMTIuMiIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3B1L3g4Ni1taWxhbi8xLjAiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9vZmkvMS4wL2NyYXktbXBpY2gvOC4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMCIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxl"
declare -x _ModuleTable014_="ZmlsZXMvbWl4X2NvbXBpbGVycyIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29tcGlsZXIvZ251LzEyLjAiLCAiL3NvZnQvbW9kdWxlZmlsZXMiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL3BlcmZ0b29scy8yMy4xMi4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9uZXQvb2ZpLzEuMCIsICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0xpbnV4IgosICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmUiLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZSIKLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BhbHMvbG1vZC9tb2R1bGVmaWxlcy9jb3JlIgosICIvb3B0L2Ny"
declare -x _ModuleTable015_="YXkvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZSIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdCIKLCAiL3NvZnQvcGVyZnRvb2xzL2RhcnNoYW4vZGFyc2hhbi0zLjQuNC9zaGFyZS9jcmF5cGUtMi54L21vZHVsZWZpbGVzIiwgIi9zb2Z0L3hhbHQvbW9kdWxlZmlsZXMiLAp9LApzeXN0ZW1CYXNlTVBBVEggPSAiL3Vzci9zaGFyZS9tb2R1bGVmaWxlcy9MaW51eDovdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmU6L3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZTovdXNyL3NoYXJlL2xtb2QvbG1vZC9tb2R1bGVmaWxlczovb3B0L2NyYXkvcGFscy9sbW9kL21vZHVs"
declare -x _ModuleTable016_="ZWZpbGVzL2NvcmU6L29wdC9jcmF5L21vZHVsZWZpbGVzOi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmU6L29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdDovc29mdC9wZXJmdG9vbHMvZGFyc2hhbi9kYXJzaGFuLTMuNC40L3NoYXJlL2NyYXlwZS0yLngvbW9kdWxlZmlsZXM6L3NvZnQveGFsdC9tb2R1bGVmaWxlcyIsCn0K"
declare -x _ModuleTable_Sz_="16"
declare -x __LMOD_Priority_PATH="/soft/xalt/3.0.2-202408282050/bin:-100"
declare -x __LMOD_REF_COUNT_COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin:1"
declare -x __LMOD_REF_COUNT_CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:1;/opt/cray/pe/pmi/6.1.13/lib:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:1;/opt/cray/pe/mpich/8.1.28/gtl/lib:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib:1;/opt/cray/pe/perftools/23.12.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:1;/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:1;/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:1;/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:1;/soft/perftools/darshan/darshan-3.4.4/lib:1;/opt/cray/pe/papi/7.0.1.2/lib64:1;/opt/cray/libfabric/1.15.2.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so:1"
declare -x __LMOD_REF_COUNT_MANPATH="/opt/cray/pals/1.3.4/man:2;/opt/cray/pe/pmi/6.1.13/man:1;/opt/cray/pe/mpich/8.1.28/ofi/man:1;/opt/cray/pe/mpich/8.1.28/man/mpich:1;/opt/cray/pe/dsmml/0.2.2/dsmml/man:1;/opt/cray/pe/craype/2.7.30/man:1;/opt/cray/pe/perftools/23.12.0/man:1;/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:1;/opt/cray/libfabric/1.15.2.0/share/man:1;/usr/share/lmod/lmod/share/man:1;/home/shourya01/.local/man:1;/usr/local/man:1;/usr/share/man:1;/usr/man:1;/opt/c3/man:1;/opt/pbs/share/man:1;/opt/clmgr/man:1;/opt/sgi/share/man:1;/opt/clmgr/share/man:1;/opt/clmgr/lib/cm-cli/man:1"
declare -x __LMOD_REF_COUNT_MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:1;/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:1;/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:1;/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:1;/opt/cray/pe/lmod/modulefiles/mix_compilers:1;/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:1;/soft/modulefiles:1;/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:1;/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:1;/usr/share/modulefiles/Linux:1;/usr/share/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles:1;/opt/cray/pals/lmod/modulefiles/core:1;/opt/cray/modulefiles:1;/opt/cray/pe/lmod/modulefiles/core:1;/opt/cray/pe/lmod/modulefiles/craype-targets/default:1;/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:1;/soft/xalt/modulefiles:1"
declare -x __LMOD_REF_COUNT_PATH="/soft/xalt/3.0.2-202408282050/bin:1;/soft/compilers/cudatoolkit/cuda-12.4.1/bin:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:1;/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:1;/opt/cray/pe/hdf5/1.12.2.9/bin:1;/opt/cray/pals/1.3.4/bin:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:1;/opt/cray/pe/mpich/8.1.28/bin:1;/opt/cray/pe/craype/2.7.30/bin:1;/home/shourya01/.local/bin:4;/soft/perftools/darshan/darshan-3.4.4/bin:1;/opt/cray/pe/perftools/23.12.0/bin:1;/opt/cray/pe/papi/7.0.1.2/bin:1;/opt/cray/libfabric/1.15.2.0/bin:1;/opt/clmgr/sbin:1;/opt/clmgr/bin:1;/opt/sgi/sbin:1;/opt/sgi/bin:1;/usr/local/bin:1;/usr/bin:1;/bin:2;/opt/c3/bin:1;/usr/lib/mit/bin:1;/usr/lib/mit/sbin:1;/opt/pbs/bin:1;/sbin:1;/home/shourya01/bin:1;/opt/cray/pe/bin:1"
declare -x __LMOD_REF_COUNT_PE_DSMML_PKGCONFIG_LIBS="dsmml:1"
declare -x __LMOD_REF_COUNT_PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:1;hdf5_fortran_parallel:1;mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FIXED_PRGENV="GNU:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_GENCOMPILERS_GNU="12.3:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_PKGCONFIG_LIBS="mpich:1"
declare -x __LMOD_REF_COUNT_PE_PALS_PKGCONFIG_LIBS="libpals:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_LIBS="hdf5_hl_parallel:1;hdf5_parallel:1;mpich:1;dsmml:1;darshan-runtime:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_PRODUCTS="PE_PALS:1;PE_PMI:1;PE_MPICH:1;PE_DSMML:1"
declare -x __LMOD_REF_COUNT_PE_PMI_PKGCONFIG_LIBS="cray-pmi:1"
declare -x __LMOD_REF_COUNT_PE_PRODUCT_LIST="CRAYPE_X86_MILAN:1;PERFTOOLS:1;CRAYPAT:1"
declare -x __LMOD_REF_COUNT_PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:1;/opt/cray/pals/1.3.4/lib/pkgconfig:1;/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:1;/opt/cray/pe/craype/2.7.30/pkg-config:1;/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:1;/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages:1"
declare -x ftp_proxy="http://proxy.alcf.anl.gov:3128"
declare -x http_proxy="http://proxy.alcf.anl.gov:3128"
declare -x https_proxy="http://proxy.alcf.anl.gov:3128"
declare -x no_proxy="admin,polaris-adminvm-01,localhost,*.cm.polaris.alcf.anl.gov,polaris-*,*.polaris.alcf.anl.gov,*.alcf.anl.gov"
Running on 5 nodes
Total number of GPUs: 20
Connected to tcp://x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov:7919
Found executable /soft/applications/conda/2024-04-29/mconda3/bin/python
Launching application 6666606a-7407-4a66-ae53-993478a18a8d
Using PMI port 59892,59893
[2025-06-23 12:35:27,525] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:35:27,525] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:35:27,525] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:35:27,525] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:35:27,551] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:35:27,551] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:35:27,551] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:35:27,551] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:35:27,579] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:35:27,579] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:35:27,579] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:35:27,579] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:35:27,795] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:35:27,795] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:35:27,795] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:35:27,795] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:31,496] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:31,496] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:35:31,496] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:31,496] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:35:31,496] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 12:35:31,496] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:31,497] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:31,497] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:35:31,497] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:31,496] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:35:31,496] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:31,497] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:35:31,497] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:31,497] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:35:31,497] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 12:35:31,497] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:31,674] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:31,674] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:35:31,674] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:31,674] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:35:31,674] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:31,674] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:35:31,674] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 12:35:31,674] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:31,905] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:31,905] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:35:31,905] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:31,905] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:35:31,905] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:31,905] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:35:31,905] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 12:35:31,905] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 12:35:34,545] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:35:34,545] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:35:34,545] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:35:34,545] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:43,959] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:43,959] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:35:43,959] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:43,959] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:35:43,959] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:35:43,959] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:35:43,959] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 12:35:43,959] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=8, local_rank=0, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:35:43,971] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=0, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=12, local_rank=0, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=9, local_rank=1, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=16, local_rank=0, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=10, local_rank=2, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=17, local_rank=1, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=11, local_rank=3, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=1, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=18, local_rank=2, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=13, local_rank=1, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=2, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=19, local_rank=3, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=14, local_rank=2, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=3, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:35:43,971] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=15, local_rank=3, world_size=20, master_addr=10.140.57.88, master_port=29500
Initialized deepspeed on global rank 6, local rank 2 with world size 20.
[rank6]: Traceback (most recent call last):
[rank6]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank6]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank6]:                                 ^^^^^^^^^^^^^^^^^
[rank6]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank6]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank6]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank6]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank6]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
Initialized deepspeed on global rank 4, local rank 0 with world size 20.
[rank4]: Traceback (most recent call last):
[rank4]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank4]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank4]:                                 ^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank4]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank4]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank4]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank4]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
Initialized deepspeed on global rank 5, local rank 1 with world size 20.
[rank5]: Traceback (most recent call last):
[rank5]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank5]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank5]:                                 ^^^^^^^^^^^^^^^^^
[rank5]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank5]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank5]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank5]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank5]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
Initialized deepspeed on global rank 7, local rank 3 with world size 20.
[rank7]: Traceback (most recent call last):
[rank7]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank7]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank7]:                                 ^^^^^^^^^^^^^^^^^
[rank7]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank7]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank7]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank7]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank7]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
Initialized deepspeed on global rank 8, local rank 0 with world size 20.
Initialized deepspeed on global rank 9, local rank 1 with world size 20.
[rank8]: Traceback (most recent call last):
[rank8]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank8]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank8]:                                 ^^^^^^^^^^^^^^^^^
[rank8]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank8]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank8]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank8]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank8]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank8]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
[rank9]: Traceback (most recent call last):
[rank9]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank9]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank9]:                                 ^^^^^^^^^^^^^^^^^
[rank9]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank9]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank9]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank9]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank9]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank9]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
Initialized deepspeed on global rank 11, local rank 3 with world size 20.
[rank11]: Traceback (most recent call last):
[rank11]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank11]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank11]:                                 ^^^^^^^^^^^^^^^^^
[rank11]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank11]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank11]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank11]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank11]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank11]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
Initialized deepspeed on global rank 10, local rank 2 with world size 20.
[rank10]: Traceback (most recent call last):
[rank10]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank10]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank10]:                                 ^^^^^^^^^^^^^^^^^
[rank10]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank10]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank10]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank10]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank10]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank10]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
Initialized deepspeed on global rank 16, local rank 0 with world size 20.
Initialized deepspeed on global rank 17, local rank 1 with world size 20.
[rank16]: Traceback (most recent call last):
[rank16]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank16]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank16]:                                 ^^^^^^^^^^^^^^^^^
[rank16]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank16]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank16]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank16]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank16]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank16]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
[rank17]: Traceback (most recent call last):
[rank17]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank17]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank17]:                                 ^^^^^^^^^^^^^^^^^
[rank17]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank17]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank17]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank17]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank17]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank17]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
Initialized deepspeed on global rank 18, local rank 2 with world size 20.
Initialized deepspeed on global rank 19, local rank 3 with world size 20.
[rank18]: Traceback (most recent call last):
[rank18]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank18]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank18]:                                 ^^^^^^^^^^^^^^^^^
[rank18]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank18]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank18]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank18]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank18]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank18]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
[rank19]: Traceback (most recent call last):
[rank19]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank19]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank19]:                                 ^^^^^^^^^^^^^^^^^
[rank19]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank19]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank19]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank19]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank19]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank19]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
Initialized deepspeed on global rank 3, local rank 3 with world size 20.
Initialized deepspeed on global rank 12, local rank 0 with world size 20.
Initialized deepspeed on global rank 13, local rank 1 with world size 20.
[rank12]: Traceback (most recent call last):
[rank12]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank12]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank12]:                                 ^^^^^^^^^^^^^^^^^
[rank12]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank12]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank12]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank12]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank12]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank12]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
[rank13]: Traceback (most recent call last):
[rank13]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank13]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank13]:                                 ^^^^^^^^^^^^^^^^^
[rank13]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank13]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank13]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank13]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank13]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank13]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank3]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank3]:                                 ^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank3]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank3]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank3]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank3]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
Initialized deepspeed on global rank 14, local rank 2 with world size 20.
[rank14]: Traceback (most recent call last):
[rank14]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank14]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank14]:                                 ^^^^^^^^^^^^^^^^^
[rank14]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank14]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank14]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank14]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank14]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank14]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
Initialized deepspeed on global rank 0, local rank 0 with world size 20.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank0]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank0]:                                 ^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank0]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank0]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank0]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank0]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
Initialized deepspeed on global rank 15, local rank 3 with world size 20.
[rank15]: Traceback (most recent call last):
[rank15]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank15]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank15]:                                 ^^^^^^^^^^^^^^^^^
[rank15]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank15]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank15]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank15]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank15]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank15]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
Initialized deepspeed on global rank 2, local rank 2 with world size 20.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank2]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank2]:                                 ^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank2]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank2]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank2]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank2]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
Initialized deepspeed on global rank 1, local rank 1 with world size 20.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/shourya01/stormer_deepspeed/train.py", line 196, in <module>
[rank1]:     train_ds, val_ds, test_ds = get_dataset(args)
[rank1]:                                 ^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/shourya01/stormer_deepspeed/train.py", line 94, in get_dataset
[rank1]:     energy_ds = netCDF4.Dataset(str(Path(args.energy_nc_base) / Path(args.dataset_name) / Path(args.dataset_name+".nc")), mode="r")
[rank1]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "src/netCDF4/_netCDF4.pyx", line 2469, in netCDF4._netCDF4.Dataset.__init__
[rank1]:   File "src/netCDF4/_netCDF4.pyx", line 2028, in netCDF4._netCDF4._ensure_nc_success
[rank1]: FileNotFoundError: [Errno 2] No such file or directory: '/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/california/california.nc'
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 16 exited with code 1
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 4 exited with code 1
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 12 exited with code 1
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 8 exited with code 1
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 11 exited with code 1
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 0 died from signal 15
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 6 exited with code 1
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 5 exited with code 1
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 18 exited with code 1
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 13 exited with code 1
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 9 exited with code 1
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 14 exited with code 1
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 7 exited with code 1
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 17 exited with code 1
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 19 exited with code 1
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 15 exited with code 1
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 10 exited with code 1
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 2 died from signal 15
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 1 died from signal 15
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 3 died from signal 15
Application 6666606a resources: utime=180s stime=202s maxrss=2731912KB inblock=9724222 oublock=336 minflt=4476273 majflt=28840 nvcsw=621781 nivcsw=11805
Training completed
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled

Lmod is automatically replacing "nvhpc/23.9" with "gcc-native/12.3".


Lmod is automatically replacing "PrgEnv-nvhpc/8.5.0" with "PrgEnv-gnu/8.5.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.28

declare -x APP2_STATE="23.12.0"
declare -x BASH_ENV="/usr/share/lmod/lmod/init/bash"
declare -x C3_RSH="ssh -oConnectTimeout=10 -oForwardX11=no"
declare -x CFLAGS="-I/soft/applications/conda/2024-04-29/mconda3/include"
declare -x COLORTERM="1"
declare -x COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin"
declare -x CONDA_DEFAULT_ENV="base"
declare -x CONDA_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/conda"
declare -x CONDA_PREFIX="/soft/applications/conda/2024-04-29/mconda3"
declare -x CONDA_PROMPT_MODIFIER="(2024-04-29/base) "
declare -x CONDA_PYTHON_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/python"
declare -x CONDA_SHLVL="1"
declare -x CPU="x86_64"
declare -x CRAYPAT_LD_LIBRARY_PATH="/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAYPAT_OPTS_EXECUTABLE="libexec64/opts"
declare -x CRAYPAT_ROOT="/opt/cray/pe/perftools/23.12.0"
declare -x CRAYPE_DIR="/opt/cray/pe/craype/2.7.30"
declare -x CRAYPE_NETWORK_TARGET="ofi"
declare -x CRAYPE_VERSION="2.7.30"
declare -x CRAY_CPU_TARGET="x86-milan"
declare -x CRAY_DSMML_BASEDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_DIR="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_PREFIX="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_ROOTDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_VER="0.2.2"
declare -x CRAY_DSMML_VERSION="0.2.2"
declare -x CRAY_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x CRAY_HDF5_PARALLEL_PREFIX="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x CRAY_HDF5_PARALLEL_VERSION="1.12.2.9"
declare -x CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:/opt/cray/pe/pmi/6.1.13/lib:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:/opt/cray/pe/mpich/8.1.28/gtl/lib:/opt/cray/pe/dsmml/0.2.2/dsmml/lib:/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAY_LMOD_COMPILER="gnu/12.0"
declare -x CRAY_LMOD_CPU="x86-milan/1.0"
declare -x CRAY_LMOD_MPI="cray-mpich/8.0"
declare -x CRAY_LMOD_NET="ofi/1.0"
declare -x CRAY_MPICH_BASEDIR="/opt/cray/pe/mpich/8.1.28/ofi"
declare -x CRAY_MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_PREFIX="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_ROOTDIR="/opt/cray/pe/mpich/8.1.28"
declare -x CRAY_MPICH_VER="8.1.28"
declare -x CRAY_MPICH_VERSION="8.1.28"
declare -x CRAY_PERFTOOLS_PREFIX="/opt/cray/pe/perftools/23.12.0"
declare -x CRAY_PERFTOOLS_VERSION="23.12.0"
declare -x CRAY_PMI_INCLUDE_OPTS="-I/opt/cray/pe/pmi/6.1.13/include"
declare -x CRAY_PMI_POST_LINK_OPTS="-L/opt/cray/pe/pmi/6.1.13/lib"
declare -x CRAY_PMI_PREFIX="/opt/cray/pe/pmi/6.1.13"
declare -x CRAY_PMI_VERSION="6.1.13"
declare -x CSHEDIT="emacs"
declare -x CUDA_HOME="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_TOOLKIT_BASE="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDNN_HOME="/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/"
declare -x ENVIRONMENT="BATCH"
declare -x ENV_NAME="conda/2024-04-29"
declare -x FROM_HEADER=""
declare -x GCC_PATH="/usr/bin"
declare -x GCC_PREFIX="/usr/lib64/gcc/x86_64-suse-linux/12"
declare -x GCC_VERSION="12.3"
declare -x GNU_VERSION="12.3"
declare -x GPG_TTY="not a tty"
declare -x GSETTINGS_SCHEMA_DIR="/soft/applications/conda/2024-04-29/mconda3/share/glib-2.0/schemas"
declare -x GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=""
declare -x G_BROKEN_FILENAMES="1"
declare -x G_FILENAME_ENCODING="@locale,UTF-8,ISO-8859-15,CP1252"
declare -x HDF5_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HDF5_ROOT="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HISTSIZE="1000"
declare -x HOME="/home/shourya01"
declare -x HOST="x3005c0s37b0n0"
declare -x HOSTNAME="x3005c0s37b0n0"
declare -x HOSTTYPE="x86_64"
declare -x HTTPS_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x HTTP_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x LANG="en_US.UTF-8"
declare -x LANGUAGE="en_US.UTF-8"
declare -x LDFLAGS="-L/soft/applications/conda/2024-04-29/mconda3/lib -Wl,--enable-new-dtags,-rpath,/soft/applications/conda/2024-04-29/mconda3/lib"
declare -x LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:/soft/perftools/darshan/darshan-3.4.4/lib:/opt/cray/pe/papi/7.0.1.2/lib64:/opt/cray/libfabric/1.15.2.0/lib64"
declare -x LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so"
declare -x LESS="-M -I -R"
declare -x LESSCLOSE="lessclose.sh %s %s"
declare -x LESSKEY="/etc/lesskey.bin"
declare -x LESSOPEN="lessopen.sh %s"
declare -x LESS_ADVANCED_PREPROCESSOR="no"
declare -x LMOD_CMD="/usr/share/lmod/lmod/libexec/lmod"
declare -x LMOD_DIR="/usr/share/lmod/lmod/libexec"
declare -x LMOD_FAMILY_COMPILER="gcc-native"
declare -x LMOD_FAMILY_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_CRAYPE="craype"
declare -x LMOD_FAMILY_CRAYPE_CPU="craype-x86-milan"
declare -x LMOD_FAMILY_CRAYPE_CPU_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_NETWORK="craype-network-ofi"
declare -x LMOD_FAMILY_CRAYPE_NETWORK_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_VERSION="2.7.30"
declare -x LMOD_FAMILY_GCC_COMPILER="gcc-native"
declare -x LMOD_FAMILY_GCC_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_HDF5="cray-hdf5-parallel"
declare -x LMOD_FAMILY_HDF5_VERSION="1.12.2.9"
declare -x LMOD_FAMILY_MPI="cray-mpich"
declare -x LMOD_FAMILY_MPI_VERSION="8.1.28"
declare -x LMOD_FAMILY_PRGENV="PrgEnv-gnu"
declare -x LMOD_FAMILY_PRGENV_VERSION="8.5.0"
declare -x LMOD_FAMILY_PYTHON="conda"
declare -x LMOD_FAMILY_PYTHON_VERSION="2024-04-29"
declare -x LMOD_PKG="/usr/share/lmod/lmod"
declare -x LMOD_ROOT="/usr/share/lmod"
declare -x LMOD_SETTARG_FULL_SUPPORT="no"
declare -x LMOD_SYSTEM_DEFAULT_MODULES="PrgEnv-nvhpc:craype-network-ofi:perftools-base:darshan:xalt"
declare -x LMOD_VERSION="8.7.34"
declare -x LMOD_sys="Linux"
declare -x LOADEDMODULES="libfabric/1.15.2.0:craype-network-ofi:perftools-base/23.12.0:darshan/3.4.4:xalt/3.0.2-202408282050:gcc-native/12.3:craype/2.7.30:cray-dsmml/0.2.2:cray-mpich/8.1.28:cray-pmi/6.1.13:cray-pals/1.3.4:cray-libpals/1.3.4:craype-x86-milan:PrgEnv-gnu/8.5.0:cray-hdf5-parallel/1.12.2.9:cudnn/9.1.0:conda/2024-04-29"
declare -x LOGNAME="shourya01"
declare -x MACHTYPE="x86_64-suse-linux"
declare -x MAIL="/var/spool/mail/shourya01"
declare -x MANPATH="/opt/cray/pals/1.3.4/man:/opt/cray/pe/pmi/6.1.13/man:/opt/cray/pe/mpich/8.1.28/ofi/man:/opt/cray/pe/mpich/8.1.28/man/mpich:/opt/cray/pe/dsmml/0.2.2/dsmml/man:/opt/cray/pe/craype/2.7.30/man:/opt/cray/pe/perftools/23.12.0/man:/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:/opt/cray/libfabric/1.15.2.0/share/man:/usr/share/lmod/lmod/share/man:/home/shourya01/.local/man:/usr/local/man:/usr/share/man:/usr/man:/opt/c3/man:/opt/pbs/share/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man"
declare -x MINICOM="-c on"
declare -x MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:/opt/cray/pe/lmod/modulefiles/mix_compilers:/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:/soft/modulefiles:/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:/usr/share/modulefiles/Linux:/usr/share/modulefiles/Core:/usr/share/lmod/lmod/modulefiles/Core:/usr/share/lmod/lmod/modulefiles:/opt/cray/pals/lmod/modulefiles/core:/opt/cray/modulefiles:/opt/cray/pe/lmod/modulefiles/core:/opt/cray/pe/lmod/modulefiles/craype-targets/default:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:/soft/xalt/modulefiles"
declare -x MODULEPATH_ROOT="/usr/share/modulefiles"
declare -x MODULESHOME="/usr/share/lmod/lmod"
declare -x MORE="-sl"
declare -x MPI4JAX_USE_CUDA_MPI="1"
declare -x MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x MPICH_GPU_SUPPORT_ENABLED="1"
declare -x NCCL_IB_DISABLE="1"
declare -x NCCL_SOCKET_IFNAME="hsn"
declare -x NCPUS="64"
declare -x OFFLOAD_INIT="on_start"
declare -x OLDPWD
declare -x OMP_NUM_THREADS="4"
declare -x OSCAR_HOME="/opt/oscar"
declare -x OSTYPE="linux"
declare -x PAGER="less"
declare -x PALS_TRANSFER="0"
declare -x PATH="/soft/applications/conda/2024-04-29/mconda3/bin:/soft/applications/conda/2024-04-29/mconda3/condabin:/soft/xalt/3.0.2-202408282050/bin:/soft/compilers/cudatoolkit/cuda-12.4.1/bin:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:/opt/cray/pe/hdf5/1.12.2.9/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/home/shourya01/.local/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/shourya01/bin:/opt/cray/pe/bin"
declare -x PAT_RT_PERFCTR_DISABLE_COMPONENTS="nvml,rocm_smi"
declare -x PBS_ACCOUNT="ParaLLMs"
declare -x PBS_ENVIRONMENT="PBS_BATCH"
declare -x PBS_JOBCOOKIE="4C72346B72B071224F00859C62A618DE"
declare -x PBS_JOBDIR="/home/shourya01"
declare -x PBS_JOBID="5236104.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_JOBNAME="bash"
declare -x PBS_MOMPORT="15003"
declare -x PBS_NODEFILE="/var/spool/pbs/aux/5236104.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_NODENUM="0"
declare -x PBS_O_HOME="/home/shourya01"
declare -x PBS_O_HOST="polaris-login-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_O_INTERACTIVE_AUTH_METHOD="resvport"
declare -x PBS_O_LANG="en_US.UTF-8"
declare -x PBS_O_LOGNAME="shourya01"
declare -x PBS_O_MAIL="/var/spool/mail/shourya01"
declare -x PBS_O_PATH="/home/shourya01/.local/bin:/home/shourya01/.vscode-server/cli/servers/Stable-dfaf44141ea9deb3b4096f7cd6d24e00c147a4b1/server/bin/remote-cli:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/soft/xalt/3.0.2-202408282050/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/nvidia/23.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/extras/qd/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/shourya01/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/dbhome/db2cat/sqllib/bin:/dbhome/db2cat/sqllib/adm:/dbhome/db2cat/sqllib/misc:/dbhome/db2cat/sqllib/gskit/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/opt/cray/pe/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.vscode-server/extensions/ms-python.debugpy-2025.8.0/bundled/scripts/noConfigScripts"
declare -x PBS_O_QUEUE="debug-scaling"
declare -x PBS_O_SHELL="/bin/bash"
declare -x PBS_O_SYSTEM="Linux"
declare -x PBS_O_WORKDIR="/home/shourya01"
declare -x PBS_QUEUE="debug-scaling"
declare -x PBS_TASKNUM="1"
declare -x PELOCAL_PRGENV="true"
declare -x PERFTOOLS_VERSION="23.12.0"
declare -x PE_DSMML_MODULE_NAME="cray-dsmml"
declare -x PE_DSMML_PKGCONFIG_LIBS="dsmml"
declare -x PE_ENV="GNU"
declare -x PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel:mpichf90"
declare -x PE_GCC_EXTERNAL="native"
declare -x PE_GCC_LEVEL="12"
declare -x PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig"
declare -x PE_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x PE_HDF5_PARALLEL_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel"
declare -x PE_HDF5_PARALLEL_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel"
declare -x PE_MPICH_FIXED_PRGENV="GNU"
declare -x PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90"
declare -x PE_MPICH_GENCOMPILERS_GNU="12.3"
declare -x PE_MPICH_GTL_DIR_amd_gfx906="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx908="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx90a="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx940="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx942="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia70="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia80="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia90="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_ponteVecchio="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_LIBS_amd_gfx906="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx908="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx90a="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx940="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx942="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_nvidia70="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia80="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia90="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_ponteVecchio="-lmpi_gtl_ze"
declare -x PE_MPICH_MODULE_NAME="cray-mpich"
declare -x PE_MPICH_PKGCONFIG_LIBS="mpich"
declare -x PE_MPICH_PKGCONFIG_VARIABLES="PE_MPICH_GTL_DIR_@accelerator@:PE_MPICH_GTL_LIBS_@accelerator@"
declare -x PE_PALS_PKGCONFIG_LIBS="libpals"
declare -x PE_PERFTOOLS_MPICH_LIBDIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib"
declare -x PE_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel:mpich:dsmml:darshan-runtime"
declare -x PE_PKGCONFIG_PRODUCTS="PE_PALS:PE_PMI:PE_MPICH:PE_DSMML"
declare -x PE_PMI_PKGCONFIG_LIBS="cray-pmi"
declare -x PE_PRODUCT_LIST="CRAYPE_X86_MILAN"
declare -x PKGCONFIG_ENABLED="1"
declare -x PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:/opt/cray/pals/1.3.4/lib/pkgconfig:/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:/opt/cray/pe/craype/2.7.30/pkg-config:/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig"
declare -x PROFILEREAD="true"
declare -x PWD="/home/shourya01"
declare -x PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages"
declare -x PYTHONUSERBASE="/home/shourya01/.local/polaris/conda/2024-04-29"
declare -x QT_SYSTEM_DIR="/usr/share/desktop-data"
declare -x SHELL="/bin/bash"
declare -x SHLVL="2"
declare -x SLURM_MPI_TYPE="cray_shasta"
declare -x STARSHIP_SESSION_KEY="1929224217429218"
declare -x STARSHIP_SHELL="bash"
declare -x TMPDIR="/var/tmp/pbs.5236104.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x TRITON_DISABLE_AUTOTUNE="1"
declare -x TZ="Etc/UTC"
declare -x USER="shourya01"
declare -x USE_PCM_DB="2"
declare -x WINDOWMANAGER="xterm"
declare -x XALT_DIR="/soft/xalt/3.0.2-202408282050"
declare -x XALT_EXECUTABLE_TRACKING="yes"
declare -x XALT_SAMPLING="no"
declare -x XALT_SCALAR_AND_SPSR_SAMPLING="yes"
declare -x XCURSOR_THEME="DMZ"
declare -x XDG_CONFIG_DIRS="/etc/xdg"
declare -x XDG_DATA_DIRS="/usr/share"
declare -x XKEYSYMDB="/usr/X11R6/lib/X11/XKeysymDB"
declare -x XLA_FLAGS="--xla_gpu_force_compilation_parallelism=1 --xla_gpu_cuda_data_dir=/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x XLA_PYTHON_CLIENT_PREALLOCATE="false"
declare -x XML_CATALOG_FILES="file:///soft/applications/conda/2024-04-29/mconda3/etc/xml/catalog file:///etc/xml/catalog"
declare -x XNLSPATH="/usr/X11R6/lib/X11/nls"
declare -x _CE_CONDA=""
declare -x _CE_M=""
declare -x _LMFILES_="/opt/cray/modulefiles/libfabric/1.15.2.0:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-network-ofi.lua:/opt/cray/pe/lmod/modulefiles/core/perftools-base/23.12.0.lua:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles/darshan/3.4.4:/soft/xalt/modulefiles/xalt/3.0.2-202408282050:/opt/cray/pe/lmod/modulefiles/core/gcc-native/12.3.lua:/opt/cray/pe/lmod/modulefiles/core/craype/2.7.30.lua:/opt/cray/pe/lmod/modulefiles/core/cray-dsmml/0.2.2.lua:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0/cray-mpich/8.1.28.lua:/opt/cray/pe/lmod/modulefiles/core/cray-pmi/6.1.13.lua:/opt/cray/pals/lmod/modulefiles/core/cray-pals/1.3.4.lua:/opt/cray/pals/lmod/modulefiles/core/cray-libpals/1.3.4.lua:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-x86-milan.lua:/opt/cray/pe/lmod/modulefiles/core/PrgEnv-gnu/8.5.0.lua:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2.9.lua:/soft/modulefiles/cudnn/9.1.0.lua:/soft/modulefiles/conda/2024-04-29.lua"
declare -x _ModuleTable001_="X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpQcmdFbnYgPSAiUHJnRW52LWdudSIsCmNvbXBpbGVyID0gImdjYy1uYXRpdmUiLApjcmF5cGUgPSAiY3JheXBlIiwKY3JheXBlX2NwdSA9ICJjcmF5cGUteDg2LW1pbGFuIiwKY3JheXBlX25ldHdvcmsgPSAiY3JheXBlLW5ldHdvcmstb2ZpIiwKZ2NjX2NvbXBpbGVyID0gImdjYy1uYXRpdmUiLApoZGY1ID0gImNyYXktaGRmNS1wYXJhbGxlbCIsCm1waSA9ICJjcmF5LW1waWNoIiwKcHl0aG9uID0gImNvbmRhIiwKfSwKbVQgPSB7ClsiUHJnRW52LWdudSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUv"
declare -x _ModuleTable002_="bG1vZC9tb2R1bGVmaWxlcy9jb3JlL1ByZ0Vudi1nbnUvOC41LjAubHVhIiwKZnVsbE5hbWUgPSAiUHJnRW52LWdudS84LjUuMCIsCmxvYWRPcmRlciA9IDE0LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gIlByZ0Vudi1nbnUiLAp3ViA9ICJeMDAwMDAwMDguMDAwMDAwMDA1Lip6ZmluYWwiLAp9LApjb25kYSA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY29uZGEvMjAyNC0wNC0yOS5sdWEiLApmdWxsTmFtZSA9ICJjb25kYS8yMDI0LTA0LTI5IiwKbG9hZE9yZGVyID0gMTcsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY29uZGEiLAp3ViA9ICJeMDAw"
declare -x _ModuleTable003_="MDIwMjQuKnpmaW5hbC0uMDAwMDAwMDA0Lip6ZmluYWwtLjAwMDAwMDAyOS4qemZpbmFsIiwKfSwKWyJjcmF5LWRzbW1sIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1kc21tbC8wLjIuMi5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWRzbW1sLzAuMi4yIiwKbG9hZE9yZGVyID0gOCwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWRzbW1sIiwKd1YgPSAiXjAwMDAwMDAwLjAwMDAwMDAwMi4wMDAwMDAwMDIuKnpmaW5hbCIsCn0sClsiY3JheS1oZGY1LXBhcmFsbGVsIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9v"
declare -x _ModuleTable004_="ZmkvMS4wL2NyYXktbXBpY2gvOC4wL2NyYXktaGRmNS1wYXJhbGxlbC8xLjEyLjIuOS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLApsb2FkT3JkZXIgPSAxNSwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDEyLjAwMDAwMDAwMi4wMDAwMDAwMDkuKnpmaW5hbCIsCn0sClsiY3JheS1saWJwYWxzIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wYWxzL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5LWxpYnBhbHMvMS4zLjQubHVhIiwKZnVsbE5hbWUgPSAiY3JheS1s"
declare -x _ModuleTable005_="aWJwYWxzLzEuMy40IiwKbG9hZE9yZGVyID0gMTIsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1saWJwYWxzIiwKd1YgPSAiXjAwMDAwMDAxLjAwMDAwMDAwMy4wMDAwMDAwMDQuKnpmaW5hbCIsCn0sClsiY3JheS1tcGljaCJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMC9jcmF5LW1waWNoLzguMS4yOC5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LW1waWNoLzguMS4yOCIsCmxvYWRPcmRlciA9IDksCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1tcGljaCIsCndWID0gIl4w"
declare -x _ModuleTable006_="MDAwMDAwOC4wMDAwMDAwMDEuMDAwMDAwMDI4Lip6ZmluYWwiLAp9LApbImNyYXktcGFscyJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGFscy9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wYWxzLzEuMy40Lmx1YSIsCmZ1bGxOYW1lID0gImNyYXktcGFscy8xLjMuNCIsCmxvYWRPcmRlciA9IDExLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcGFscyIsCndWID0gIl4wMDAwMDAwMS4wMDAwMDAwMDMuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImNyYXktcG1pIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wbWkvNi4xLjEzLmx1YSIsCmZ1bGxOYW1lID0gImNy"
declare -x _ModuleTable007_="YXktcG1pLzYuMS4xMyIsCmxvYWRPcmRlciA9IDEwLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcG1pIiwKd1YgPSAiXjAwMDAwMDA2LjAwMDAwMDAwMS4wMDAwMDAwMTMuKnpmaW5hbCIsCn0sCmNyYXlwZSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5cGUvMi43LjMwLmx1YSIsCmZ1bGxOYW1lID0gImNyYXlwZS8yLjcuMzAiLApsb2FkT3JkZXIgPSA3LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZSIsCndWID0gIl4wMDAwMDAwMi4wMDAwMDAwMDcuMDAwMDAwMDMwLip6ZmluYWwiLAp9LApb"
declare -x _ModuleTable008_="ImNyYXlwZS1uZXR3b3JrLW9maSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS1uZXR3b3JrLW9maS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5cGUtbmV0d29yay1vZmkiLApsb2FkT3JkZXIgPSAyLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS1uZXR3b3JrLW9maSIsCndWID0gIk0uKnpmaW5hbCIsCn0sClsiY3JheXBlLXg4Ni1taWxhbiJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS14ODYtbWlsYW4ubHVhIiwKZnVsbE5hbWUgPSAiY3JheXBl"
declare -x _ModuleTable009_="LXg4Ni1taWxhbiIsCmxvYWRPcmRlciA9IDEzLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS14ODYtbWlsYW4iLAp3ViA9ICJNLip6ZmluYWwiLAp9LApjdWRubiA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY3Vkbm4vOS4xLjAubHVhIiwKZnVsbE5hbWUgPSAiY3Vkbm4vOS4xLjAiLApsb2FkT3JkZXIgPSAxNiwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjdWRubi85LjEuMCIsCndWID0gIjAwMDAwMDAwOS4wMDAwMDAwMDEuKnpmaW5hbCIsCn0sCmRhcnNoYW4gPSB7CmZuID0gIi9zb2Z0L3BlcmZ0b29scy9k"
declare -x _ModuleTable010_="YXJzaGFuL2RhcnNoYW4tMy40LjQvc2hhcmUvY3JheXBlLTIueC9tb2R1bGVmaWxlcy9kYXJzaGFuLzMuNC40IiwKZnVsbE5hbWUgPSAiZGFyc2hhbi8zLjQuNCIsCmxvYWRPcmRlciA9IDQsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiZGFyc2hhbiIsCndWID0gIjAwMDAwMDAwMy4wMDAwMDAwMDQuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImdjYy1uYXRpdmUiXSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9nY2MtbmF0aXZlLzEyLjMubHVhIiwKZnVsbE5hbWUgPSAiZ2NjLW5hdGl2ZS8xMi4zIiwKbG9hZE9yZGVyID0gNiwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0"
declare -x _ModuleTable011_="YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJnY2MtbmF0aXZlIiwKd1YgPSAiXjAwMDAwMDEyLjAwMDAwMDAwMy4qemZpbmFsIiwKfSwKbGliZmFicmljID0gewpmbiA9ICIvb3B0L2NyYXkvbW9kdWxlZmlsZXMvbGliZmFicmljLzEuMTUuMi4wIiwKZnVsbE5hbWUgPSAibGliZmFicmljLzEuMTUuMi4wIiwKbG9hZE9yZGVyID0gMSwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJsaWJmYWJyaWMiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDE1LjAwMDAwMDAwMi4qemZpbmFsIiwKfSwKWyJwZXJmdG9vbHMtYmFzZSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb3JlL3BlcmZ0b29s"
declare -x _ModuleTable012_="cy1iYXNlLzIzLjEyLjAubHVhIiwKZnVsbE5hbWUgPSAicGVyZnRvb2xzLWJhc2UvMjMuMTIuMCIsCmxvYWRPcmRlciA9IDMsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAicGVyZnRvb2xzLWJhc2UiLAp3ViA9ICJeMDAwMDAwMjMuMDAwMDAwMDEyLip6ZmluYWwiLAp9LAp4YWx0ID0gewpmbiA9ICIvc29mdC94YWx0L21vZHVsZWZpbGVzL3hhbHQvMy4wLjItMjAyNDA4MjgyMDUwIiwKZnVsbE5hbWUgPSAieGFsdC8zLjAuMi0yMDI0MDgyODIwNTAiLApsb2FkT3JkZXIgPSA1LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gInhhbHQiLAp3ViA9ICJeMDAwMDAw"
declare -x _ModuleTable013_="MDMuMDAwMDAwMDAwLjAwMDAwMDAwMi4qemZpbmFsLS4yMDI0MDgyODIwNTAuKnpmaW5hbCIsCn0sCn0sCm1wYXRoQSA9IHsKCiIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9oZGY1LXBhcmFsbGVsL2dudS8xMi4wL29maS8xLjAvY3JheS1tcGljaC84LjAvY3JheS1oZGY1LXBhcmFsbGVsLzEuMTIuMiIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3B1L3g4Ni1taWxhbi8xLjAiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9vZmkvMS4wL2NyYXktbXBpY2gvOC4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMCIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxl"
declare -x _ModuleTable014_="ZmlsZXMvbWl4X2NvbXBpbGVycyIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29tcGlsZXIvZ251LzEyLjAiLCAiL3NvZnQvbW9kdWxlZmlsZXMiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL3BlcmZ0b29scy8yMy4xMi4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9uZXQvb2ZpLzEuMCIsICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0xpbnV4IgosICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmUiLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZSIKLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BhbHMvbG1vZC9tb2R1bGVmaWxlcy9jb3JlIgosICIvb3B0L2Ny"
declare -x _ModuleTable015_="YXkvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZSIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdCIKLCAiL3NvZnQvcGVyZnRvb2xzL2RhcnNoYW4vZGFyc2hhbi0zLjQuNC9zaGFyZS9jcmF5cGUtMi54L21vZHVsZWZpbGVzIiwgIi9zb2Z0L3hhbHQvbW9kdWxlZmlsZXMiLAp9LApzeXN0ZW1CYXNlTVBBVEggPSAiL3Vzci9zaGFyZS9tb2R1bGVmaWxlcy9MaW51eDovdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmU6L3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZTovdXNyL3NoYXJlL2xtb2QvbG1vZC9tb2R1bGVmaWxlczovb3B0L2NyYXkvcGFscy9sbW9kL21vZHVs"
declare -x _ModuleTable016_="ZWZpbGVzL2NvcmU6L29wdC9jcmF5L21vZHVsZWZpbGVzOi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmU6L29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdDovc29mdC9wZXJmdG9vbHMvZGFyc2hhbi9kYXJzaGFuLTMuNC40L3NoYXJlL2NyYXlwZS0yLngvbW9kdWxlZmlsZXM6L3NvZnQveGFsdC9tb2R1bGVmaWxlcyIsCn0K"
declare -x _ModuleTable_Sz_="16"
declare -x __LMOD_Priority_PATH="/soft/xalt/3.0.2-202408282050/bin:-100"
declare -x __LMOD_REF_COUNT_COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin:1"
declare -x __LMOD_REF_COUNT_CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:1;/opt/cray/pe/pmi/6.1.13/lib:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:1;/opt/cray/pe/mpich/8.1.28/gtl/lib:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib:1;/opt/cray/pe/perftools/23.12.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:1;/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:1;/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:1;/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:1;/soft/perftools/darshan/darshan-3.4.4/lib:1;/opt/cray/pe/papi/7.0.1.2/lib64:1;/opt/cray/libfabric/1.15.2.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so:1"
declare -x __LMOD_REF_COUNT_MANPATH="/opt/cray/pals/1.3.4/man:2;/opt/cray/pe/pmi/6.1.13/man:1;/opt/cray/pe/mpich/8.1.28/ofi/man:1;/opt/cray/pe/mpich/8.1.28/man/mpich:1;/opt/cray/pe/dsmml/0.2.2/dsmml/man:1;/opt/cray/pe/craype/2.7.30/man:1;/opt/cray/pe/perftools/23.12.0/man:1;/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:1;/opt/cray/libfabric/1.15.2.0/share/man:1;/usr/share/lmod/lmod/share/man:1;/home/shourya01/.local/man:1;/usr/local/man:1;/usr/share/man:1;/usr/man:1;/opt/c3/man:1;/opt/pbs/share/man:1;/opt/clmgr/man:1;/opt/sgi/share/man:1;/opt/clmgr/share/man:1;/opt/clmgr/lib/cm-cli/man:1"
declare -x __LMOD_REF_COUNT_MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:1;/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:1;/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:1;/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:1;/opt/cray/pe/lmod/modulefiles/mix_compilers:1;/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:1;/soft/modulefiles:1;/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:1;/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:1;/usr/share/modulefiles/Linux:1;/usr/share/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles:1;/opt/cray/pals/lmod/modulefiles/core:1;/opt/cray/modulefiles:1;/opt/cray/pe/lmod/modulefiles/core:1;/opt/cray/pe/lmod/modulefiles/craype-targets/default:1;/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:1;/soft/xalt/modulefiles:1"
declare -x __LMOD_REF_COUNT_PATH="/soft/xalt/3.0.2-202408282050/bin:1;/soft/compilers/cudatoolkit/cuda-12.4.1/bin:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:1;/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:1;/opt/cray/pe/hdf5/1.12.2.9/bin:1;/opt/cray/pals/1.3.4/bin:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:1;/opt/cray/pe/mpich/8.1.28/bin:1;/opt/cray/pe/craype/2.7.30/bin:1;/home/shourya01/.local/bin:4;/soft/perftools/darshan/darshan-3.4.4/bin:1;/opt/cray/pe/perftools/23.12.0/bin:1;/opt/cray/pe/papi/7.0.1.2/bin:1;/opt/cray/libfabric/1.15.2.0/bin:1;/opt/clmgr/sbin:1;/opt/clmgr/bin:1;/opt/sgi/sbin:1;/opt/sgi/bin:1;/usr/local/bin:1;/usr/bin:1;/bin:2;/opt/c3/bin:1;/usr/lib/mit/bin:1;/usr/lib/mit/sbin:1;/opt/pbs/bin:1;/sbin:1;/home/shourya01/bin:1;/opt/cray/pe/bin:1"
declare -x __LMOD_REF_COUNT_PE_DSMML_PKGCONFIG_LIBS="dsmml:1"
declare -x __LMOD_REF_COUNT_PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:1;hdf5_fortran_parallel:1;mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FIXED_PRGENV="GNU:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_GENCOMPILERS_GNU="12.3:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_PKGCONFIG_LIBS="mpich:1"
declare -x __LMOD_REF_COUNT_PE_PALS_PKGCONFIG_LIBS="libpals:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_LIBS="hdf5_hl_parallel:1;hdf5_parallel:1;mpich:1;dsmml:1;darshan-runtime:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_PRODUCTS="PE_PALS:1;PE_PMI:1;PE_MPICH:1;PE_DSMML:1"
declare -x __LMOD_REF_COUNT_PE_PMI_PKGCONFIG_LIBS="cray-pmi:1"
declare -x __LMOD_REF_COUNT_PE_PRODUCT_LIST="CRAYPE_X86_MILAN:1;PERFTOOLS:1;CRAYPAT:1"
declare -x __LMOD_REF_COUNT_PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:1;/opt/cray/pals/1.3.4/lib/pkgconfig:1;/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:1;/opt/cray/pe/craype/2.7.30/pkg-config:1;/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:1;/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages:1"
declare -x ftp_proxy="http://proxy.alcf.anl.gov:3128"
declare -x http_proxy="http://proxy.alcf.anl.gov:3128"
declare -x https_proxy="http://proxy.alcf.anl.gov:3128"
declare -x no_proxy="admin,polaris-adminvm-01,localhost,*.cm.polaris.alcf.anl.gov,polaris-*,*.polaris.alcf.anl.gov,*.alcf.anl.gov"
Running on 5 nodes
Total number of GPUs: 20
Connected to tcp://x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov:7919
Found executable /soft/applications/conda/2024-04-29/mconda3/bin/python
Launching application b9e7db6c-3616-4d87-ab3c-11ba94c759e0
Using PMI port 36150,36151
[2025-06-23 12:39:47,471] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:39:47,471] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:39:47,471] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:39:47,471] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:39:47,776] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:39:47,777] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:39:47,777] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:39:47,777] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:39:48,291] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:39:48,291] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:39:48,291] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:39:48,291] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:39:48,554] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:39:48,554] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:39:48,554] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:39:48,554] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:39:49,359] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:39:49,360] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:39:49,360] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-23 12:39:49,360] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:52,146] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:52,146] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:39:52,146] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:52,146] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:39:52,146] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 12:39:52,146] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:52,146] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:39:52,146] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:53,008] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:53,008] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:39:53,008] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 12:39:53,008] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:53,008] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:39:53,008] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:53,008] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:39:53,008] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:53,319] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:53,319] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:39:53,319] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 12:39:53,319] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:53,319] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:39:53,319] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:53,319] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:39:53,319] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:53,453] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:53,453] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:39:53,453] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 12:39:53,453] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:53,453] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:39:53,453] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:53,453] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:39:53,453] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=0, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=8, local_rank=0, world_size=20, master_addr=10.140.57.88, master_port=29500
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:54,205] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=16, local_rank=0, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=9, local_rank=1, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=17, local_rank=1, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=1, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=10, local_rank=2, world_size=20, master_addr=10.140.57.88, master_port=29500
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:54,205] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:39:54,205] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=11, local_rank=3, world_size=20, master_addr=10.140.57.88, master_port=29500
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:54,205] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:39:54,205] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=13, local_rank=1, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=18, local_rank=2, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=2, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:39:54,205] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=14, local_rank=2, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=19, local_rank=3, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=3, world_size=20, master_addr=10.140.57.88, master_port=29500
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-23 12:39:54,205] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-23 12:39:54,205] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=15, local_rank=3, world_size=20, master_addr=10.140.57.88, master_port=29500
[2025-06-23 12:39:54,206] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-06-23 12:39:54,206] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=12, local_rank=0, world_size=20, master_addr=10.140.57.88, master_port=29500
Initialized deepspeed on global rank 0, local rank 0 with world size 20.
[2025-06-23 12:44:16,443] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2+5f631abc, git-hash=5f631abc, git-branch=HEAD
[2025-06-23 12:44:22,842] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-06-23 12:44:22,847] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-06-23 12:44:22,847] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-06-23 12:44:22,853] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-06-23 12:44:22,854] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-06-23 12:44:22,854] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-06-23 12:44:22,854] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-06-23 12:44:22,854] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2025-06-23 12:44:22,854] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2025-06-23 12:44:22,854] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-06-23 12:44:22,854] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-06-23 12:44:22,854] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2025-06-23 12:44:22,854] [INFO] [config.py:1000:print]   amp_params ................... False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1543c7e57d10>
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   dump_state ................... False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   global_rank .................. 0
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 5e-05, 'weight_decay': 0.01}
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   pld_params ................... False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2025-06-23 12:44:22,855] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2025-06-23 12:44:22,856] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2025-06-23 12:44:22,856] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2025-06-23 12:44:22,856] [INFO] [config.py:1000:print]   steps_per_print .............. 100000
[2025-06-23 12:44:22,856] [INFO] [config.py:1000:print]   train_batch_size ............. 2560
[2025-06-23 12:44:22,856] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128
[2025-06-23 12:44:22,856] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2025-06-23 12:44:22,856] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2025-06-23 12:44:22,856] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2025-06-23 12:44:22,856] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2025-06-23 12:44:22,856] [INFO] [config.py:1000:print]   world_size ................... 20
[2025-06-23 12:44:22,856] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2025-06-23 12:44:22,856] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-06-23 12:44:22,856] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2025-06-23 12:44:22,856] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2025-06-23 12:44:22,856] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2025-06-23 12:44:22,856] [INFO] [config.py:986:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 128, 
    "train_batch_size": 2.560000e+03, 
    "steps_per_print": 1.000000e+05, 
    "gradient_accumulation_steps": 1, 
    "fp16": {
        "enabled": false
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 5e-05, 
            "weight_decay": 0.01
        }
    }, 
    "comms_logger": {
        "enabled": true, 
        "verbose": false
    }, 
    "zero_optimization": {
        "stage": 0
    }
}
Validating lr=5e-05, train epoch 0.:   0%|          | 0/214 [00:00<?, ?it/s]Validating lr=5e-05, train epoch 0.:   0%|          | 1/214 [00:03<13:05,  3.69s/it]Validating lr=5e-05, train epoch 0.:   1%|          | 2/214 [00:06<10:12,  2.89s/it]Validating lr=5e-05, train epoch 0.:   1%|▏         | 3/214 [00:08<09:15,  2.63s/it]Validating lr=5e-05, train epoch 0.:   2%|▏         | 4/214 [00:10<08:49,  2.52s/it]Validating lr=5e-05, train epoch 0.:   2%|▏         | 5/214 [00:13<08:37,  2.48s/it]Validating lr=5e-05, train epoch 0.:   3%|▎         | 6/214 [00:15<08:28,  2.45s/it]Validating lr=5e-05, train epoch 0.:   3%|▎         | 7/214 [00:17<08:20,  2.42s/it]Validating lr=5e-05, train epoch 0.:   4%|▎         | 8/214 [00:20<08:13,  2.40s/it]Validating lr=5e-05, train epoch 0.:   4%|▍         | 9/214 [00:22<08:09,  2.39s/it]Validating lr=5e-05, train epoch 0.:   5%|▍         | 10/214 [00:24<08:04,  2.37s/it]Validating lr=5e-05, train epoch 0.:   5%|▌         | 11/214 [00:27<07:58,  2.35s/it]Validating lr=5e-05, train epoch 0.:   6%|▌         | 12/214 [00:29<07:54,  2.35s/it]Validating lr=5e-05, train epoch 0.:   6%|▌         | 13/214 [00:31<07:53,  2.35s/it]Validating lr=5e-05, train epoch 0.:   7%|▋         | 14/214 [00:34<07:50,  2.35s/it]Validating lr=5e-05, train epoch 0.:   7%|▋         | 15/214 [00:36<07:46,  2.35s/it]Validating lr=5e-05, train epoch 0.:   7%|▋         | 16/214 [00:38<07:45,  2.35s/it]Validating lr=5e-05, train epoch 0.:   8%|▊         | 17/214 [00:41<07:43,  2.35s/it]Validating lr=5e-05, train epoch 0.:   8%|▊         | 18/214 [00:43<07:43,  2.36s/it]Validating lr=5e-05, train epoch 0.:   9%|▉         | 19/214 [00:46<07:42,  2.37s/it]Validating lr=5e-05, train epoch 0.:   9%|▉         | 20/214 [00:48<07:41,  2.38s/it]Validating lr=5e-05, train epoch 0.:  10%|▉         | 21/214 [00:50<07:36,  2.37s/it]Validating lr=5e-05, train epoch 0.:  10%|█         | 22/214 [00:53<07:37,  2.38s/it]Validating lr=5e-05, train epoch 0.:  11%|█         | 23/214 [00:55<07:33,  2.37s/it]Validating lr=5e-05, train epoch 0.:  11%|█         | 24/214 [00:57<07:31,  2.38s/it]Validating lr=5e-05, train epoch 0.:  12%|█▏        | 25/214 [01:00<07:28,  2.37s/it]Validating lr=5e-05, train epoch 0.:  12%|█▏        | 26/214 [01:02<07:22,  2.36s/it]Validating lr=5e-05, train epoch 0.:  13%|█▎        | 27/214 [01:05<07:20,  2.35s/it]Validating lr=5e-05, train epoch 0.:  13%|█▎        | 28/214 [01:07<07:19,  2.36s/it]Validating lr=5e-05, train epoch 0.:  14%|█▎        | 29/214 [01:09<07:16,  2.36s/it]Validating lr=5e-05, train epoch 0.:  14%|█▍        | 30/214 [01:12<07:13,  2.36s/it]Validating lr=5e-05, train epoch 0.:  14%|█▍        | 31/214 [01:14<07:07,  2.34s/it]Validating lr=5e-05, train epoch 0.:  15%|█▍        | 32/214 [01:16<07:06,  2.34s/it]Validating lr=5e-05, train epoch 0.:  15%|█▌        | 33/214 [01:19<07:05,  2.35s/it]Validating lr=5e-05, train epoch 0.:  16%|█▌        | 34/214 [01:21<07:04,  2.36s/it]Validating lr=5e-05, train epoch 0.:  16%|█▋        | 35/214 [01:23<07:02,  2.36s/it]Validating lr=5e-05, train epoch 0.:  17%|█▋        | 36/214 [01:26<07:00,  2.36s/it]Validating lr=5e-05, train epoch 0.:  17%|█▋        | 37/214 [01:28<06:57,  2.36s/it]Validating lr=5e-05, train epoch 0.:  18%|█▊        | 38/214 [01:30<06:57,  2.37s/it]Validating lr=5e-05, train epoch 0.:  18%|█▊        | 39/214 [01:33<06:54,  2.37s/it]Validating lr=5e-05, train epoch 0.:  19%|█▊        | 40/214 [01:35<06:52,  2.37s/it]Validating lr=5e-05, train epoch 0.:  19%|█▉        | 41/214 [01:38<06:48,  2.36s/it]Validating lr=5e-05, train epoch 0.:  20%|█▉        | 42/214 [01:40<06:43,  2.35s/it]Validating lr=5e-05, train epoch 0.:  20%|██        | 43/214 [01:42<06:41,  2.35s/it]Validating lr=5e-05, train epoch 0.:  21%|██        | 44/214 [01:45<06:41,  2.36s/it]Validating lr=5e-05, train epoch 0.:  21%|██        | 45/214 [01:47<06:40,  2.37s/it]Validating lr=5e-05, train epoch 0.:  21%|██▏       | 46/214 [01:49<06:39,  2.38s/it]Validating lr=5e-05, train epoch 0.:  22%|██▏       | 47/214 [01:52<06:36,  2.37s/it]Validating lr=5e-05, train epoch 0.:  22%|██▏       | 48/214 [01:54<06:34,  2.38s/it]Validating lr=5e-05, train epoch 0.:  23%|██▎       | 49/214 [01:56<06:30,  2.37s/it]Validating lr=5e-05, train epoch 0.:  23%|██▎       | 50/214 [01:59<06:25,  2.35s/it]Validating lr=5e-05, train epoch 0.:  24%|██▍       | 51/214 [02:01<06:21,  2.34s/it]Validating lr=5e-05, train epoch 0.:  24%|██▍       | 52/214 [02:03<06:18,  2.33s/it]Validating lr=5e-05, train epoch 0.:  25%|██▍       | 53/214 [02:06<06:14,  2.32s/it]Validating lr=5e-05, train epoch 0.:  25%|██▌       | 54/214 [02:08<06:10,  2.31s/it]Validating lr=5e-05, train epoch 0.:  26%|██▌       | 55/214 [02:10<06:07,  2.31s/it]Validating lr=5e-05, train epoch 0.:  26%|██▌       | 56/214 [02:13<06:05,  2.31s/it]Validating lr=5e-05, train epoch 0.:  27%|██▋       | 57/214 [02:15<06:01,  2.31s/it]Validating lr=5e-05, train epoch 0.:  27%|██▋       | 58/214 [02:17<05:59,  2.31s/it]Validating lr=5e-05, train epoch 0.:  28%|██▊       | 59/214 [02:20<05:56,  2.30s/it]Validating lr=5e-05, train epoch 0.:  28%|██▊       | 60/214 [02:22<05:57,  2.32s/it]Validating lr=5e-05, train epoch 0.:  29%|██▊       | 61/214 [02:24<05:55,  2.32s/it]Validating lr=5e-05, train epoch 0.:  29%|██▉       | 62/214 [02:27<05:53,  2.33s/it]Validating lr=5e-05, train epoch 0.:  29%|██▉       | 63/214 [02:29<05:50,  2.32s/it]Validating lr=5e-05, train epoch 0.:  30%|██▉       | 64/214 [02:31<05:49,  2.33s/it]Validating lr=5e-05, train epoch 0.:  30%|███       | 65/214 [02:34<05:46,  2.33s/it]Validating lr=5e-05, train epoch 0.:  31%|███       | 66/214 [02:36<05:42,  2.32s/it]Validating lr=5e-05, train epoch 0.:  31%|███▏      | 67/214 [02:38<05:44,  2.35s/it]Validating lr=5e-05, train epoch 0.:  32%|███▏      | 68/214 [02:41<05:40,  2.33s/it]Validating lr=5e-05, train epoch 0.:  32%|███▏      | 69/214 [02:43<05:36,  2.32s/it]Validating lr=5e-05, train epoch 0.:  33%|███▎      | 70/214 [02:45<05:36,  2.34s/it]Validating lr=5e-05, train epoch 0.:  33%|███▎      | 71/214 [02:48<05:32,  2.33s/it]Validating lr=5e-05, train epoch 0.:  34%|███▎      | 72/214 [02:50<05:30,  2.33s/it]Validating lr=5e-05, train epoch 0.:  34%|███▍      | 73/214 [02:52<05:30,  2.34s/it]Validating lr=5e-05, train epoch 0.:  35%|███▍      | 74/214 [02:55<05:28,  2.34s/it]Validating lr=5e-05, train epoch 0.:  35%|███▌      | 75/214 [02:57<05:28,  2.36s/it]Validating lr=5e-05, train epoch 0.:  36%|███▌      | 76/214 [02:59<05:24,  2.35s/it]Validating lr=5e-05, train epoch 0.:  36%|███▌      | 77/214 [03:02<05:19,  2.33s/it]Validating lr=5e-05, train epoch 0.:  36%|███▋      | 78/214 [03:04<05:18,  2.34s/it]Validating lr=5e-05, train epoch 0.:  37%|███▋      | 79/214 [03:06<05:14,  2.33s/it]Validating lr=5e-05, train epoch 0.:  37%|███▋      | 80/214 [03:09<05:11,  2.33s/it]Validating lr=5e-05, train epoch 0.:  38%|███▊      | 81/214 [03:11<05:10,  2.33s/it]Validating lr=5e-05, train epoch 0.:  38%|███▊      | 82/214 [03:13<05:08,  2.34s/it]Validating lr=5e-05, train epoch 0.:  39%|███▉      | 83/214 [03:16<05:04,  2.32s/it]Validating lr=5e-05, train epoch 0.:  39%|███▉      | 84/214 [03:18<05:02,  2.32s/it]Validating lr=5e-05, train epoch 0.:  40%|███▉      | 85/214 [03:20<04:59,  2.32s/it]Validating lr=5e-05, train epoch 0.:  40%|████      | 86/214 [03:23<04:58,  2.33s/it]Validating lr=5e-05, train epoch 0.:  41%|████      | 87/214 [03:25<04:55,  2.33s/it]Validating lr=5e-05, train epoch 0.:  41%|████      | 88/214 [03:27<04:52,  2.32s/it]Validating lr=5e-05, train epoch 0.:  42%|████▏     | 89/214 [03:30<04:52,  2.34s/it]Validating lr=5e-05, train epoch 0.:  42%|████▏     | 90/214 [03:32<04:51,  2.35s/it]Validating lr=5e-05, train epoch 0.:  43%|████▎     | 91/214 [03:34<04:50,  2.36s/it]Validating lr=5e-05, train epoch 0.:  43%|████▎     | 92/214 [03:37<04:47,  2.36s/it]Validating lr=5e-05, train epoch 0.:  43%|████▎     | 93/214 [03:39<04:44,  2.35s/it]Validating lr=5e-05, train epoch 0.:  44%|████▍     | 94/214 [03:41<04:40,  2.34s/it]Validating lr=5e-05, train epoch 0.:  44%|████▍     | 95/214 [03:44<04:36,  2.33s/it]Validating lr=5e-05, train epoch 0.:  45%|████▍     | 96/214 [03:46<04:35,  2.34s/it]Validating lr=5e-05, train epoch 0.:  45%|████▌     | 97/214 [03:48<04:34,  2.35s/it]Validating lr=5e-05, train epoch 0.:  46%|████▌     | 98/214 [03:51<04:30,  2.34s/it]Validating lr=5e-05, train epoch 0.:  46%|████▋     | 99/214 [03:53<04:27,  2.32s/it]Validating lr=5e-05, train epoch 0.:  47%|████▋     | 100/214 [03:55<04:25,  2.33s/it]Validating lr=5e-05, train epoch 0.:  47%|████▋     | 101/214 [03:58<04:23,  2.33s/it]Validating lr=5e-05, train epoch 0.:  48%|████▊     | 102/214 [04:00<04:20,  2.33s/it]Validating lr=5e-05, train epoch 0.:  48%|████▊     | 103/214 [04:02<04:17,  2.32s/it]Validating lr=5e-05, train epoch 0.:  49%|████▊     | 104/214 [04:05<04:14,  2.32s/it]Validating lr=5e-05, train epoch 0.:  49%|████▉     | 105/214 [04:07<04:13,  2.32s/it]Validating lr=5e-05, train epoch 0.:  50%|████▉     | 106/214 [04:09<04:11,  2.33s/it]Validating lr=5e-05, train epoch 0.:  50%|█████     | 107/214 [04:12<04:09,  2.33s/it]Validating lr=5e-05, train epoch 0.:  50%|█████     | 108/214 [04:14<04:07,  2.34s/it]Validating lr=5e-05, train epoch 0.:  51%|█████     | 109/214 [04:16<04:04,  2.33s/it]Validating lr=5e-05, train epoch 0.:  51%|█████▏    | 110/214 [04:19<04:01,  2.32s/it]Validating lr=5e-05, train epoch 0.:  52%|█████▏    | 111/214 [04:21<03:59,  2.32s/it]Validating lr=5e-05, train epoch 0.:  52%|█████▏    | 112/214 [04:23<03:56,  2.32s/it]Validating lr=5e-05, train epoch 0.:  53%|█████▎    | 113/214 [04:25<03:53,  2.31s/it]Validating lr=5e-05, train epoch 0.:  53%|█████▎    | 114/214 [04:28<03:51,  2.31s/it]Validating lr=5e-05, train epoch 0.:  54%|█████▎    | 115/214 [04:30<03:49,  2.32s/it]Validating lr=5e-05, train epoch 0.:  54%|█████▍    | 116/214 [04:32<03:45,  2.31s/it]Validating lr=5e-05, train epoch 0.:  55%|█████▍    | 117/214 [04:35<03:44,  2.31s/it]Validating lr=5e-05, train epoch 0.:  55%|█████▌    | 118/214 [04:37<03:44,  2.34s/it]Validating lr=5e-05, train epoch 0.:  56%|█████▌    | 119/214 [04:40<03:43,  2.35s/it]Validating lr=5e-05, train epoch 0.:  56%|█████▌    | 120/214 [04:42<03:39,  2.34s/it]Validating lr=5e-05, train epoch 0.:  57%|█████▋    | 121/214 [04:44<03:36,  2.32s/it]Validating lr=5e-05, train epoch 0.:  57%|█████▋    | 122/214 [04:46<03:34,  2.33s/it]Validating lr=5e-05, train epoch 0.:  57%|█████▋    | 123/214 [04:49<03:31,  2.32s/it]Validating lr=5e-05, train epoch 0.:  58%|█████▊    | 124/214 [04:51<03:29,  2.33s/it]Validating lr=5e-05, train epoch 0.:  58%|█████▊    | 125/214 [04:53<03:27,  2.33s/it]Validating lr=5e-05, train epoch 0.:  59%|█████▉    | 126/214 [04:56<03:26,  2.35s/it]Validating lr=5e-05, train epoch 0.:  59%|█████▉    | 127/214 [04:58<03:24,  2.35s/it]Validating lr=5e-05, train epoch 0.:  60%|█████▉    | 128/214 [05:01<03:21,  2.34s/it]Validating lr=5e-05, train epoch 0.:  60%|██████    | 129/214 [05:03<03:17,  2.32s/it]Validating lr=5e-05, train epoch 0.:  61%|██████    | 130/214 [05:05<03:15,  2.32s/it]Validating lr=5e-05, train epoch 0.:  61%|██████    | 131/214 [05:07<03:13,  2.33s/it]Validating lr=5e-05, train epoch 0.:  62%|██████▏   | 132/214 [05:10<03:11,  2.33s/it]Validating lr=5e-05, train epoch 0.:  62%|██████▏   | 133/214 [05:12<03:07,  2.32s/it]Validating lr=5e-05, train epoch 0.:  63%|██████▎   | 134/214 [05:14<03:05,  2.32s/it]Validating lr=5e-05, train epoch 0.:  63%|██████▎   | 135/214 [05:17<03:04,  2.33s/it]Validating lr=5e-05, train epoch 0.:  64%|██████▎   | 136/214 [05:19<03:01,  2.33s/it]Validating lr=5e-05, train epoch 0.:  64%|██████▍   | 137/214 [05:21<02:59,  2.33s/it]Validating lr=5e-05, train epoch 0.:  64%|██████▍   | 138/214 [05:24<02:57,  2.33s/it]Validating lr=5e-05, train epoch 0.:  65%|██████▍   | 139/214 [05:26<02:55,  2.33s/it]Validating lr=5e-05, train epoch 0.:  65%|██████▌   | 140/214 [05:28<02:52,  2.33s/it]Validating lr=5e-05, train epoch 0.:  66%|██████▌   | 141/214 [05:31<02:49,  2.32s/it]Validating lr=5e-05, train epoch 0.:  66%|██████▋   | 142/214 [05:33<02:47,  2.33s/it]Validating lr=5e-05, train epoch 0.:  67%|██████▋   | 143/214 [05:35<02:45,  2.33s/it]Validating lr=5e-05, train epoch 0.:  67%|██████▋   | 144/214 [05:38<02:43,  2.33s/it]Validating lr=5e-05, train epoch 0.:  68%|██████▊   | 145/214 [05:40<02:40,  2.33s/it]Validating lr=5e-05, train epoch 0.:  68%|██████▊   | 146/214 [05:42<02:38,  2.33s/it]Validating lr=5e-05, train epoch 0.:  69%|██████▊   | 147/214 [05:45<02:35,  2.32s/it]Validating lr=5e-05, train epoch 0.:  69%|██████▉   | 148/214 [05:47<02:32,  2.32s/it]Validating lr=5e-05, train epoch 0.:  70%|██████▉   | 149/214 [05:49<02:30,  2.32s/it]Validating lr=5e-05, train epoch 0.:  70%|███████   | 150/214 [05:52<02:28,  2.32s/it]Validating lr=5e-05, train epoch 0.:  71%|███████   | 151/214 [05:54<02:26,  2.32s/it]Validating lr=5e-05, train epoch 0.:  71%|███████   | 152/214 [05:56<02:24,  2.33s/it]Validating lr=5e-05, train epoch 0.:  71%|███████▏  | 153/214 [05:59<02:21,  2.32s/it]Validating lr=5e-05, train epoch 0.:  72%|███████▏  | 154/214 [06:01<02:20,  2.34s/it]Validating lr=5e-05, train epoch 0.:  72%|███████▏  | 155/214 [06:03<02:18,  2.34s/it]Validating lr=5e-05, train epoch 0.:  73%|███████▎  | 156/214 [06:06<02:16,  2.36s/it]Validating lr=5e-05, train epoch 0.:  73%|███████▎  | 157/214 [06:08<02:14,  2.36s/it]Validating lr=5e-05, train epoch 0.:  74%|███████▍  | 158/214 [06:10<02:10,  2.33s/it]Validating lr=5e-05, train epoch 0.:  74%|███████▍  | 159/214 [06:13<02:07,  2.33s/it]Validating lr=5e-05, train epoch 0.:  75%|███████▍  | 160/214 [06:15<02:05,  2.32s/it]Validating lr=5e-05, train epoch 0.:  75%|███████▌  | 161/214 [06:17<02:02,  2.32s/it]Validating lr=5e-05, train epoch 0.:  76%|███████▌  | 162/214 [06:20<02:01,  2.33s/it]Validating lr=5e-05, train epoch 0.:  76%|███████▌  | 163/214 [06:22<01:58,  2.33s/it]Validating lr=5e-05, train epoch 0.:  77%|███████▋  | 164/214 [06:24<01:56,  2.32s/it]Validating lr=5e-05, train epoch 0.:  77%|███████▋  | 165/214 [06:27<01:54,  2.34s/it]Validating lr=5e-05, train epoch 0.:  78%|███████▊  | 166/214 [06:29<01:52,  2.34s/it]Validating lr=5e-05, train epoch 0.:  78%|███████▊  | 167/214 [06:31<01:49,  2.33s/it]Validating lr=5e-05, train epoch 0.:  79%|███████▊  | 168/214 [06:34<01:47,  2.33s/it]Validating lr=5e-05, train epoch 0.:  79%|███████▉  | 169/214 [06:36<01:44,  2.32s/it]Validating lr=5e-05, train epoch 0.:  79%|███████▉  | 170/214 [06:38<01:42,  2.33s/it]Validating lr=5e-05, train epoch 0.:  80%|███████▉  | 171/214 [06:41<01:40,  2.34s/it]Validating lr=5e-05, train epoch 0.:  80%|████████  | 172/214 [06:43<01:37,  2.32s/it]Validating lr=5e-05, train epoch 0.:  81%|████████  | 173/214 [06:45<01:35,  2.32s/it]Validating lr=5e-05, train epoch 0.:  81%|████████▏ | 174/214 [06:48<01:32,  2.32s/it]Validating lr=5e-05, train epoch 0.:  82%|████████▏ | 175/214 [06:50<01:31,  2.34s/it]Validating lr=5e-05, train epoch 0.:  82%|████████▏ | 176/214 [06:52<01:29,  2.36s/it]Validating lr=5e-05, train epoch 0.:  83%|████████▎ | 177/214 [06:55<01:26,  2.34s/it]Validating lr=5e-05, train epoch 0.:  83%|████████▎ | 178/214 [06:57<01:24,  2.34s/it]Validating lr=5e-05, train epoch 0.:  84%|████████▎ | 179/214 [06:59<01:21,  2.33s/it]Validating lr=5e-05, train epoch 0.:  84%|████████▍ | 180/214 [07:02<01:18,  2.32s/it]Validating lr=5e-05, train epoch 0.:  85%|████████▍ | 181/214 [07:04<01:16,  2.31s/it]Validating lr=5e-05, train epoch 0.:  85%|████████▌ | 182/214 [07:06<01:14,  2.33s/it]Validating lr=5e-05, train epoch 0.:  86%|████████▌ | 183/214 [07:09<01:12,  2.34s/it]Validating lr=5e-05, train epoch 0.:  86%|████████▌ | 184/214 [07:11<01:09,  2.32s/it]Validating lr=5e-05, train epoch 0.:  86%|████████▋ | 185/214 [07:13<01:06,  2.31s/it]Validating lr=5e-05, train epoch 0.:  87%|████████▋ | 186/214 [07:16<01:04,  2.32s/it]Validating lr=5e-05, train epoch 0.:  87%|████████▋ | 187/214 [07:18<01:02,  2.31s/it]Validating lr=5e-05, train epoch 0.:  88%|████████▊ | 188/214 [07:20<01:00,  2.33s/it]Validating lr=5e-05, train epoch 0.:  88%|████████▊ | 189/214 [07:23<00:58,  2.34s/it]Validating lr=5e-05, train epoch 0.:  89%|████████▉ | 190/214 [07:25<00:55,  2.33s/it]Validating lr=5e-05, train epoch 0.:  89%|████████▉ | 191/214 [07:27<00:53,  2.32s/it]Validating lr=5e-05, train epoch 0.:  90%|████████▉ | 192/214 [07:29<00:50,  2.31s/it]Validating lr=5e-05, train epoch 0.:  90%|█████████ | 193/214 [07:32<00:48,  2.31s/it]Validating lr=5e-05, train epoch 0.:  91%|█████████ | 194/214 [07:34<00:46,  2.30s/it]Validating lr=5e-05, train epoch 0.:  91%|█████████ | 195/214 [07:36<00:44,  2.32s/it]Validating lr=5e-05, train epoch 0.:  92%|█████████▏| 196/214 [07:39<00:42,  2.34s/it]Validating lr=5e-05, train epoch 0.:  92%|█████████▏| 197/214 [07:41<00:39,  2.34s/it]Validating lr=5e-05, train epoch 0.:  93%|█████████▎| 198/214 [07:44<00:37,  2.34s/it]Validating lr=5e-05, train epoch 0.:  93%|█████████▎| 199/214 [07:46<00:34,  2.33s/it]Validating lr=5e-05, train epoch 0.:  93%|█████████▎| 200/214 [07:48<00:32,  2.34s/it]Validating lr=5e-05, train epoch 0.:  94%|█████████▍| 201/214 [07:51<00:30,  2.36s/it]Validating lr=5e-05, train epoch 0.:  94%|█████████▍| 202/214 [07:53<00:28,  2.37s/it]Validating lr=5e-05, train epoch 0.:  95%|█████████▍| 203/214 [07:55<00:26,  2.37s/it]Validating lr=5e-05, train epoch 0.:  95%|█████████▌| 204/214 [07:58<00:23,  2.37s/it]Validating lr=5e-05, train epoch 0.:  96%|█████████▌| 205/214 [08:00<00:21,  2.37s/it]Validating lr=5e-05, train epoch 0.:  96%|█████████▋| 206/214 [08:02<00:18,  2.37s/it]Validating lr=5e-05, train epoch 0.:  97%|█████████▋| 207/214 [08:05<00:16,  2.37s/it]Validating lr=5e-05, train epoch 0.:  97%|█████████▋| 208/214 [08:07<00:14,  2.35s/it]Validating lr=5e-05, train epoch 0.:  98%|█████████▊| 209/214 [08:10<00:11,  2.36s/it]Validating lr=5e-05, train epoch 0.:  98%|█████████▊| 210/214 [08:12<00:09,  2.37s/it]Validating lr=5e-05, train epoch 0.:  99%|█████████▊| 211/214 [08:14<00:07,  2.38s/it]Validating lr=5e-05, train epoch 0.:  99%|█████████▉| 212/214 [08:17<00:04,  2.36s/it]Validating lr=5e-05, train epoch 0.: 100%|█████████▉| 213/214 [08:19<00:02,  2.37s/it]Validating lr=5e-05, train epoch 0.: 100%|██████████| 214/214 [08:21<00:00,  2.36s/it]Validating lr=5e-05, train epoch 0.: 100%|██████████| 214/214 [08:21<00:00,  2.35s/it]
Validating lr=5e-05, train epoch 1.:   0%|          | 0/214 [00:00<?, ?it/s]Validating lr=5e-05, train epoch 1.:   0%|          | 1/214 [00:02<08:31,  2.40s/it]Validating lr=5e-05, train epoch 1.:   1%|          | 2/214 [00:04<08:17,  2.35s/it]Validating lr=5e-05, train epoch 1.:   1%|▏         | 3/214 [00:07<08:19,  2.37s/it]Validating lr=5e-05, train epoch 1.:   2%|▏         | 4/214 [00:09<08:18,  2.38s/it]Validating lr=5e-05, train epoch 1.:   2%|▏         | 5/214 [00:11<08:17,  2.38s/it]Validating lr=5e-05, train epoch 1.:   3%|▎         | 6/214 [00:14<08:12,  2.37s/it]Validating lr=5e-05, train epoch 1.:   3%|▎         | 7/214 [00:16<08:10,  2.37s/it]Validating lr=5e-05, train epoch 1.:   4%|▎         | 8/214 [00:18<08:07,  2.37s/it]Validating lr=5e-05, train epoch 1.:   4%|▍         | 9/214 [00:21<08:03,  2.36s/it]Validating lr=5e-05, train epoch 1.:   5%|▍         | 10/214 [00:23<07:58,  2.34s/it]Validating lr=5e-05, train epoch 1.:   5%|▌         | 11/214 [00:25<07:53,  2.33s/it]Validating lr=5e-05, train epoch 1.:   6%|▌         | 12/214 [00:28<07:52,  2.34s/it]Validating lr=5e-05, train epoch 1.:   6%|▌         | 13/214 [00:30<07:48,  2.33s/it]Validating lr=5e-05, train epoch 1.:   7%|▋         | 14/214 [00:32<07:49,  2.35s/it]Validating lr=5e-05, train epoch 1.:   7%|▋         | 15/214 [00:35<07:46,  2.34s/it]Validating lr=5e-05, train epoch 1.:   7%|▋         | 16/214 [00:37<07:47,  2.36s/it]Validating lr=5e-05, train epoch 1.:   8%|▊         | 17/214 [00:40<07:43,  2.35s/it]Validating lr=5e-05, train epoch 1.:   8%|▊         | 18/214 [00:42<07:41,  2.36s/it]Validating lr=5e-05, train epoch 1.:   9%|▉         | 19/214 [00:44<07:35,  2.34s/it]Validating lr=5e-05, train epoch 1.:   9%|▉         | 20/214 [00:47<07:35,  2.35s/it]Validating lr=5e-05, train epoch 1.:  10%|▉         | 21/214 [00:49<07:31,  2.34s/it]Validating lr=5e-05, train epoch 1.:  10%|█         | 22/214 [00:51<07:30,  2.35s/it]Validating lr=5e-05, train epoch 1.:  11%|█         | 23/214 [00:54<07:27,  2.35s/it]Validating lr=5e-05, train epoch 1.:  11%|█         | 24/214 [00:56<07:28,  2.36s/it]Validating lr=5e-05, train epoch 1.:  12%|█▏        | 25/214 [00:58<07:29,  2.38s/it]Validating lr=5e-05, train epoch 1.:  12%|█▏        | 26/214 [01:01<07:23,  2.36s/it]Validating lr=5e-05, train epoch 1.:  13%|█▎        | 27/214 [01:03<07:19,  2.35s/it]Validating lr=5e-05, train epoch 1.:  13%|█▎        | 28/214 [01:05<07:19,  2.37s/it]Validating lr=5e-05, train epoch 1.:  14%|█▎        | 29/214 [01:08<07:16,  2.36s/it]Validating lr=5e-05, train epoch 1.:  14%|█▍        | 30/214 [01:10<07:17,  2.38s/it]Validating lr=5e-05, train epoch 1.:  14%|█▍        | 31/214 [01:13<07:14,  2.37s/it]Validating lr=5e-05, train epoch 1.:  15%|█▍        | 32/214 [01:15<07:11,  2.37s/it]Validating lr=5e-05, train epoch 1.:  15%|█▌        | 33/214 [01:17<07:05,  2.35s/it]Validating lr=5e-05, train epoch 1.:  16%|█▌        | 34/214 [01:20<07:07,  2.38s/it]Validating lr=5e-05, train epoch 1.:  16%|█▋        | 35/214 [01:22<07:02,  2.36s/it]Validating lr=5e-05, train epoch 1.:  17%|█▋        | 36/214 [01:24<07:02,  2.38s/it]Validating lr=5e-05, train epoch 1.:  17%|█▋        | 37/214 [01:27<07:01,  2.38s/it]Validating lr=5e-05, train epoch 1.:  18%|█▊        | 38/214 [01:29<06:59,  2.39s/it]Validating lr=5e-05, train epoch 1.:  18%|█▊        | 39/214 [01:32<06:54,  2.37s/it]Validating lr=5e-05, train epoch 1.:  19%|█▊        | 40/214 [01:34<06:51,  2.36s/it]Validating lr=5e-05, train epoch 1.:  19%|█▉        | 41/214 [01:36<06:48,  2.36s/it]Validating lr=5e-05, train epoch 1.:  20%|█▉        | 42/214 [01:39<06:46,  2.36s/it]Validating lr=5e-05, train epoch 1.:  20%|██        | 43/214 [01:41<06:46,  2.37s/it]Validating lr=5e-05, train epoch 1.:  21%|██        | 44/214 [01:43<06:44,  2.38s/it]Validating lr=5e-05, train epoch 1.:  21%|██        | 45/214 [01:46<06:43,  2.39s/it]Validating lr=5e-05, train epoch 1.:  21%|██▏       | 46/214 [01:48<06:40,  2.38s/it]Validating lr=5e-05, train epoch 1.:  22%|██▏       | 47/214 [01:51<06:35,  2.37s/it]Validating lr=5e-05, train epoch 1.:  22%|██▏       | 48/214 [01:53<06:34,  2.37s/it]Validating lr=5e-05, train epoch 1.:  23%|██▎       | 49/214 [01:55<06:34,  2.39s/it]Validating lr=5e-05, train epoch 1.:  23%|██▎       | 50/214 [01:58<06:30,  2.38s/it]Validating lr=5e-05, train epoch 1.:  24%|██▍       | 51/214 [02:00<06:28,  2.38s/it]Validating lr=5e-05, train epoch 1.:  24%|██▍       | 52/214 [02:02<06:26,  2.38s/it]Validating lr=5e-05, train epoch 1.:  25%|██▍       | 53/214 [02:05<06:21,  2.37s/it]Validating lr=5e-05, train epoch 1.:  25%|██▌       | 54/214 [02:07<06:17,  2.36s/it]Validating lr=5e-05, train epoch 1.:  26%|██▌       | 55/214 [02:09<06:12,  2.34s/it]Validating lr=5e-05, train epoch 1.:  26%|██▌       | 56/214 [02:12<06:13,  2.36s/it]Validating lr=5e-05, train epoch 1.:  27%|██▋       | 57/214 [02:14<06:11,  2.37s/it]Validating lr=5e-05, train epoch 1.:  27%|██▋       | 58/214 [02:17<06:10,  2.37s/it]Validating lr=5e-05, train epoch 1.:  28%|██▊       | 59/214 [02:19<06:06,  2.37s/it]Validating lr=5e-05, train epoch 1.:  28%|██▊       | 60/214 [02:21<06:04,  2.37s/it]Validating lr=5e-05, train epoch 1.:  29%|██▊       | 61/214 [02:24<06:04,  2.38s/it]Validating lr=5e-05, train epoch 1.:  29%|██▉       | 62/214 [02:26<06:02,  2.39s/it]Validating lr=5e-05, train epoch 1.:  29%|██▉       | 63/214 [02:29<06:00,  2.39s/it]Validating lr=5e-05, train epoch 1.:  30%|██▉       | 64/214 [02:31<05:58,  2.39s/it]Validating lr=5e-05, train epoch 1.:  30%|███       | 65/214 [02:33<05:53,  2.37s/it]Validating lr=5e-05, train epoch 1.:  31%|███       | 66/214 [02:36<05:51,  2.38s/it]Validating lr=5e-05, train epoch 1.:  31%|███▏      | 67/214 [02:38<05:50,  2.38s/it]Validating lr=5e-05, train epoch 1.:  32%|███▏      | 68/214 [02:40<05:49,  2.39s/it]Validating lr=5e-05, train epoch 1.:  32%|███▏      | 69/214 [02:43<05:49,  2.41s/it]Validating lr=5e-05, train epoch 1.:  33%|███▎      | 70/214 [02:45<05:46,  2.41s/it]Validating lr=5e-05, train epoch 1.:  33%|███▎      | 71/214 [02:48<05:42,  2.40s/it]Validating lr=5e-05, train epoch 1.:  34%|███▎      | 72/214 [02:50<05:38,  2.39s/it]Validating lr=5e-05, train epoch 1.:  34%|███▍      | 73/214 [02:52<05:37,  2.39s/it]Validating lr=5e-05, train epoch 1.:  35%|███▍      | 74/214 [02:55<05:34,  2.39s/it]Validating lr=5e-05, train epoch 1.:  35%|███▌      | 75/214 [02:57<05:31,  2.39s/it]Validating lr=5e-05, train epoch 1.:  36%|███▌      | 76/214 [03:00<05:28,  2.38s/it]Validating lr=5e-05, train epoch 1.:  36%|███▌      | 77/214 [03:02<05:26,  2.38s/it]Validating lr=5e-05, train epoch 1.:  36%|███▋      | 78/214 [03:04<05:23,  2.38s/it]Validating lr=5e-05, train epoch 1.:  37%|███▋      | 79/214 [03:07<05:21,  2.38s/it]Validating lr=5e-05, train epoch 1.:  37%|███▋      | 80/214 [03:09<05:17,  2.37s/it]Validating lr=5e-05, train epoch 1.:  38%|███▊      | 81/214 [03:11<05:16,  2.38s/it]Validating lr=5e-05, train epoch 1.:  38%|███▊      | 82/214 [03:14<05:14,  2.38s/it]Validating lr=5e-05, train epoch 1.:  39%|███▉      | 83/214 [03:16<05:12,  2.38s/it]Validating lr=5e-05, train epoch 1.:  39%|███▉      | 84/214 [03:19<05:09,  2.38s/it]Validating lr=5e-05, train epoch 1.:  40%|███▉      | 85/214 [03:21<05:06,  2.38s/it]Validating lr=5e-05, train epoch 1.:  40%|████      | 86/214 [03:23<05:05,  2.38s/it]Validating lr=5e-05, train epoch 1.:  41%|████      | 87/214 [03:26<05:03,  2.39s/it]Validating lr=5e-05, train epoch 1.:  41%|████      | 88/214 [03:28<05:02,  2.40s/it]Validating lr=5e-05, train epoch 1.:  42%|████▏     | 89/214 [03:31<04:59,  2.40s/it]Validating lr=5e-05, train epoch 1.:  42%|████▏     | 90/214 [03:33<04:56,  2.39s/it]Validating lr=5e-05, train epoch 1.:  43%|████▎     | 91/214 [03:35<04:54,  2.40s/it]Validating lr=5e-05, train epoch 1.:  43%|████▎     | 92/214 [03:38<04:50,  2.38s/it]Validating lr=5e-05, train epoch 1.:  43%|████▎     | 93/214 [03:40<04:48,  2.39s/it]Validating lr=5e-05, train epoch 1.:  44%|████▍     | 94/214 [03:43<04:45,  2.38s/it]Validating lr=5e-05, train epoch 1.:  44%|████▍     | 95/214 [03:45<04:40,  2.35s/it]Validating lr=5e-05, train epoch 1.:  45%|████▍     | 96/214 [03:47<04:38,  2.36s/it]Validating lr=5e-05, train epoch 1.:  45%|████▌     | 97/214 [03:50<04:36,  2.36s/it]Validating lr=5e-05, train epoch 1.:  46%|████▌     | 98/214 [03:52<04:32,  2.35s/it]Validating lr=5e-05, train epoch 1.:  46%|████▋     | 99/214 [03:54<04:31,  2.36s/it]Validating lr=5e-05, train epoch 1.:  47%|████▋     | 100/214 [03:57<04:28,  2.36s/it]Validating lr=5e-05, train epoch 1.:  47%|████▋     | 101/214 [03:59<04:24,  2.34s/it]Validating lr=5e-05, train epoch 1.:  48%|████▊     | 102/214 [04:01<04:21,  2.33s/it]Validating lr=5e-05, train epoch 1.:  48%|████▊     | 103/214 [04:04<04:18,  2.33s/it]Validating lr=5e-05, train epoch 1.:  49%|████▊     | 104/214 [04:06<04:15,  2.33s/it]Validating lr=5e-05, train epoch 1.:  49%|████▉     | 105/214 [04:08<04:14,  2.34s/it]Validating lr=5e-05, train epoch 1.:  50%|████▉     | 106/214 [04:11<04:12,  2.34s/it]Validating lr=5e-05, train epoch 1.:  50%|█████     | 107/214 [04:13<04:09,  2.33s/it]Validating lr=5e-05, train epoch 1.:  50%|█████     | 108/214 [04:15<04:07,  2.33s/it]Validating lr=5e-05, train epoch 1.:  51%|█████     | 109/214 [04:18<04:03,  2.32s/it]Validating lr=5e-05, train epoch 1.:  51%|█████▏    | 110/214 [04:20<04:01,  2.32s/it]Validating lr=5e-05, train epoch 1.:  52%|█████▏    | 111/214 [04:22<04:00,  2.33s/it]Validating lr=5e-05, train epoch 1.:  52%|█████▏    | 112/214 [04:25<03:57,  2.33s/it]Validating lr=5e-05, train epoch 1.:  53%|█████▎    | 113/214 [04:27<03:55,  2.33s/it]Validating lr=5e-05, train epoch 1.:  53%|█████▎    | 114/214 [04:29<03:55,  2.35s/it]Validating lr=5e-05, train epoch 1.:  54%|█████▎    | 115/214 [04:32<03:52,  2.35s/it]Validating lr=5e-05, train epoch 1.:  54%|█████▍    | 116/214 [04:34<03:50,  2.36s/it]Validating lr=5e-05, train epoch 1.:  55%|█████▍    | 117/214 [04:36<03:48,  2.36s/it]Validating lr=5e-05, train epoch 1.:  55%|█████▌    | 118/214 [04:39<03:45,  2.35s/it]Validating lr=5e-05, train epoch 1.:  56%|█████▌    | 119/214 [04:41<03:43,  2.35s/it]Validating lr=5e-05, train epoch 1.:  56%|█████▌    | 120/214 [04:43<03:42,  2.37s/it]Validating lr=5e-05, train epoch 1.:  57%|█████▋    | 121/214 [04:46<03:40,  2.37s/it]Validating lr=5e-05, train epoch 1.:  57%|█████▋    | 122/214 [04:48<03:36,  2.35s/it]Validating lr=5e-05, train epoch 1.:  57%|█████▋    | 123/214 [04:50<03:32,  2.33s/it]Validating lr=5e-05, train epoch 1.:  58%|█████▊    | 124/214 [04:53<03:29,  2.32s/it]Validating lr=5e-05, train epoch 1.:  58%|█████▊    | 125/214 [04:55<03:26,  2.32s/it]Validating lr=5e-05, train epoch 1.:  59%|█████▉    | 126/214 [04:57<03:23,  2.31s/it]Validating lr=5e-05, train epoch 1.:  59%|█████▉    | 127/214 [05:00<03:21,  2.32s/it]Validating lr=5e-05, train epoch 1.:  60%|█████▉    | 128/214 [05:02<03:20,  2.33s/it]Validating lr=5e-05, train epoch 1.:  60%|██████    | 129/214 [05:04<03:17,  2.32s/it]Validating lr=5e-05, train epoch 1.:  61%|██████    | 130/214 [05:07<03:15,  2.33s/it]Validating lr=5e-05, train epoch 1.:  61%|██████    | 131/214 [05:09<03:14,  2.35s/it]Validating lr=5e-05, train epoch 1.:  62%|██████▏   | 132/214 [05:11<03:11,  2.34s/it]Validating lr=5e-05, train epoch 1.:  62%|██████▏   | 133/214 [05:14<03:09,  2.34s/it]Validating lr=5e-05, train epoch 1.:  63%|██████▎   | 134/214 [05:16<03:07,  2.34s/it]Validating lr=5e-05, train epoch 1.:  63%|██████▎   | 135/214 [05:18<03:04,  2.33s/it]Validating lr=5e-05, train epoch 1.:  64%|██████▎   | 136/214 [05:21<03:01,  2.33s/it]Validating lr=5e-05, train epoch 1.:  64%|██████▍   | 137/214 [05:23<02:58,  2.32s/it]Validating lr=5e-05, train epoch 1.:  64%|██████▍   | 138/214 [05:25<02:56,  2.32s/it]Validating lr=5e-05, train epoch 1.:  65%|██████▍   | 139/214 [05:28<02:54,  2.32s/it]Validating lr=5e-05, train epoch 1.:  65%|██████▌   | 140/214 [05:30<02:51,  2.32s/it]Validating lr=5e-05, train epoch 1.:  66%|██████▌   | 141/214 [05:32<02:49,  2.32s/it]Validating lr=5e-05, train epoch 1.:  66%|██████▋   | 142/214 [05:35<02:46,  2.32s/it]Validating lr=5e-05, train epoch 1.:  67%|██████▋   | 143/214 [05:37<02:44,  2.32s/it]Validating lr=5e-05, train epoch 1.:  67%|██████▋   | 144/214 [05:39<02:43,  2.33s/it]Validating lr=5e-05, train epoch 1.:  68%|██████▊   | 145/214 [05:42<02:42,  2.35s/it]Validating lr=5e-05, train epoch 1.:  68%|██████▊   | 146/214 [05:44<02:39,  2.35s/it]Validating lr=5e-05, train epoch 1.:  69%|██████▊   | 147/214 [05:46<02:36,  2.34s/it]Validating lr=5e-05, train epoch 1.:  69%|██████▉   | 148/214 [05:49<02:34,  2.34s/it]Validating lr=5e-05, train epoch 1.:  70%|██████▉   | 149/214 [05:51<02:32,  2.34s/it]Validating lr=5e-05, train epoch 1.:  70%|███████   | 150/214 [05:53<02:30,  2.35s/it]Validating lr=5e-05, train epoch 1.:  71%|███████   | 151/214 [05:56<02:27,  2.34s/it]Validating lr=5e-05, train epoch 1.:  71%|███████   | 152/214 [05:58<02:24,  2.33s/it]Validating lr=5e-05, train epoch 1.:  71%|███████▏  | 153/214 [06:00<02:22,  2.33s/it]Validating lr=5e-05, train epoch 1.:  72%|███████▏  | 154/214 [06:03<02:18,  2.32s/it]Validating lr=5e-05, train epoch 1.:  72%|███████▏  | 155/214 [06:05<02:16,  2.32s/it]Validating lr=5e-05, train epoch 1.:  73%|███████▎  | 156/214 [06:07<02:14,  2.32s/it]Validating lr=5e-05, train epoch 1.:  73%|███████▎  | 157/214 [06:10<02:13,  2.34s/it]Validating lr=5e-05, train epoch 1.:  74%|███████▍  | 158/214 [06:12<02:10,  2.34s/it]Validating lr=5e-05, train epoch 1.:  74%|███████▍  | 159/214 [06:14<02:08,  2.34s/it]Validating lr=5e-05, train epoch 1.:  75%|███████▍  | 160/214 [06:17<02:05,  2.33s/it]Validating lr=5e-05, train epoch 1.:  75%|███████▌  | 161/214 [06:19<02:03,  2.33s/it]Validating lr=5e-05, train epoch 1.:  76%|███████▌  | 162/214 [06:21<02:00,  2.32s/it]Validating lr=5e-05, train epoch 1.:  76%|███████▌  | 163/214 [06:24<01:58,  2.32s/it]Validating lr=5e-05, train epoch 1.:  77%|███████▋  | 164/214 [06:26<01:56,  2.32s/it]Validating lr=5e-05, train epoch 1.:  77%|███████▋  | 165/214 [06:28<01:53,  2.31s/it]Validating lr=5e-05, train epoch 1.:  78%|███████▊  | 166/214 [06:31<01:51,  2.32s/it]Validating lr=5e-05, train epoch 1.:  78%|███████▊  | 167/214 [06:33<01:50,  2.35s/it]Validating lr=5e-05, train epoch 1.:  79%|███████▊  | 168/214 [06:35<01:48,  2.35s/it]Validating lr=5e-05, train epoch 1.:  79%|███████▉  | 169/214 [06:38<01:46,  2.36s/it]Validating lr=5e-05, train epoch 1.:  79%|███████▉  | 170/214 [06:40<01:44,  2.37s/it]Validating lr=5e-05, train epoch 1.:  80%|███████▉  | 171/214 [06:42<01:41,  2.35s/it]Validating lr=5e-05, train epoch 1.:  80%|████████  | 172/214 [06:45<01:38,  2.36s/it]Validating lr=5e-05, train epoch 1.:  81%|████████  | 173/214 [06:47<01:36,  2.35s/it]Validating lr=5e-05, train epoch 1.:  81%|████████▏ | 174/214 [06:49<01:33,  2.35s/it]Validating lr=5e-05, train epoch 1.:  82%|████████▏ | 175/214 [06:52<01:31,  2.35s/it]Validating lr=5e-05, train epoch 1.:  82%|████████▏ | 176/214 [06:54<01:29,  2.35s/it]Validating lr=5e-05, train epoch 1.:  83%|████████▎ | 177/214 [06:56<01:27,  2.35s/it]Validating lr=5e-05, train epoch 1.:  83%|████████▎ | 178/214 [06:59<01:24,  2.35s/it]Validating lr=5e-05, train epoch 1.:  84%|████████▎ | 179/214 [07:01<01:22,  2.35s/it]Validating lr=5e-05, train epoch 1.:  84%|████████▍ | 180/214 [07:03<01:19,  2.34s/it]Validating lr=5e-05, train epoch 1.:  85%|████████▍ | 181/214 [07:06<01:16,  2.33s/it]Validating lr=5e-05, train epoch 1.:  85%|████████▌ | 182/214 [07:08<01:14,  2.34s/it]Validating lr=5e-05, train epoch 1.:  86%|████████▌ | 183/214 [07:10<01:12,  2.34s/it]Validating lr=5e-05, train epoch 1.:  86%|████████▌ | 184/214 [07:13<01:09,  2.33s/it]Validating lr=5e-05, train epoch 1.:  86%|████████▋ | 185/214 [07:15<01:07,  2.34s/it]Validating lr=5e-05, train epoch 1.:  87%|████████▋ | 186/214 [07:17<01:05,  2.33s/it]Validating lr=5e-05, train epoch 1.:  87%|████████▋ | 187/214 [07:20<01:03,  2.34s/it]Validating lr=5e-05, train epoch 1.:  88%|████████▊ | 188/214 [07:22<01:01,  2.35s/it]Validating lr=5e-05, train epoch 1.:  88%|████████▊ | 189/214 [07:25<00:58,  2.35s/it]Validating lr=5e-05, train epoch 1.:  89%|████████▉ | 190/214 [07:27<00:56,  2.35s/it]Validating lr=5e-05, train epoch 1.:  89%|████████▉ | 191/214 [07:29<00:53,  2.33s/it]Validating lr=5e-05, train epoch 1.:  90%|████████▉ | 192/214 [07:32<00:51,  2.33s/it]Validating lr=5e-05, train epoch 1.:  90%|█████████ | 193/214 [07:34<00:48,  2.32s/it]Validating lr=5e-05, train epoch 1.:  91%|█████████ | 194/214 [07:36<00:46,  2.32s/it]Validating lr=5e-05, train epoch 1.:  91%|█████████ | 195/214 [07:38<00:44,  2.32s/it]Validating lr=5e-05, train epoch 1.:  92%|█████████▏| 196/214 [07:41<00:41,  2.31s/it]Validating lr=5e-05, train epoch 1.:  92%|█████████▏| 197/214 [07:43<00:39,  2.31s/it]Validating lr=5e-05, train epoch 1.:  93%|█████████▎| 198/214 [07:45<00:36,  2.31s/it]Validating lr=5e-05, train epoch 1.:  93%|█████████▎| 199/214 [07:48<00:34,  2.31s/it]Validating lr=5e-05, train epoch 1.:  93%|█████████▎| 200/214 [07:50<00:32,  2.30s/it]Validating lr=5e-05, train epoch 1.:  94%|█████████▍| 201/214 [07:52<00:30,  2.33s/it]Validating lr=5e-05, train epoch 1.:  94%|█████████▍| 202/214 [07:55<00:27,  2.32s/it]Validating lr=5e-05, train epoch 1.:  95%|█████████▍| 203/214 [07:57<00:25,  2.32s/it]Validating lr=5e-05, train epoch 1.:  95%|█████████▌| 204/214 [07:59<00:23,  2.32s/it]Validating lr=5e-05, train epoch 1.:  96%|█████████▌| 205/214 [08:02<00:20,  2.32s/it]Validating lr=5e-05, train epoch 1.:  96%|█████████▋| 206/214 [08:04<00:18,  2.32s/it]Validating lr=5e-05, train epoch 1.:  97%|█████████▋| 207/214 [08:06<00:16,  2.32s/it]Validating lr=5e-05, train epoch 1.:  97%|█████████▋| 208/214 [08:09<00:13,  2.31s/it]Validating lr=5e-05, train epoch 1.:  98%|█████████▊| 209/214 [08:11<00:11,  2.31s/it]Validating lr=5e-05, train epoch 1.:  98%|█████████▊| 210/214 [08:13<00:09,  2.36s/it]Validating lr=5e-05, train epoch 1.:  99%|█████████▊| 211/214 [08:16<00:07,  2.35s/it]Validating lr=5e-05, train epoch 1.:  99%|█████████▉| 212/214 [08:18<00:04,  2.34s/it]Validating lr=5e-05, train epoch 1.: 100%|█████████▉| 213/214 [08:20<00:02,  2.36s/it]Validating lr=5e-05, train epoch 1.: 100%|██████████| 214/214 [08:23<00:00,  2.35s/it]Validating lr=5e-05, train epoch 1.: 100%|██████████| 214/214 [08:23<00:00,  2.35s/it]
Evaluating for lr=5e-05:   0%|          | 0/22 [00:00<?, ?it/s]Evaluating for lr=5e-05:   5%|▍         | 1/22 [00:01<00:26,  1.26s/it]Evaluating for lr=5e-05:   9%|▉         | 2/22 [00:02<00:25,  1.27s/it]Evaluating for lr=5e-05:  14%|█▎        | 3/22 [00:03<00:25,  1.32s/it]Evaluating for lr=5e-05:  18%|█▊        | 4/22 [00:05<00:23,  1.32s/it]Evaluating for lr=5e-05:  23%|██▎       | 5/22 [00:06<00:22,  1.34s/it]Evaluating for lr=5e-05:  27%|██▋       | 6/22 [00:07<00:21,  1.33s/it]Evaluating for lr=5e-05:  32%|███▏      | 7/22 [00:09<00:20,  1.35s/it]Evaluating for lr=5e-05:  36%|███▋      | 8/22 [00:10<00:18,  1.33s/it]Evaluating for lr=5e-05:  41%|████      | 9/22 [00:11<00:17,  1.34s/it]Evaluating for lr=5e-05:  45%|████▌     | 10/22 [00:13<00:16,  1.34s/it]Evaluating for lr=5e-05:  50%|█████     | 11/22 [00:14<00:14,  1.35s/it]Evaluating for lr=5e-05:  55%|█████▍    | 12/22 [00:15<00:13,  1.33s/it]Evaluating for lr=5e-05:  59%|█████▉    | 13/22 [00:17<00:11,  1.32s/it]Evaluating for lr=5e-05:  64%|██████▎   | 14/22 [00:18<00:10,  1.32s/it]Evaluating for lr=5e-05:  68%|██████▊   | 15/22 [00:19<00:09,  1.32s/it]Evaluating for lr=5e-05:  73%|███████▎  | 16/22 [00:21<00:07,  1.33s/it]Evaluating for lr=5e-05:  77%|███████▋  | 17/22 [00:22<00:06,  1.32s/it]Evaluating for lr=5e-05:  82%|████████▏ | 18/22 [00:23<00:05,  1.32s/it]Evaluating for lr=5e-05:  86%|████████▋ | 19/22 [00:25<00:03,  1.31s/it]Evaluating for lr=5e-05:  91%|█████████ | 20/22 [00:26<00:02,  1.31s/it]Evaluating for lr=5e-05:  95%|█████████▌| 21/22 [00:27<00:01,  1.31s/it]Evaluating for lr=5e-05: 100%|██████████| 22/22 [00:29<00:00,  1.32s/it]Evaluating for lr=5e-05: 100%|██████████| 22/22 [00:29<00:00,  1.32s/it]
Hyperparameter tuning process completed, using lr 5e-05!
[2025-06-23 13:01:37,165] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2+5f631abc, git-hash=5f631abc, git-branch=HEAD
[2025-06-23 13:01:42,300] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-06-23 13:01:42,302] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-06-23 13:01:42,302] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-06-23 13:01:42,309] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-06-23 13:01:42,309] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-06-23 13:01:42,309] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-06-23 13:01:42,309] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-06-23 13:01:42,309] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2025-06-23 13:01:42,310] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   amp_params ................... False
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1543c7e329d0>
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   dump_state ................... False
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2025-06-23 13:01:42,310] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   global_rank .................. 0
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 5e-05, 'weight_decay': 0.01}
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   pld_params ................... False
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   steps_per_print .............. 100000
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   train_batch_size ............. 2560
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   world_size ................... 20
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2025-06-23 13:01:42,311] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2025-06-23 13:01:42,311] [INFO] [config.py:986:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 128, 
    "train_batch_size": 2.560000e+03, 
    "steps_per_print": 1.000000e+05, 
    "gradient_accumulation_steps": 1, 
    "fp16": {
        "enabled": false
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 5e-05, 
            "weight_decay": 0.01
        }
    }, 
    "comms_logger": {
        "enabled": true, 
        "verbose": false
    }, 
    "zero_optimization": {
        "stage": 0
    }
}
Main training loop, train epoch 0.:   0%|          | 0/214 [00:00<?, ?it/s]Main training loop, train epoch 0.:   0%|          | 1/214 [00:02<08:37,  2.43s/it]Main training loop, train epoch 0.:   1%|          | 2/214 [00:04<08:28,  2.40s/it]Main training loop, train epoch 0.:   1%|▏         | 3/214 [00:07<08:21,  2.38s/it]Main training loop, train epoch 0.:   2%|▏         | 4/214 [00:09<08:15,  2.36s/it]Main training loop, train epoch 0.:   2%|▏         | 5/214 [00:11<08:12,  2.36s/it]Main training loop, train epoch 0.:   3%|▎         | 6/214 [00:14<08:06,  2.34s/it]Main training loop, train epoch 0.:   3%|▎         | 7/214 [00:16<08:01,  2.32s/it]Main training loop, train epoch 0.:   4%|▎         | 8/214 [00:18<08:00,  2.33s/it]Main training loop, train epoch 0.:   4%|▍         | 9/214 [00:21<07:59,  2.34s/it]Main training loop, train epoch 0.:   5%|▍         | 10/214 [00:23<07:57,  2.34s/it]Main training loop, train epoch 0.:   5%|▌         | 11/214 [00:25<07:53,  2.33s/it]Main training loop, train epoch 0.:   6%|▌         | 12/214 [00:28<07:54,  2.35s/it]Main training loop, train epoch 0.:   6%|▌         | 13/214 [00:30<07:53,  2.35s/it]Main training loop, train epoch 0.:   7%|▋         | 14/214 [00:32<07:53,  2.37s/it]Main training loop, train epoch 0.:   7%|▋         | 15/214 [00:35<07:48,  2.36s/it]Main training loop, train epoch 0.:   7%|▋         | 16/214 [00:37<07:45,  2.35s/it]Main training loop, train epoch 0.:   8%|▊         | 17/214 [00:39<07:43,  2.35s/it]Main training loop, train epoch 0.:   8%|▊         | 18/214 [00:42<07:44,  2.37s/it]Main training loop, train epoch 0.:   9%|▉         | 19/214 [00:44<07:38,  2.35s/it]Main training loop, train epoch 0.:   9%|▉         | 20/214 [00:47<07:34,  2.34s/it]Main training loop, train epoch 0.:  10%|▉         | 21/214 [00:49<07:32,  2.34s/it]Main training loop, train epoch 0.:  10%|█         | 22/214 [00:51<07:29,  2.34s/it]Main training loop, train epoch 0.:  11%|█         | 23/214 [00:54<07:31,  2.36s/it]Main training loop, train epoch 0.:  11%|█         | 24/214 [00:56<07:30,  2.37s/it]Main training loop, train epoch 0.:  12%|█▏        | 25/214 [00:58<07:24,  2.35s/it]Main training loop, train epoch 0.:  12%|█▏        | 26/214 [01:01<07:22,  2.36s/it]Main training loop, train epoch 0.:  13%|█▎        | 27/214 [01:03<07:18,  2.34s/it]Main training loop, train epoch 0.:  13%|█▎        | 28/214 [01:05<07:17,  2.35s/it]Main training loop, train epoch 0.:  14%|█▎        | 29/214 [01:08<07:17,  2.36s/it]Main training loop, train epoch 0.:  14%|█▍        | 30/214 [01:10<07:15,  2.36s/it]Main training loop, train epoch 0.:  14%|█▍        | 31/214 [01:12<07:09,  2.35s/it]Main training loop, train epoch 0.:  15%|█▍        | 32/214 [01:15<07:08,  2.35s/it]Main training loop, train epoch 0.:  15%|█▌        | 33/214 [01:17<07:06,  2.36s/it]Main training loop, train epoch 0.:  16%|█▌        | 34/214 [01:19<07:02,  2.35s/it]Main training loop, train epoch 0.:  16%|█▋        | 35/214 [01:22<07:00,  2.35s/it]Main training loop, train epoch 0.:  17%|█▋        | 36/214 [01:24<06:58,  2.35s/it]Main training loop, train epoch 0.:  17%|█▋        | 37/214 [01:27<06:55,  2.34s/it]Main training loop, train epoch 0.:  18%|█▊        | 38/214 [01:29<06:52,  2.34s/it]Main training loop, train epoch 0.:  18%|█▊        | 39/214 [01:31<06:47,  2.33s/it]Main training loop, train epoch 0.:  19%|█▊        | 40/214 [01:33<06:44,  2.32s/it]Main training loop, train epoch 0.:  19%|█▉        | 41/214 [01:36<06:46,  2.35s/it]Main training loop, train epoch 0.:  20%|█▉        | 42/214 [01:38<06:43,  2.35s/it]Main training loop, train epoch 0.:  20%|██        | 43/214 [01:41<06:42,  2.35s/it]Main training loop, train epoch 0.:  21%|██        | 44/214 [01:43<06:38,  2.34s/it]Main training loop, train epoch 0.:  21%|██        | 45/214 [01:45<06:34,  2.33s/it]Main training loop, train epoch 0.:  21%|██▏       | 46/214 [01:48<06:33,  2.34s/it]Main training loop, train epoch 0.:  22%|██▏       | 47/214 [01:50<06:32,  2.35s/it]Main training loop, train epoch 0.:  22%|██▏       | 48/214 [01:52<06:31,  2.36s/it]Main training loop, train epoch 0.:  23%|██▎       | 49/214 [01:55<06:28,  2.36s/it]Main training loop, train epoch 0.:  23%|██▎       | 50/214 [01:57<06:29,  2.37s/it]Main training loop, train epoch 0.:  24%|██▍       | 51/214 [01:59<06:24,  2.36s/it]Main training loop, train epoch 0.:  24%|██▍       | 52/214 [02:02<06:20,  2.35s/it]Main training loop, train epoch 0.:  25%|██▍       | 53/214 [02:04<06:17,  2.35s/it]Main training loop, train epoch 0.:  25%|██▌       | 54/214 [02:06<06:16,  2.35s/it]Main training loop, train epoch 0.:  26%|██▌       | 55/214 [02:09<06:11,  2.34s/it]Main training loop, train epoch 0.:  26%|██▌       | 56/214 [02:11<06:10,  2.35s/it]Main training loop, train epoch 0.:  27%|██▋       | 57/214 [02:13<06:08,  2.35s/it]Main training loop, train epoch 0.:  27%|██▋       | 58/214 [02:16<06:06,  2.35s/it]Main training loop, train epoch 0.:  28%|██▊       | 59/214 [02:18<06:04,  2.35s/it]Main training loop, train epoch 0.:  28%|██▊       | 60/214 [02:21<06:03,  2.36s/it]Main training loop, train epoch 0.:  29%|██▊       | 61/214 [02:23<05:58,  2.34s/it]Main training loop, train epoch 0.:  29%|██▉       | 62/214 [02:25<05:55,  2.34s/it]Main training loop, train epoch 0.:  29%|██▉       | 63/214 [02:27<05:51,  2.33s/it]Main training loop, train epoch 0.:  30%|██▉       | 64/214 [02:30<05:50,  2.34s/it]Main training loop, train epoch 0.:  30%|███       | 65/214 [02:32<05:45,  2.32s/it]Main training loop, train epoch 0.:  31%|███       | 66/214 [02:34<05:45,  2.33s/it]Main training loop, train epoch 0.:  31%|███▏      | 67/214 [02:37<05:46,  2.36s/it]Main training loop, train epoch 0.:  32%|███▏      | 68/214 [02:39<05:44,  2.36s/it]Main training loop, train epoch 0.:  32%|███▏      | 69/214 [02:42<05:43,  2.37s/it]Main training loop, train epoch 0.:  33%|███▎      | 70/214 [02:44<05:41,  2.37s/it]Main training loop, train epoch 0.:  33%|███▎      | 71/214 [02:46<05:34,  2.34s/it]Main training loop, train epoch 0.:  34%|███▎      | 72/214 [02:49<05:32,  2.34s/it]Main training loop, train epoch 0.:  34%|███▍      | 73/214 [02:51<05:28,  2.33s/it]Main training loop, train epoch 0.:  35%|███▍      | 74/214 [02:53<05:29,  2.35s/it]Main training loop, train epoch 0.:  35%|███▌      | 75/214 [02:56<05:27,  2.35s/it]Main training loop, train epoch 0.:  36%|███▌      | 76/214 [02:58<05:22,  2.34s/it]Main training loop, train epoch 0.:  36%|███▌      | 77/214 [03:00<05:20,  2.34s/it]Main training loop, train epoch 0.:  36%|███▋      | 78/214 [03:03<05:18,  2.34s/it]Main training loop, train epoch 0.:  37%|███▋      | 79/214 [03:05<05:15,  2.34s/it]Main training loop, train epoch 0.:  37%|███▋      | 80/214 [03:07<05:12,  2.34s/it]Main training loop, train epoch 0.:  38%|███▊      | 81/214 [03:10<05:09,  2.33s/it]Main training loop, train epoch 0.:  38%|███▊      | 82/214 [03:12<05:07,  2.33s/it]Main training loop, train epoch 0.:  39%|███▉      | 83/214 [03:14<05:03,  2.32s/it]Main training loop, train epoch 0.:  39%|███▉      | 84/214 [03:17<05:05,  2.35s/it]Main training loop, train epoch 0.:  40%|███▉      | 85/214 [03:19<05:01,  2.34s/it]Main training loop, train epoch 0.:  40%|████      | 86/214 [03:21<05:03,  2.37s/it]Main training loop, train epoch 0.:  41%|████      | 87/214 [03:24<04:59,  2.36s/it]Main training loop, train epoch 0.:  41%|████      | 88/214 [03:26<04:57,  2.36s/it]Main training loop, train epoch 0.:  42%|████▏     | 89/214 [03:29<04:53,  2.35s/it]Main training loop, train epoch 0.:  42%|████▏     | 90/214 [03:31<04:49,  2.33s/it]Main training loop, train epoch 0.:  43%|████▎     | 91/214 [03:33<04:48,  2.35s/it]Main training loop, train epoch 0.:  43%|████▎     | 92/214 [03:36<04:47,  2.36s/it]Main training loop, train epoch 0.:  43%|████▎     | 93/214 [03:38<04:45,  2.36s/it]Main training loop, train epoch 0.:  44%|████▍     | 94/214 [03:40<04:41,  2.35s/it]Main training loop, train epoch 0.:  44%|████▍     | 95/214 [03:43<04:39,  2.35s/it]Main training loop, train epoch 0.:  45%|████▍     | 96/214 [03:45<04:36,  2.35s/it]Main training loop, train epoch 0.:  45%|████▌     | 97/214 [03:47<04:34,  2.35s/it]Main training loop, train epoch 0.:  46%|████▌     | 98/214 [03:50<04:31,  2.34s/it]Main training loop, train epoch 0.:  46%|████▋     | 99/214 [03:52<04:29,  2.35s/it]Main training loop, train epoch 0.:  47%|████▋     | 100/214 [03:54<04:29,  2.36s/it]Main training loop, train epoch 0.:  47%|████▋     | 101/214 [03:57<04:25,  2.35s/it]Main training loop, train epoch 0.:  48%|████▊     | 102/214 [03:59<04:24,  2.36s/it]Main training loop, train epoch 0.:  48%|████▊     | 103/214 [04:01<04:22,  2.37s/it]Main training loop, train epoch 0.:  49%|████▊     | 104/214 [04:04<04:20,  2.37s/it]Main training loop, train epoch 0.:  49%|████▉     | 105/214 [04:06<04:17,  2.36s/it]Main training loop, train epoch 0.:  50%|████▉     | 106/214 [04:09<04:14,  2.36s/it]Main training loop, train epoch 0.:  50%|█████     | 107/214 [04:11<04:10,  2.34s/it]Main training loop, train epoch 0.:  50%|█████     | 108/214 [04:13<04:08,  2.35s/it]Main training loop, train epoch 0.:  51%|█████     | 109/214 [04:16<04:05,  2.34s/it]Main training loop, train epoch 0.:  51%|█████▏    | 110/214 [04:18<04:03,  2.34s/it]Main training loop, train epoch 0.:  52%|█████▏    | 111/214 [04:20<04:03,  2.36s/it]Main training loop, train epoch 0.:  52%|█████▏    | 112/214 [04:23<04:01,  2.37s/it]Main training loop, train epoch 0.:  53%|█████▎    | 113/214 [04:25<03:58,  2.36s/it]Main training loop, train epoch 0.:  53%|█████▎    | 114/214 [04:27<03:55,  2.35s/it]Main training loop, train epoch 0.:  54%|█████▎    | 115/214 [04:30<03:51,  2.34s/it]Main training loop, train epoch 0.:  54%|█████▍    | 116/214 [04:32<03:48,  2.33s/it]Main training loop, train epoch 0.:  55%|█████▍    | 117/214 [04:34<03:48,  2.35s/it]Main training loop, train epoch 0.:  55%|█████▌    | 118/214 [04:37<03:45,  2.35s/it]Main training loop, train epoch 0.:  56%|█████▌    | 119/214 [04:39<03:43,  2.35s/it]Main training loop, train epoch 0.:  56%|█████▌    | 120/214 [04:41<03:42,  2.37s/it]Main training loop, train epoch 0.:  57%|█████▋    | 121/214 [04:44<03:37,  2.34s/it]Main training loop, train epoch 0.:  57%|█████▋    | 122/214 [04:46<03:36,  2.35s/it]Main training loop, train epoch 0.:  57%|█████▋    | 123/214 [04:48<03:33,  2.34s/it]Main training loop, train epoch 0.:  58%|█████▊    | 124/214 [04:51<03:30,  2.33s/it]Main training loop, train epoch 0.:  58%|█████▊    | 125/214 [04:53<03:27,  2.33s/it]Main training loop, train epoch 0.:  59%|█████▉    | 126/214 [04:55<03:26,  2.34s/it]Main training loop, train epoch 0.:  59%|█████▉    | 127/214 [04:58<03:25,  2.36s/it]Main training loop, train epoch 0.:  60%|█████▉    | 128/214 [05:00<03:21,  2.34s/it]Main training loop, train epoch 0.:  60%|██████    | 129/214 [05:02<03:18,  2.33s/it]Main training loop, train epoch 0.:  61%|██████    | 130/214 [05:05<03:16,  2.34s/it]Main training loop, train epoch 0.:  61%|██████    | 131/214 [05:07<03:15,  2.36s/it]Main training loop, train epoch 0.:  62%|██████▏   | 132/214 [05:10<03:13,  2.36s/it]Main training loop, train epoch 0.:  62%|██████▏   | 133/214 [05:12<03:10,  2.35s/it]Main training loop, train epoch 0.:  63%|██████▎   | 134/214 [05:14<03:06,  2.33s/it]Main training loop, train epoch 0.:  63%|██████▎   | 135/214 [05:16<03:03,  2.32s/it]Main training loop, train epoch 0.:  64%|██████▎   | 136/214 [05:19<03:02,  2.34s/it]Main training loop, train epoch 0.:  64%|██████▍   | 137/214 [05:21<02:59,  2.33s/it]Main training loop, train epoch 0.:  64%|██████▍   | 138/214 [05:24<02:58,  2.35s/it]Main training loop, train epoch 0.:  65%|██████▍   | 139/214 [05:26<02:55,  2.34s/it]Main training loop, train epoch 0.:  65%|██████▌   | 140/214 [05:28<02:52,  2.34s/it]Main training loop, train epoch 0.:  66%|██████▌   | 141/214 [05:31<02:50,  2.34s/it]Main training loop, train epoch 0.:  66%|██████▋   | 142/214 [05:33<02:48,  2.34s/it]Main training loop, train epoch 0.:  67%|██████▋   | 143/214 [05:35<02:47,  2.36s/it]Main training loop, train epoch 0.:  67%|██████▋   | 144/214 [05:38<02:45,  2.36s/it]Main training loop, train epoch 0.:  68%|██████▊   | 145/214 [05:40<02:43,  2.37s/it]Main training loop, train epoch 0.:  68%|██████▊   | 146/214 [05:42<02:40,  2.36s/it]Main training loop, train epoch 0.:  69%|██████▊   | 147/214 [05:45<02:37,  2.35s/it]Main training loop, train epoch 0.:  69%|██████▉   | 148/214 [05:47<02:36,  2.37s/it]Main training loop, train epoch 0.:  70%|██████▉   | 149/214 [05:49<02:33,  2.36s/it]Main training loop, train epoch 0.:  70%|███████   | 150/214 [05:52<02:31,  2.36s/it]Main training loop, train epoch 0.:  71%|███████   | 151/214 [05:54<02:27,  2.34s/it]Main training loop, train epoch 0.:  71%|███████   | 152/214 [05:57<02:25,  2.35s/it]Main training loop, train epoch 0.:  71%|███████▏  | 153/214 [05:59<02:22,  2.34s/it]Main training loop, train epoch 0.:  72%|███████▏  | 154/214 [06:01<02:19,  2.32s/it]Main training loop, train epoch 0.:  72%|███████▏  | 155/214 [06:03<02:17,  2.33s/it]Main training loop, train epoch 0.:  73%|███████▎  | 156/214 [06:06<02:15,  2.34s/it]Main training loop, train epoch 0.:  73%|███████▎  | 157/214 [06:08<02:13,  2.33s/it]Main training loop, train epoch 0.:  74%|███████▍  | 158/214 [06:10<02:10,  2.32s/it]Main training loop, train epoch 0.:  74%|███████▍  | 159/214 [06:13<02:07,  2.32s/it]Main training loop, train epoch 0.:  75%|███████▍  | 160/214 [06:15<02:05,  2.33s/it]Main training loop, train epoch 0.:  75%|███████▌  | 161/214 [06:17<02:04,  2.35s/it]Main training loop, train epoch 0.:  76%|███████▌  | 162/214 [06:20<02:02,  2.35s/it]Main training loop, train epoch 0.:  76%|███████▌  | 163/214 [06:22<01:59,  2.33s/it]Main training loop, train epoch 0.:  77%|███████▋  | 164/214 [06:25<01:57,  2.36s/it]Main training loop, train epoch 0.:  77%|███████▋  | 165/214 [06:27<01:55,  2.36s/it]Main training loop, train epoch 0.:  78%|███████▊  | 166/214 [06:29<01:54,  2.38s/it]Main training loop, train epoch 0.:  78%|███████▊  | 167/214 [06:32<01:52,  2.39s/it]Main training loop, train epoch 0.:  79%|███████▊  | 168/214 [06:34<01:48,  2.36s/it]Main training loop, train epoch 0.:  79%|███████▉  | 169/214 [06:36<01:46,  2.37s/it]Main training loop, train epoch 0.:  79%|███████▉  | 170/214 [06:39<01:44,  2.38s/it]Main training loop, train epoch 0.:  80%|███████▉  | 171/214 [06:41<01:41,  2.36s/it]Main training loop, train epoch 0.:  80%|████████  | 172/214 [06:43<01:38,  2.34s/it]Main training loop, train epoch 0.:  81%|████████  | 173/214 [06:46<01:35,  2.34s/it]Main training loop, train epoch 0.:  81%|████████▏ | 174/214 [06:48<01:33,  2.34s/it]Main training loop, train epoch 0.:  82%|████████▏ | 175/214 [06:51<01:31,  2.35s/it]Main training loop, train epoch 0.:  82%|████████▏ | 176/214 [06:53<01:29,  2.37s/it]Main training loop, train epoch 0.:  83%|████████▎ | 177/214 [06:55<01:27,  2.36s/it]Main training loop, train epoch 0.:  83%|████████▎ | 178/214 [06:58<01:24,  2.36s/it]Main training loop, train epoch 0.:  84%|████████▎ | 179/214 [07:00<01:22,  2.35s/it]Main training loop, train epoch 0.:  84%|████████▍ | 180/214 [07:02<01:20,  2.36s/it]Main training loop, train epoch 0.:  85%|████████▍ | 181/214 [07:05<01:17,  2.34s/it]Main training loop, train epoch 0.:  85%|████████▌ | 182/214 [07:07<01:14,  2.34s/it]Main training loop, train epoch 0.:  86%|████████▌ | 183/214 [07:09<01:12,  2.34s/it]Main training loop, train epoch 0.:  86%|████████▌ | 184/214 [07:12<01:10,  2.35s/it]Main training loop, train epoch 0.:  86%|████████▋ | 185/214 [07:14<01:08,  2.36s/it]Main training loop, train epoch 0.:  87%|████████▋ | 186/214 [07:16<01:06,  2.37s/it]Main training loop, train epoch 0.:  87%|████████▋ | 187/214 [07:19<01:04,  2.37s/it]Main training loop, train epoch 0.:  88%|████████▊ | 188/214 [07:21<01:01,  2.37s/it]Main training loop, train epoch 0.:  88%|████████▊ | 189/214 [07:24<00:58,  2.36s/it]Main training loop, train epoch 0.:  89%|████████▉ | 190/214 [07:26<00:56,  2.35s/it]Main training loop, train epoch 0.:  89%|████████▉ | 191/214 [07:28<00:53,  2.34s/it]Main training loop, train epoch 0.:  90%|████████▉ | 192/214 [07:31<00:51,  2.35s/it]Main training loop, train epoch 0.:  90%|█████████ | 193/214 [07:33<00:49,  2.36s/it]Main training loop, train epoch 0.:  91%|█████████ | 194/214 [07:35<00:47,  2.37s/it]Main training loop, train epoch 0.:  91%|█████████ | 195/214 [07:38<00:44,  2.35s/it]Main training loop, train epoch 0.:  92%|█████████▏| 196/214 [07:40<00:42,  2.36s/it]Main training loop, train epoch 0.:  92%|█████████▏| 197/214 [07:42<00:39,  2.35s/it]Main training loop, train epoch 0.:  93%|█████████▎| 198/214 [07:45<00:37,  2.36s/it]Main training loop, train epoch 0.:  93%|█████████▎| 199/214 [07:47<00:35,  2.35s/it]Main training loop, train epoch 0.:  93%|█████████▎| 200/214 [07:49<00:33,  2.37s/it]Main training loop, train epoch 0.:  94%|█████████▍| 201/214 [07:52<00:30,  2.36s/it]Main training loop, train epoch 0.:  94%|█████████▍| 202/214 [07:54<00:28,  2.38s/it]Main training loop, train epoch 0.:  95%|█████████▍| 203/214 [07:57<00:26,  2.40s/it]Main training loop, train epoch 0.:  95%|█████████▌| 204/214 [07:59<00:23,  2.36s/it]Main training loop, train epoch 0.:  96%|█████████▌| 205/214 [08:01<00:21,  2.35s/it]Main training loop, train epoch 0.:  96%|█████████▋| 206/214 [08:04<00:18,  2.35s/it]Main training loop, train epoch 0.:  97%|█████████▋| 207/214 [08:06<00:16,  2.34s/it]Main training loop, train epoch 0.:  97%|█████████▋| 208/214 [08:08<00:14,  2.35s/it]Main training loop, train epoch 0.:  98%|█████████▊| 209/214 [08:11<00:11,  2.34s/it]Main training loop, train epoch 0.:  98%|█████████▊| 210/214 [08:13<00:09,  2.34s/it]Main training loop, train epoch 0.:  99%|█████████▊| 211/214 [08:15<00:07,  2.34s/it]Main training loop, train epoch 0.:  99%|█████████▉| 212/214 [08:18<00:04,  2.34s/it]Main training loop, train epoch 0.: 100%|█████████▉| 213/214 [08:20<00:02,  2.32s/it]Main training loop, train epoch 0.: 100%|██████████| 214/214 [08:22<00:00,  2.34s/it]Main training loop, train epoch 0.: 100%|██████████| 214/214 [08:22<00:00,  2.35s/it]
Main training loop, train epoch 1.:   0%|          | 0/214 [00:00<?, ?it/s]Main training loop, train epoch 1.:   0%|          | 1/214 [00:02<08:21,  2.35s/it]Main training loop, train epoch 1.:   1%|          | 2/214 [00:04<08:12,  2.32s/it]Main training loop, train epoch 1.:   1%|▏         | 3/214 [00:07<08:12,  2.34s/it]Main training loop, train epoch 1.:   2%|▏         | 4/214 [00:09<08:13,  2.35s/it]Main training loop, train epoch 1.:   2%|▏         | 5/214 [00:11<08:08,  2.34s/it]Main training loop, train epoch 1.:   3%|▎         | 6/214 [00:13<08:04,  2.33s/it]Main training loop, train epoch 1.:   3%|▎         | 7/214 [00:16<08:07,  2.35s/it]Main training loop, train epoch 1.:   4%|▎         | 8/214 [00:18<08:03,  2.35s/it]Main training loop, train epoch 1.:   4%|▍         | 9/214 [00:21<07:58,  2.33s/it]Main training loop, train epoch 1.:   5%|▍         | 10/214 [00:23<07:53,  2.32s/it]Main training loop, train epoch 1.:   5%|▌         | 11/214 [00:25<07:50,  2.32s/it]Main training loop, train epoch 1.:   6%|▌         | 12/214 [00:27<07:48,  2.32s/it]Main training loop, train epoch 1.:   6%|▌         | 13/214 [00:30<07:46,  2.32s/it]Main training loop, train epoch 1.:   7%|▋         | 14/214 [00:32<07:45,  2.33s/it]Main training loop, train epoch 1.:   7%|▋         | 15/214 [00:34<07:41,  2.32s/it]Main training loop, train epoch 1.:   7%|▋         | 16/214 [00:37<07:44,  2.35s/it]Main training loop, train epoch 1.:   8%|▊         | 17/214 [00:39<07:44,  2.36s/it]Main training loop, train epoch 1.:   8%|▊         | 18/214 [00:42<07:38,  2.34s/it]Main training loop, train epoch 1.:   9%|▉         | 19/214 [00:44<07:34,  2.33s/it]Main training loop, train epoch 1.:   9%|▉         | 20/214 [00:46<07:30,  2.32s/it]Main training loop, train epoch 1.:  10%|▉         | 21/214 [00:49<07:33,  2.35s/it]Main training loop, train epoch 1.:  10%|█         | 22/214 [00:51<07:31,  2.35s/it]Main training loop, train epoch 1.:  11%|█         | 23/214 [00:53<07:32,  2.37s/it]Main training loop, train epoch 1.:  11%|█         | 24/214 [00:56<07:27,  2.35s/it]Main training loop, train epoch 1.:  12%|█▏        | 25/214 [00:58<07:23,  2.35s/it]Main training loop, train epoch 1.:  12%|█▏        | 26/214 [01:00<07:21,  2.35s/it]Main training loop, train epoch 1.:  13%|█▎        | 27/214 [01:03<07:19,  2.35s/it]Main training loop, train epoch 1.:  13%|█▎        | 28/214 [01:05<07:15,  2.34s/it]Main training loop, train epoch 1.:  14%|█▎        | 29/214 [01:07<07:17,  2.36s/it]Main training loop, train epoch 1.:  14%|█▍        | 30/214 [01:10<07:10,  2.34s/it]Main training loop, train epoch 1.:  14%|█▍        | 31/214 [01:12<07:10,  2.35s/it]Main training loop, train epoch 1.:  15%|█▍        | 32/214 [01:14<07:10,  2.37s/it]Main training loop, train epoch 1.:  15%|█▌        | 33/214 [01:17<07:06,  2.36s/it]Main training loop, train epoch 1.:  16%|█▌        | 34/214 [01:19<07:03,  2.35s/it]Main training loop, train epoch 1.:  16%|█▋        | 35/214 [01:21<06:59,  2.35s/it]Main training loop, train epoch 1.:  17%|█▋        | 36/214 [01:24<06:54,  2.33s/it]Main training loop, train epoch 1.:  17%|█▋        | 37/214 [01:26<06:54,  2.34s/it]Main training loop, train epoch 1.:  18%|█▊        | 38/214 [01:28<06:50,  2.33s/it]Main training loop, train epoch 1.:  18%|█▊        | 39/214 [01:31<06:48,  2.33s/it]Main training loop, train epoch 1.:  19%|█▊        | 40/214 [01:33<06:47,  2.34s/it]Main training loop, train epoch 1.:  19%|█▉        | 41/214 [01:36<06:47,  2.36s/it]Main training loop, train epoch 1.:  20%|█▉        | 42/214 [01:38<06:41,  2.34s/it]Main training loop, train epoch 1.:  20%|██        | 43/214 [01:40<06:37,  2.32s/it]Main training loop, train epoch 1.:  21%|██        | 44/214 [01:42<06:36,  2.33s/it]Main training loop, train epoch 1.:  21%|██        | 45/214 [01:45<06:34,  2.33s/it]Main training loop, train epoch 1.:  21%|██▏       | 46/214 [01:47<06:30,  2.32s/it]Main training loop, train epoch 1.:  22%|██▏       | 47/214 [01:49<06:30,  2.34s/it]Main training loop, train epoch 1.:  22%|██▏       | 48/214 [01:52<06:27,  2.34s/it]Main training loop, train epoch 1.:  23%|██▎       | 49/214 [01:54<06:28,  2.36s/it]Main training loop, train epoch 1.:  23%|██▎       | 50/214 [01:57<06:25,  2.35s/it]Main training loop, train epoch 1.:  24%|██▍       | 51/214 [01:59<06:24,  2.36s/it]Main training loop, train epoch 1.:  24%|██▍       | 52/214 [02:01<06:22,  2.36s/it]Main training loop, train epoch 1.:  25%|██▍       | 53/214 [02:04<06:19,  2.36s/it]Main training loop, train epoch 1.:  25%|██▌       | 54/214 [02:06<06:14,  2.34s/it]Main training loop, train epoch 1.:  26%|██▌       | 55/214 [02:08<06:14,  2.36s/it]Main training loop, train epoch 1.:  26%|██▌       | 56/214 [02:11<06:12,  2.36s/it]Main training loop, train epoch 1.:  27%|██▋       | 57/214 [02:13<06:08,  2.35s/it]Main training loop, train epoch 1.:  27%|██▋       | 58/214 [02:15<06:03,  2.33s/it]Main training loop, train epoch 1.:  28%|██▊       | 59/214 [02:18<06:02,  2.34s/it]Main training loop, train epoch 1.:  28%|██▊       | 60/214 [02:20<05:58,  2.33s/it]Main training loop, train epoch 1.:  29%|██▊       | 61/214 [02:22<05:59,  2.35s/it]Main training loop, train epoch 1.:  29%|██▉       | 62/214 [02:25<05:56,  2.35s/it]Main training loop, train epoch 1.:  29%|██▉       | 63/214 [02:27<05:51,  2.33s/it]Main training loop, train epoch 1.:  30%|██▉       | 64/214 [02:29<05:48,  2.32s/it]Main training loop, train epoch 1.:  30%|███       | 65/214 [02:32<05:49,  2.34s/it]Main training loop, train epoch 1.:  31%|███       | 66/214 [02:34<05:44,  2.33s/it]Main training loop, train epoch 1.:  31%|███▏      | 67/214 [02:36<05:40,  2.32s/it]Main training loop, train epoch 1.:  32%|███▏      | 68/214 [02:39<05:40,  2.33s/it]Main training loop, train epoch 1.:  32%|███▏      | 69/214 [02:41<05:37,  2.33s/it]Main training loop, train epoch 1.:  33%|███▎      | 70/214 [02:43<05:35,  2.33s/it]Main training loop, train epoch 1.:  33%|███▎      | 71/214 [02:46<05:32,  2.33s/it]Main training loop, train epoch 1.:  34%|███▎      | 72/214 [02:48<05:30,  2.32s/it]Main training loop, train epoch 1.:  34%|███▍      | 73/214 [02:50<05:29,  2.34s/it]Main training loop, train epoch 1.:  35%|███▍      | 74/214 [02:53<05:27,  2.34s/it]Main training loop, train epoch 1.:  35%|███▌      | 75/214 [02:55<05:25,  2.34s/it]Main training loop, train epoch 1.:  36%|███▌      | 76/214 [02:57<05:21,  2.33s/it]Main training loop, train epoch 1.:  36%|███▌      | 77/214 [03:00<05:22,  2.36s/it]Main training loop, train epoch 1.:  36%|███▋      | 78/214 [03:02<05:18,  2.35s/it]Main training loop, train epoch 1.:  37%|███▋      | 79/214 [03:04<05:17,  2.35s/it]Main training loop, train epoch 1.:  37%|███▋      | 80/214 [03:07<05:13,  2.34s/it]Main training loop, train epoch 1.:  38%|███▊      | 81/214 [03:09<05:14,  2.36s/it]Main training loop, train epoch 1.:  38%|███▊      | 82/214 [03:12<05:13,  2.37s/it]Main training loop, train epoch 1.:  39%|███▉      | 83/214 [03:14<05:07,  2.35s/it]Main training loop, train epoch 1.:  39%|███▉      | 84/214 [03:16<05:04,  2.34s/it]Main training loop, train epoch 1.:  40%|███▉      | 85/214 [03:18<05:01,  2.34s/it]Main training loop, train epoch 1.:  40%|████      | 86/214 [03:21<04:59,  2.34s/it]Main training loop, train epoch 1.:  41%|████      | 87/214 [03:23<05:00,  2.36s/it]Main training loop, train epoch 1.:  41%|████      | 88/214 [03:26<04:56,  2.36s/it]Main training loop, train epoch 1.:  42%|████▏     | 89/214 [03:28<04:55,  2.36s/it]Main training loop, train epoch 1.:  42%|████▏     | 90/214 [03:30<04:50,  2.34s/it]Main training loop, train epoch 1.:  43%|████▎     | 91/214 [03:33<04:48,  2.35s/it]Main training loop, train epoch 1.:  43%|████▎     | 92/214 [03:35<04:44,  2.33s/it]Main training loop, train epoch 1.:  43%|████▎     | 93/214 [03:37<04:42,  2.33s/it]Main training loop, train epoch 1.:  44%|████▍     | 94/214 [03:40<04:39,  2.33s/it]Main training loop, train epoch 1.:  44%|████▍     | 95/214 [03:42<04:37,  2.33s/it]Main training loop, train epoch 1.:  45%|████▍     | 96/214 [03:44<04:35,  2.33s/it]Main training loop, train epoch 1.:  45%|████▌     | 97/214 [03:47<04:34,  2.34s/it]Main training loop, train epoch 1.:  46%|████▌     | 98/214 [03:49<04:31,  2.34s/it]Main training loop, train epoch 1.:  46%|████▋     | 99/214 [03:51<04:29,  2.35s/it]Main training loop, train epoch 1.:  47%|████▋     | 100/214 [03:54<04:26,  2.34s/it]Main training loop, train epoch 1.:  47%|████▋     | 101/214 [03:56<04:25,  2.35s/it]Main training loop, train epoch 1.:  48%|████▊     | 102/214 [03:58<04:22,  2.34s/it]Main training loop, train epoch 1.:  48%|████▊     | 103/214 [04:01<04:19,  2.33s/it]Main training loop, train epoch 1.:  49%|████▊     | 104/214 [04:03<04:15,  2.33s/it]Main training loop, train epoch 1.:  49%|████▉     | 105/214 [04:05<04:15,  2.34s/it]Main training loop, train epoch 1.:  50%|████▉     | 106/214 [04:08<04:11,  2.33s/it]Main training loop, train epoch 1.:  50%|█████     | 107/214 [04:10<04:10,  2.34s/it]Main training loop, train epoch 1.:  50%|█████     | 108/214 [04:12<04:07,  2.33s/it]Main training loop, train epoch 1.:  51%|█████     | 109/214 [04:15<04:04,  2.33s/it]Main training loop, train epoch 1.:  51%|█████▏    | 110/214 [04:17<04:02,  2.33s/it]Main training loop, train epoch 1.:  52%|█████▏    | 111/214 [04:19<04:00,  2.34s/it]Main training loop, train epoch 1.:  52%|█████▏    | 112/214 [04:22<03:57,  2.33s/it]Main training loop, train epoch 1.:  53%|█████▎    | 113/214 [04:24<03:56,  2.35s/it]Main training loop, train epoch 1.:  53%|█████▎    | 114/214 [04:26<03:55,  2.36s/it]Main training loop, train epoch 1.:  54%|█████▎    | 115/214 [04:29<03:53,  2.35s/it]Main training loop, train epoch 1.:  54%|█████▍    | 116/214 [04:31<03:50,  2.35s/it]Main training loop, train epoch 1.:  55%|█████▍    | 117/214 [04:33<03:49,  2.36s/it]Main training loop, train epoch 1.:  55%|█████▌    | 118/214 [04:36<03:44,  2.34s/it]Main training loop, train epoch 1.:  56%|█████▌    | 119/214 [04:38<03:43,  2.35s/it]Main training loop, train epoch 1.:  56%|█████▌    | 120/214 [04:41<03:40,  2.35s/it]Main training loop, train epoch 1.:  57%|█████▋    | 121/214 [04:43<03:38,  2.35s/it]Main training loop, train epoch 1.:  57%|█████▋    | 122/214 [04:45<03:35,  2.35s/it]Main training loop, train epoch 1.:  57%|█████▋    | 123/214 [04:48<03:33,  2.35s/it]Main training loop, train epoch 1.:  58%|█████▊    | 124/214 [04:50<03:30,  2.34s/it]Main training loop, train epoch 1.:  58%|█████▊    | 125/214 [04:52<03:28,  2.34s/it]Main training loop, train epoch 1.:  59%|█████▉    | 126/214 [04:55<03:27,  2.36s/it]Main training loop, train epoch 1.:  59%|█████▉    | 127/214 [04:57<03:25,  2.36s/it]Main training loop, train epoch 1.:  60%|█████▉    | 128/214 [04:59<03:22,  2.35s/it]Main training loop, train epoch 1.:  60%|██████    | 129/214 [05:02<03:19,  2.34s/it]Main training loop, train epoch 1.:  61%|██████    | 130/214 [05:04<03:17,  2.35s/it]Main training loop, train epoch 1.:  61%|██████    | 131/214 [05:06<03:15,  2.35s/it]Main training loop, train epoch 1.:  62%|██████▏   | 132/214 [05:09<03:14,  2.37s/it]Main training loop, train epoch 1.:  62%|██████▏   | 133/214 [05:11<03:10,  2.35s/it]Main training loop, train epoch 1.:  63%|██████▎   | 134/214 [05:13<03:07,  2.34s/it]Main training loop, train epoch 1.:  63%|██████▎   | 135/214 [05:16<03:04,  2.33s/it]Main training loop, train epoch 1.:  64%|██████▎   | 136/214 [05:18<03:01,  2.33s/it]Main training loop, train epoch 1.:  64%|██████▍   | 137/214 [05:20<03:00,  2.35s/it]Main training loop, train epoch 1.:  64%|██████▍   | 138/214 [05:23<02:57,  2.34s/it]Main training loop, train epoch 1.:  65%|██████▍   | 139/214 [05:25<02:54,  2.32s/it]Main training loop, train epoch 1.:  65%|██████▌   | 140/214 [05:27<02:52,  2.33s/it]Main training loop, train epoch 1.:  66%|██████▌   | 141/214 [05:30<02:49,  2.33s/it]Main training loop, train epoch 1.:  66%|██████▋   | 142/214 [05:32<02:47,  2.32s/it]Main training loop, train epoch 1.:  67%|██████▋   | 143/214 [05:34<02:44,  2.32s/it]Main training loop, train epoch 1.:  67%|██████▋   | 144/214 [05:37<02:42,  2.32s/it]Main training loop, train epoch 1.:  68%|██████▊   | 145/214 [05:39<02:41,  2.34s/it]Main training loop, train epoch 1.:  68%|██████▊   | 146/214 [05:41<02:39,  2.34s/it]Main training loop, train epoch 1.:  69%|██████▊   | 147/214 [05:44<02:37,  2.35s/it]Main training loop, train epoch 1.:  69%|██████▉   | 148/214 [05:46<02:34,  2.34s/it]Main training loop, train epoch 1.:  70%|██████▉   | 149/214 [05:48<02:32,  2.34s/it]Main training loop, train epoch 1.:  70%|███████   | 150/214 [05:51<02:29,  2.33s/it]Main training loop, train epoch 1.:  71%|███████   | 151/214 [05:53<02:26,  2.33s/it]Main training loop, train epoch 1.:  71%|███████   | 152/214 [05:55<02:24,  2.33s/it]Main training loop, train epoch 1.:  71%|███████▏  | 153/214 [05:58<02:23,  2.34s/it]Main training loop, train epoch 1.:  72%|███████▏  | 154/214 [06:00<02:20,  2.34s/it]Main training loop, train epoch 1.:  72%|███████▏  | 155/214 [06:02<02:17,  2.33s/it]Main training loop, train epoch 1.:  73%|███████▎  | 156/214 [06:05<02:14,  2.33s/it]Main training loop, train epoch 1.:  73%|███████▎  | 157/214 [06:07<02:13,  2.34s/it]Main training loop, train epoch 1.:  74%|███████▍  | 158/214 [06:09<02:10,  2.33s/it]Main training loop, train epoch 1.:  74%|███████▍  | 159/214 [06:12<02:09,  2.35s/it]Main training loop, train epoch 1.:  75%|███████▍  | 160/214 [06:14<02:05,  2.33s/it]Main training loop, train epoch 1.:  75%|███████▌  | 161/214 [06:16<02:04,  2.35s/it]Main training loop, train epoch 1.:  76%|███████▌  | 162/214 [06:19<02:01,  2.34s/it]Main training loop, train epoch 1.:  76%|███████▌  | 163/214 [06:21<01:58,  2.33s/it]Main training loop, train epoch 1.:  77%|███████▋  | 164/214 [06:23<01:56,  2.33s/it]Main training loop, train epoch 1.:  77%|███████▋  | 165/214 [06:26<01:53,  2.32s/it]Main training loop, train epoch 1.:  78%|███████▊  | 166/214 [06:28<01:52,  2.34s/it]Main training loop, train epoch 1.:  78%|███████▊  | 167/214 [06:30<01:49,  2.33s/it]Main training loop, train epoch 1.:  79%|███████▊  | 168/214 [06:33<01:47,  2.34s/it]Main training loop, train epoch 1.:  79%|███████▉  | 169/214 [06:35<01:44,  2.33s/it]Main training loop, train epoch 1.:  79%|███████▉  | 170/214 [06:37<01:42,  2.33s/it]Main training loop, train epoch 1.:  80%|███████▉  | 171/214 [06:40<01:41,  2.35s/it]Main training loop, train epoch 1.:  80%|████████  | 172/214 [06:42<01:38,  2.35s/it]Main training loop, train epoch 1.:  81%|████████  | 173/214 [06:44<01:35,  2.34s/it]Main training loop, train epoch 1.:  81%|████████▏ | 174/214 [06:47<01:34,  2.36s/it]Main training loop, train epoch 1.:  82%|████████▏ | 175/214 [06:49<01:32,  2.36s/it]Main training loop, train epoch 1.:  82%|████████▏ | 176/214 [06:52<01:29,  2.34s/it]Main training loop, train epoch 1.:  83%|████████▎ | 177/214 [06:54<01:26,  2.35s/it]Main training loop, train epoch 1.:  83%|████████▎ | 178/214 [06:56<01:25,  2.37s/it]Main training loop, train epoch 1.:  84%|████████▎ | 179/214 [06:59<01:23,  2.37s/it]Main training loop, train epoch 1.:  84%|████████▍ | 180/214 [07:01<01:20,  2.37s/it]Main training loop, train epoch 1.:  85%|████████▍ | 181/214 [07:03<01:18,  2.37s/it]Main training loop, train epoch 1.:  85%|████████▌ | 182/214 [07:06<01:15,  2.36s/it]Main training loop, train epoch 1.:  86%|████████▌ | 183/214 [07:08<01:13,  2.38s/it]Main training loop, train epoch 1.:  86%|████████▌ | 184/214 [07:11<01:10,  2.36s/it]Main training loop, train epoch 1.:  86%|████████▋ | 185/214 [07:13<01:08,  2.36s/it]Main training loop, train epoch 1.:  87%|████████▋ | 186/214 [07:15<01:06,  2.37s/it]Main training loop, train epoch 1.:  87%|████████▋ | 187/214 [07:18<01:04,  2.38s/it]Main training loop, train epoch 1.:  88%|████████▊ | 188/214 [07:20<01:01,  2.36s/it]Main training loop, train epoch 1.:  88%|████████▊ | 189/214 [07:22<00:58,  2.35s/it]Main training loop, train epoch 1.:  89%|████████▉ | 190/214 [07:25<00:56,  2.34s/it]Main training loop, train epoch 1.:  89%|████████▉ | 191/214 [07:27<00:54,  2.36s/it]Main training loop, train epoch 1.:  90%|████████▉ | 192/214 [07:29<00:51,  2.34s/it]Main training loop, train epoch 1.:  90%|█████████ | 193/214 [07:32<00:48,  2.33s/it]Main training loop, train epoch 1.:  91%|█████████ | 194/214 [07:34<00:46,  2.33s/it]Main training loop, train epoch 1.:  91%|█████████ | 195/214 [07:36<00:44,  2.33s/it]Main training loop, train epoch 1.:  92%|█████████▏| 196/214 [07:39<00:41,  2.32s/it]Main training loop, train epoch 1.:  92%|█████████▏| 197/214 [07:41<00:39,  2.33s/it]Main training loop, train epoch 1.:  93%|█████████▎| 198/214 [07:43<00:37,  2.32s/it]Main training loop, train epoch 1.:  93%|█████████▎| 199/214 [07:46<00:34,  2.33s/it]Main training loop, train epoch 1.:  93%|█████████▎| 200/214 [07:48<00:32,  2.32s/it]Main training loop, train epoch 1.:  94%|█████████▍| 201/214 [07:50<00:30,  2.34s/it]Main training loop, train epoch 1.:  94%|█████████▍| 202/214 [07:53<00:27,  2.33s/it]Main training loop, train epoch 1.:  95%|█████████▍| 203/214 [07:55<00:25,  2.33s/it]Main training loop, train epoch 1.:  95%|█████████▌| 204/214 [07:57<00:23,  2.34s/it]Main training loop, train epoch 1.:  96%|█████████▌| 205/214 [08:00<00:20,  2.33s/it]Main training loop, train epoch 1.:  96%|█████████▋| 206/214 [08:02<00:18,  2.32s/it]Main training loop, train epoch 1.:  97%|█████████▋| 207/214 [08:04<00:16,  2.32s/it]Main training loop, train epoch 1.:  97%|█████████▋| 208/214 [08:07<00:13,  2.32s/it]Main training loop, train epoch 1.:  98%|█████████▊| 209/214 [08:09<00:11,  2.33s/it]Main training loop, train epoch 1.:  98%|█████████▊| 210/214 [08:11<00:09,  2.35s/it]Main training loop, train epoch 1.:  99%|█████████▊| 211/214 [08:14<00:07,  2.35s/it]Main training loop, train epoch 1.:  99%|█████████▉| 212/214 [08:16<00:04,  2.35s/it]Main training loop, train epoch 1.: 100%|█████████▉| 213/214 [08:18<00:02,  2.37s/it]Main training loop, train epoch 1.: 100%|██████████| 214/214 [08:21<00:00,  2.36s/it]Main training loop, train epoch 1.: 100%|██████████| 214/214 [08:21<00:00,  2.34s/it]
Main training loop, train epoch 2.:   0%|          | 0/214 [00:00<?, ?it/s]Main training loop, train epoch 2.:   0%|          | 1/214 [00:02<08:24,  2.37s/it]Main training loop, train epoch 2.:   1%|          | 2/214 [00:04<08:23,  2.38s/it]Main training loop, train epoch 2.:   1%|▏         | 3/214 [00:07<08:12,  2.34s/it]Main training loop, train epoch 2.:   2%|▏         | 4/214 [00:09<08:12,  2.35s/it]Main training loop, train epoch 2.:   2%|▏         | 5/214 [00:11<08:08,  2.34s/it]Main training loop, train epoch 2.:   3%|▎         | 6/214 [00:14<08:05,  2.33s/it]Main training loop, train epoch 2.:   3%|▎         | 7/214 [00:16<08:00,  2.32s/it]Main training loop, train epoch 2.:   4%|▎         | 8/214 [00:18<07:57,  2.32s/it]Main training loop, train epoch 2.:   4%|▍         | 9/214 [00:20<07:56,  2.32s/it]Main training loop, train epoch 2.:   5%|▍         | 10/214 [00:23<07:53,  2.32s/it]Main training loop, train epoch 2.:   5%|▌         | 11/214 [00:25<07:50,  2.32s/it]Main training loop, train epoch 2.:   6%|▌         | 12/214 [00:27<07:49,  2.33s/it]Main training loop, train epoch 2.:   6%|▌         | 13/214 [00:30<07:49,  2.34s/it]Main training loop, train epoch 2.:   7%|▋         | 14/214 [00:32<07:45,  2.33s/it]Main training loop, train epoch 2.:   7%|▋         | 15/214 [00:35<07:48,  2.35s/it]Main training loop, train epoch 2.:   7%|▋         | 16/214 [00:37<07:46,  2.36s/it]Main training loop, train epoch 2.:   8%|▊         | 17/214 [00:39<07:41,  2.35s/it]Main training loop, train epoch 2.:   8%|▊         | 18/214 [00:42<07:38,  2.34s/it]Main training loop, train epoch 2.:   9%|▉         | 19/214 [00:44<07:39,  2.36s/it]Main training loop, train epoch 2.:   9%|▉         | 20/214 [00:46<07:33,  2.34s/it]Main training loop, train epoch 2.:  10%|▉         | 21/214 [00:49<07:35,  2.36s/it]Main training loop, train epoch 2.:  10%|█         | 22/214 [00:51<07:33,  2.36s/it]Main training loop, train epoch 2.:  11%|█         | 23/214 [00:53<07:35,  2.39s/it]Main training loop, train epoch 2.:  11%|█         | 24/214 [00:56<07:31,  2.37s/it]Main training loop, train epoch 2.:  12%|█▏        | 25/214 [00:58<07:29,  2.38s/it]Main training loop, train epoch 2.:  12%|█▏        | 26/214 [01:01<07:27,  2.38s/it]Main training loop, train epoch 2.:  13%|█▎        | 27/214 [01:03<07:25,  2.38s/it]Main training loop, train epoch 2.:  13%|█▎        | 28/214 [01:05<07:23,  2.39s/it]Main training loop, train epoch 2.:  14%|█▎        | 29/214 [01:08<07:20,  2.38s/it]Main training loop, train epoch 2.:  14%|█▍        | 30/214 [01:10<07:13,  2.36s/it]Main training loop, train epoch 2.:  14%|█▍        | 31/214 [01:12<07:13,  2.37s/it]Main training loop, train epoch 2.:  15%|█▍        | 32/214 [01:15<07:07,  2.35s/it]Main training loop, train epoch 2.:  15%|█▌        | 33/214 [01:17<07:07,  2.36s/it]Main training loop, train epoch 2.:  16%|█▌        | 34/214 [01:19<07:01,  2.34s/it]Main training loop, train epoch 2.:  16%|█▋        | 35/214 [01:22<07:02,  2.36s/it]Main training loop, train epoch 2.:  17%|█▋        | 36/214 [01:24<07:02,  2.38s/it]Main training loop, train epoch 2.:  17%|█▋        | 37/214 [01:27<07:02,  2.38s/it]Main training loop, train epoch 2.:  18%|█▊        | 38/214 [01:29<06:54,  2.36s/it]Main training loop, train epoch 2.:  18%|█▊        | 39/214 [01:31<06:53,  2.36s/it]Main training loop, train epoch 2.:  19%|█▊        | 40/214 [01:34<06:48,  2.35s/it]Main training loop, train epoch 2.:  19%|█▉        | 41/214 [01:36<06:43,  2.33s/it]Main training loop, train epoch 2.:  20%|█▉        | 42/214 [01:38<06:41,  2.34s/it]Main training loop, train epoch 2.:  20%|██        | 43/214 [01:41<06:39,  2.33s/it]Main training loop, train epoch 2.:  21%|██        | 44/214 [01:43<06:37,  2.34s/it]Main training loop, train epoch 2.:  21%|██        | 45/214 [01:45<06:35,  2.34s/it]Main training loop, train epoch 2.:  21%|██▏       | 46/214 [01:48<06:30,  2.32s/it]Main training loop, train epoch 2.:  22%|██▏       | 47/214 [01:50<06:26,  2.32s/it]Main training loop, train epoch 2.:  22%|██▏       | 48/214 [01:52<06:25,  2.32s/it]Main training loop, train epoch 2.:  23%|██▎       | 49/214 [01:55<06:22,  2.32s/it]Main training loop, train epoch 2.:  23%|██▎       | 50/214 [01:57<06:20,  2.32s/it]Main training loop, train epoch 2.:  24%|██▍       | 51/214 [01:59<06:22,  2.35s/it]Main training loop, train epoch 2.:  24%|██▍       | 52/214 [02:02<06:18,  2.34s/it]Main training loop, train epoch 2.:  25%|██▍       | 53/214 [02:04<06:17,  2.34s/it]Main training loop, train epoch 2.:  25%|██▌       | 54/214 [02:06<06:13,  2.33s/it]Main training loop, train epoch 2.:  26%|██▌       | 55/214 [02:09<06:10,  2.33s/it]Main training loop, train epoch 2.:  26%|██▌       | 56/214 [02:11<06:10,  2.34s/it]Main training loop, train epoch 2.:  27%|██▋       | 57/214 [02:13<06:09,  2.36s/it]Main training loop, train epoch 2.:  27%|██▋       | 58/214 [02:16<06:06,  2.35s/it]Main training loop, train epoch 2.:  28%|██▊       | 59/214 [02:18<06:05,  2.36s/it]Main training loop, train epoch 2.:  28%|██▊       | 60/214 [02:20<06:01,  2.34s/it]Main training loop, train epoch 2.:  29%|██▊       | 61/214 [02:23<06:00,  2.36s/it]Main training loop, train epoch 2.:  29%|██▉       | 62/214 [02:25<05:59,  2.37s/it]Main training loop, train epoch 2.:  29%|██▉       | 63/214 [02:27<05:58,  2.37s/it]Main training loop, train epoch 2.:  30%|██▉       | 64/214 [02:30<05:56,  2.38s/it]Main training loop, train epoch 2.:  30%|███       | 65/214 [02:32<05:50,  2.36s/it]Main training loop, train epoch 2.:  31%|███       | 66/214 [02:35<05:50,  2.37s/it]Main training loop, train epoch 2.:  31%|███▏      | 67/214 [02:37<05:46,  2.36s/it]Main training loop, train epoch 2.:  32%|███▏      | 68/214 [02:39<05:45,  2.37s/it]Main training loop, train epoch 2.:  32%|███▏      | 69/214 [02:42<05:43,  2.37s/it]Main training loop, train epoch 2.:  33%|███▎      | 70/214 [02:44<05:39,  2.36s/it]Main training loop, train epoch 2.:  33%|███▎      | 71/214 [02:46<05:37,  2.36s/it]Main training loop, train epoch 2.:  34%|███▎      | 72/214 [02:49<05:37,  2.38s/it]Main training loop, train epoch 2.:  34%|███▍      | 73/214 [02:51<05:31,  2.35s/it]Main training loop, train epoch 2.:  35%|███▍      | 74/214 [02:53<05:29,  2.35s/it]Main training loop, train epoch 2.:  35%|███▌      | 75/214 [02:56<05:28,  2.36s/it]Main training loop, train epoch 2.:  36%|███▌      | 76/214 [02:58<05:25,  2.36s/it]Main training loop, train epoch 2.:  36%|███▌      | 77/214 [03:00<05:21,  2.35s/it]Main training loop, train epoch 2.:  36%|███▋      | 78/214 [03:03<05:18,  2.34s/it]Main training loop, train epoch 2.:  37%|███▋      | 79/214 [03:05<05:18,  2.36s/it]Main training loop, train epoch 2.:  37%|███▋      | 80/214 [03:08<05:15,  2.35s/it]Main training loop, train epoch 2.:  38%|███▊      | 81/214 [03:10<05:15,  2.37s/it]Main training loop, train epoch 2.:  38%|███▊      | 82/214 [03:12<05:11,  2.36s/it]Main training loop, train epoch 2.:  39%|███▉      | 83/214 [03:15<05:09,  2.36s/it]Main training loop, train epoch 2.:  39%|███▉      | 84/214 [03:17<05:05,  2.35s/it]Main training loop, train epoch 2.:  40%|███▉      | 85/214 [03:19<05:04,  2.36s/it]Main training loop, train epoch 2.:  40%|████      | 86/214 [03:22<05:00,  2.35s/it]Main training loop, train epoch 2.:  41%|████      | 87/214 [03:24<05:01,  2.37s/it]Main training loop, train epoch 2.:  41%|████      | 88/214 [03:26<04:57,  2.36s/it]Main training loop, train epoch 2.:  42%|████▏     | 89/214 [03:29<04:56,  2.37s/it]Main training loop, train epoch 2.:  42%|████▏     | 90/214 [03:31<04:51,  2.35s/it]Main training loop, train epoch 2.:  43%|████▎     | 91/214 [03:34<04:51,  2.37s/it]Main training loop, train epoch 2.:  43%|████▎     | 92/214 [03:36<04:47,  2.36s/it]Main training loop, train epoch 2.:  43%|████▎     | 93/214 [03:38<04:46,  2.37s/it]Main training loop, train epoch 2.:  44%|████▍     | 94/214 [03:41<04:43,  2.37s/it]Main training loop, train epoch 2.:  44%|████▍     | 95/214 [03:43<04:41,  2.37s/it]Main training loop, train epoch 2.:  45%|████▍     | 96/214 [03:45<04:37,  2.36s/it]Main training loop, train epoch 2.:  45%|████▌     | 97/214 [03:48<04:34,  2.35s/it]Main training loop, train epoch 2.:  46%|████▌     | 98/214 [03:50<04:33,  2.36s/it]Main training loop, train epoch 2.:  46%|████▋     | 99/214 [03:52<04:30,  2.35s/it]Main training loop, train epoch 2.:  47%|████▋     | 100/214 [03:55<04:30,  2.37s/it]Main training loop, train epoch 2.:  47%|████▋     | 101/214 [03:57<04:29,  2.38s/it]Main training loop, train epoch 2.:  48%|████▊     | 102/214 [04:00<04:26,  2.38s/it]Main training loop, train epoch 2.:  48%|████▊     | 103/214 [04:02<04:24,  2.38s/it]Main training loop, train epoch 2.:  49%|████▊     | 104/214 [04:04<04:20,  2.36s/it]Main training loop, train epoch 2.:  49%|████▉     | 105/214 [04:07<04:19,  2.38s/it]Main training loop, train epoch 2.:  50%|████▉     | 106/214 [04:09<04:18,  2.40s/it]Main training loop, train epoch 2.:  50%|█████     | 107/214 [04:12<04:14,  2.38s/it]Main training loop, train epoch 2.:  50%|█████     | 108/214 [04:14<04:10,  2.37s/it]Main training loop, train epoch 2.:  51%|█████     | 109/214 [04:16<04:08,  2.37s/it]Main training loop, train epoch 2.:  51%|█████▏    | 110/214 [04:19<04:05,  2.36s/it]Main training loop, train epoch 2.:  52%|█████▏    | 111/214 [04:21<04:01,  2.35s/it]Main training loop, train epoch 2.:  52%|█████▏    | 112/214 [04:23<03:59,  2.34s/it]Main training loop, train epoch 2.:  53%|█████▎    | 113/214 [04:26<03:57,  2.35s/it]Main training loop, train epoch 2.:  53%|█████▎    | 114/214 [04:28<03:53,  2.34s/it]Main training loop, train epoch 2.:  54%|█████▎    | 115/214 [04:30<03:52,  2.35s/it]Main training loop, train epoch 2.:  54%|█████▍    | 116/214 [04:33<03:50,  2.35s/it]Main training loop, train epoch 2.:  55%|█████▍    | 117/214 [04:35<03:49,  2.36s/it]Main training loop, train epoch 2.:  55%|█████▌    | 118/214 [04:37<03:47,  2.37s/it]Main training loop, train epoch 2.:  56%|█████▌    | 119/214 [04:40<03:45,  2.38s/it]Main training loop, train epoch 2.:  56%|█████▌    | 120/214 [04:42<03:42,  2.36s/it]Main training loop, train epoch 2.:  57%|█████▋    | 121/214 [04:44<03:38,  2.35s/it]Main training loop, train epoch 2.:  57%|█████▋    | 122/214 [04:47<03:35,  2.34s/it]Main training loop, train epoch 2.:  57%|█████▋    | 123/214 [04:49<03:34,  2.36s/it]Main training loop, train epoch 2.:  58%|█████▊    | 124/214 [04:51<03:31,  2.35s/it]Main training loop, train epoch 2.:  58%|█████▊    | 125/214 [04:54<03:29,  2.36s/it]Main training loop, train epoch 2.:  59%|█████▉    | 126/214 [04:56<03:27,  2.35s/it]Main training loop, train epoch 2.:  59%|█████▉    | 127/214 [04:59<03:26,  2.37s/it]Main training loop, train epoch 2.:  60%|█████▉    | 128/214 [05:01<03:22,  2.36s/it]Main training loop, train epoch 2.:  60%|██████    | 129/214 [05:03<03:20,  2.36s/it]Main training loop, train epoch 2.:  61%|██████    | 130/214 [05:06<03:17,  2.35s/it]Main training loop, train epoch 2.:  61%|██████    | 131/214 [05:08<03:13,  2.33s/it]Main training loop, train epoch 2.:  62%|██████▏   | 132/214 [05:10<03:11,  2.34s/it]Main training loop, train epoch 2.:  62%|██████▏   | 133/214 [05:13<03:08,  2.33s/it]Main training loop, train epoch 2.:  63%|██████▎   | 134/214 [05:15<03:05,  2.32s/it]Main training loop, train epoch 2.:  63%|██████▎   | 135/214 [05:17<03:03,  2.32s/it]Main training loop, train epoch 2.:  64%|██████▎   | 136/214 [05:20<03:02,  2.33s/it]Main training loop, train epoch 2.:  64%|██████▍   | 137/214 [05:22<03:00,  2.34s/it]Main training loop, train epoch 2.:  64%|██████▍   | 138/214 [05:24<02:57,  2.34s/it]Main training loop, train epoch 2.:  65%|██████▍   | 139/214 [05:27<02:56,  2.35s/it]Main training loop, train epoch 2.:  65%|██████▌   | 140/214 [05:29<02:54,  2.36s/it]Main training loop, train epoch 2.:  66%|██████▌   | 141/214 [05:31<02:53,  2.37s/it]Main training loop, train epoch 2.:  66%|██████▋   | 142/214 [05:34<02:49,  2.36s/it]Main training loop, train epoch 2.:  67%|██████▋   | 143/214 [05:36<02:46,  2.35s/it]Main training loop, train epoch 2.:  67%|██████▋   | 144/214 [05:38<02:43,  2.34s/it]Main training loop, train epoch 2.:  68%|██████▊   | 145/214 [05:41<02:42,  2.36s/it]Main training loop, train epoch 2.:  68%|██████▊   | 146/214 [05:43<02:40,  2.35s/it]Main training loop, train epoch 2.:  69%|██████▊   | 147/214 [05:46<02:38,  2.36s/it]Main training loop, train epoch 2.:  69%|██████▉   | 148/214 [05:48<02:35,  2.36s/it]Main training loop, train epoch 2.:  70%|██████▉   | 149/214 [05:50<02:32,  2.35s/it]Main training loop, train epoch 2.:  70%|███████   | 150/214 [05:53<02:30,  2.34s/it]Main training loop, train epoch 2.:  71%|███████   | 151/214 [05:55<02:26,  2.33s/it]Main training loop, train epoch 2.:  71%|███████   | 152/214 [05:57<02:25,  2.35s/it]Main training loop, train epoch 2.:  71%|███████▏  | 153/214 [06:00<02:22,  2.34s/it]Main training loop, train epoch 2.:  72%|███████▏  | 154/214 [06:02<02:20,  2.35s/it]Main training loop, train epoch 2.:  72%|███████▏  | 155/214 [06:04<02:18,  2.35s/it]Main training loop, train epoch 2.:  73%|███████▎  | 156/214 [06:07<02:15,  2.34s/it]Main training loop, train epoch 2.:  73%|███████▎  | 157/214 [06:09<02:12,  2.33s/it]Main training loop, train epoch 2.:  74%|███████▍  | 158/214 [06:11<02:10,  2.33s/it]Main training loop, train epoch 2.:  74%|███████▍  | 159/214 [06:14<02:08,  2.33s/it]Main training loop, train epoch 2.:  75%|███████▍  | 160/214 [06:16<02:05,  2.32s/it]Main training loop, train epoch 2.:  75%|███████▌  | 161/214 [06:18<02:03,  2.33s/it]Main training loop, train epoch 2.:  76%|███████▌  | 162/214 [06:21<02:01,  2.33s/it]Main training loop, train epoch 2.:  76%|███████▌  | 163/214 [06:23<01:59,  2.34s/it]Main training loop, train epoch 2.:  77%|███████▋  | 164/214 [06:25<01:56,  2.33s/it]Main training loop, train epoch 2.:  77%|███████▋  | 165/214 [06:28<01:54,  2.34s/it]Main training loop, train epoch 2.:  78%|███████▊  | 166/214 [06:30<01:52,  2.35s/it]Main training loop, train epoch 2.:  78%|███████▊  | 167/214 [06:32<01:50,  2.35s/it]Main training loop, train epoch 2.:  79%|███████▊  | 168/214 [06:35<01:47,  2.33s/it]Main training loop, train epoch 2.:  79%|███████▉  | 169/214 [06:37<01:45,  2.35s/it]Main training loop, train epoch 2.:  79%|███████▉  | 170/214 [06:39<01:42,  2.34s/it]Main training loop, train epoch 2.:  80%|███████▉  | 171/214 [06:42<01:40,  2.34s/it]Main training loop, train epoch 2.:  80%|████████  | 172/214 [06:44<01:37,  2.33s/it]Main training loop, train epoch 2.:  81%|████████  | 173/214 [06:46<01:36,  2.34s/it]Main training loop, train epoch 2.:  81%|████████▏ | 174/214 [06:49<01:33,  2.35s/it]Main training loop, train epoch 2.:  82%|████████▏ | 175/214 [06:51<01:32,  2.36s/it]Main training loop, train epoch 2.:  82%|████████▏ | 176/214 [06:53<01:30,  2.38s/it]Main training loop, train epoch 2.:  83%|████████▎ | 177/214 [06:56<01:27,  2.37s/it]Main training loop, train epoch 2.:  83%|████████▎ | 178/214 [06:58<01:24,  2.35s/it]Main training loop, train epoch 2.:  84%|████████▎ | 179/214 [07:01<01:22,  2.36s/it]Main training loop, train epoch 2.:  84%|████████▍ | 180/214 [07:03<01:19,  2.34s/it]Main training loop, train epoch 2.:  85%|████████▍ | 181/214 [07:05<01:17,  2.34s/it]Main training loop, train epoch 2.:  85%|████████▌ | 182/214 [07:08<01:15,  2.36s/it]Main training loop, train epoch 2.:  86%|████████▌ | 183/214 [07:10<01:13,  2.38s/it]Main training loop, train epoch 2.:  86%|████████▌ | 184/214 [07:12<01:10,  2.36s/it]Main training loop, train epoch 2.:  86%|████████▋ | 185/214 [07:15<01:08,  2.35s/it]Main training loop, train epoch 2.:  87%|████████▋ | 186/214 [07:17<01:06,  2.37s/it]Main training loop, train epoch 2.:  87%|████████▋ | 187/214 [07:19<01:03,  2.36s/it]Main training loop, train epoch 2.:  88%|████████▊ | 188/214 [07:22<01:01,  2.37s/it]Main training loop, train epoch 2.:  88%|████████▊ | 189/214 [07:24<00:59,  2.36s/it]Main training loop, train epoch 2.:  89%|████████▉ | 190/214 [07:26<00:56,  2.35s/it]Main training loop, train epoch 2.:  89%|████████▉ | 191/214 [07:29<00:54,  2.36s/it]Main training loop, train epoch 2.:  90%|████████▉ | 192/214 [07:31<00:52,  2.36s/it]Main training loop, train epoch 2.:  90%|█████████ | 193/214 [07:34<00:49,  2.37s/it]Main training loop, train epoch 2.:  91%|█████████ | 194/214 [07:36<00:47,  2.36s/it]Main training loop, train epoch 2.:  91%|█████████ | 195/214 [07:38<00:45,  2.37s/it]Main training loop, train epoch 2.:  92%|█████████▏| 196/214 [07:41<00:42,  2.39s/it]Main training loop, train epoch 2.:  92%|█████████▏| 197/214 [07:43<00:40,  2.37s/it]Main training loop, train epoch 2.:  93%|█████████▎| 198/214 [07:45<00:37,  2.36s/it]Main training loop, train epoch 2.:  93%|█████████▎| 199/214 [07:48<00:35,  2.37s/it]Main training loop, train epoch 2.:  93%|█████████▎| 200/214 [07:50<00:32,  2.35s/it]Main training loop, train epoch 2.:  94%|█████████▍| 201/214 [07:52<00:30,  2.34s/it]Main training loop, train epoch 2.:  94%|█████████▍| 202/214 [07:55<00:28,  2.33s/it]Main training loop, train epoch 2.:  95%|█████████▍| 203/214 [07:57<00:25,  2.35s/it]Main training loop, train epoch 2.:  95%|█████████▌| 204/214 [08:00<00:23,  2.36s/it]Main training loop, train epoch 2.:  96%|█████████▌| 205/214 [08:02<00:21,  2.35s/it]Main training loop, train epoch 2.:  96%|█████████▋| 206/214 [08:04<00:18,  2.34s/it]Main training loop, train epoch 2.:  97%|█████████▋| 207/214 [08:06<00:16,  2.33s/it]Main training loop, train epoch 2.:  97%|█████████▋| 208/214 [08:09<00:14,  2.35s/it]Main training loop, train epoch 2.:  98%|█████████▊| 209/214 [08:11<00:11,  2.35s/it]Main training loop, train epoch 2.:  98%|█████████▊| 210/214 [08:14<00:09,  2.35s/it]Main training loop, train epoch 2.:  99%|█████████▊| 211/214 [08:16<00:07,  2.34s/it]Main training loop, train epoch 2.:  99%|█████████▉| 212/214 [08:18<00:04,  2.33s/it]Main training loop, train epoch 2.: 100%|█████████▉| 213/214 [08:21<00:02,  2.33s/it]Main training loop, train epoch 2.: 100%|██████████| 214/214 [08:23<00:00,  2.34s/it]Main training loop, train epoch 2.: 100%|██████████| 214/214 [08:23<00:00,  2.35s/it]
Main training loop, train epoch 3.:   0%|          | 0/214 [00:00<?, ?it/s]Main training loop, train epoch 3.:   0%|          | 1/214 [00:02<08:22,  2.36s/it]Main training loop, train epoch 3.:   1%|          | 2/214 [00:04<08:25,  2.38s/it]Main training loop, train epoch 3.:   1%|▏         | 3/214 [00:07<08:26,  2.40s/it]Main training loop, train epoch 3.:   2%|▏         | 4/214 [00:09<08:18,  2.37s/it]Main training loop, train epoch 3.:   2%|▏         | 5/214 [00:11<08:13,  2.36s/it]Main training loop, train epoch 3.:   3%|▎         | 6/214 [00:14<08:12,  2.37s/it]Main training loop, train epoch 3.:   3%|▎         | 7/214 [00:16<08:06,  2.35s/it]Main training loop, train epoch 3.:   4%|▎         | 8/214 [00:18<08:02,  2.34s/it]Main training loop, train epoch 3.:   4%|▍         | 9/214 [00:21<07:59,  2.34s/it]Main training loop, train epoch 3.:   5%|▍         | 10/214 [00:23<07:57,  2.34s/it]Main training loop, train epoch 3.:   5%|▌         | 11/214 [00:25<07:55,  2.34s/it]Main training loop, train epoch 3.:   6%|▌         | 12/214 [00:28<07:52,  2.34s/it]Main training loop, train epoch 3.:   6%|▌         | 13/214 [00:30<07:51,  2.35s/it]Main training loop, train epoch 3.:   7%|▋         | 14/214 [00:32<07:49,  2.35s/it]Main training loop, train epoch 3.:   7%|▋         | 15/214 [00:35<07:45,  2.34s/it]Main training loop, train epoch 3.:   7%|▋         | 16/214 [00:37<07:41,  2.33s/it]Main training loop, train epoch 3.:   8%|▊         | 17/214 [00:39<07:40,  2.34s/it]Main training loop, train epoch 3.:   8%|▊         | 18/214 [00:42<07:37,  2.34s/it]Main training loop, train epoch 3.:   9%|▉         | 19/214 [00:44<07:34,  2.33s/it]Main training loop, train epoch 3.:   9%|▉         | 20/214 [00:46<07:36,  2.35s/it]read_signalfd: got signal 15 (Terminated)
Forwarding signal 15
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 12 died from signal 15
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 4 died from signal 15
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 8 died from signal 15
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 0 died from signal 15
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 16 died from signal 15
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 18 died from signal 15
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 17 died from signal 15
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 6 died from signal 15
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 7 died from signal 15
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 15 died from signal 15
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 14 died from signal 15
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 10 died from signal 15
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 9 died from signal 15
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 2 died from signal 15
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 1 died from signal 15
x3005c0s37b1n0.hsn.cm.polaris.alcf.anl.gov: rank 5 died from signal 15
x3005c0s7b0n0.hsn.cm.polaris.alcf.anl.gov: rank 11 died from signal 15
x3006c0s13b0n0.hsn.cm.polaris.alcf.anl.gov: rank 19 died from signal 15
x3005c0s7b1n0.hsn.cm.polaris.alcf.anl.gov: rank 13 died from signal 15
x3005c0s37b0n0.hsn.cm.polaris.alcf.anl.gov: rank 3 died from signal 15
Application b9e7db6c resources: utime=38167s stime=15138s maxrss=35332340KB inblock=281611874 oublock=48 minflt=1390880139 majflt=18806 nvcsw=133626757 nivcsw=39732112
