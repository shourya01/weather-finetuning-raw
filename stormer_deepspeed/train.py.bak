# need to avoid errors with MPI
from mpi4py import MPI

# standard imports
import random
import math
import torch
import torch.nn as nn
import torch.distributed as dist
import deepspeed 
from torch.optim import AdamW
from torch.utils.data import DataLoader, DistributedSampler
import torch.nn.functional as F
import numpy as np
import netCDF4
import h5py
import os
import argparse
from tqdm import tqdm
import time
from typing import Dict, Optional, List, Union
from pathlib import Path

from TimesFM import TimesFM
from stormer_specific_data_utils import WeatherEnergySet

def parse_args():

    parser = argparse.ArgumentParser(description="Training TimesFM on multiple nodes.")
    parser.add_argument("--per_device_train_batch_size", type=int, default=64)
    parser.add_argument("--epochs", type=int, default=20)
    parser.add_argument("--local_rank", type=int, default=-1)
    parser.add_argument("--deepspeed", action="store_true")
    parser.add_argument("--deepspeed_config", type=str, default="ds_config.json")
    parser.add_argument("--output_dir", type=str, default="/eagle/ParaLLMs/weather_load_forecasting/results")
    parser.add_argument("--experiment_name", type=str, default="my_experiment")
    parser.add_argument("--test_every", type=int, default=10000)
    
    return parser.parse_args()

def get_model(
        lookback: int = 512,
        lookahead: int = 96,
        lora_rank: Optional[int] = 4,
        weather_model: bool = False,
        load_weights_into_model: bool = True, # load weights via TimesFM load_weights() method
        model_load_kwargs: Dict = {}, # to be used for general TimesFM weights loading
        model_weight_kwargs: Dict = {}, # to be used for TimesFM load_weights
):
    
    model = TimesFM(
        lookback = lookback,
        lookahead = lookahead,
        lora_rank = lora_rank,
        weather_model = weather_model,
        **model_load_kwargs
    )

    if load_weights_into_model is True:
        model.load_weights(**model_weight_kwargs)

    return model

def get_dataloader(
        lookback: int = 512,
        lookahead: int = 96,
        global_rank: Optional[int] = None,
        world_size: Optional[int] = None,
        batch_size: int = 128, # in case of multi-node training, this becomes the per-GPU batch size
        is_val: bool = False, # in this case, return the val instead of the test set
        splits: List = [0.8,0.1,0.1],
        energy_nc_path: Union[str, Path] = Path("/eagle/ParaLLMs/weather_load_forecasting/comstock_datasets/small_ca/dset.nc"),
        weather_tensor_h5_path: Union[str, Path] = Path("/eagle/ParaLLMs/weather_load_forecasting/stormer/saved_tensors/tensors.h5"),
        device: Optional[torch.device] = None
):
    
    # check splits are legal and generate cumulatives
    assert len(splits) == 3 and np.isclose(sum(splits),1), "Splits must contain 3 ratios that add upto 1."
    cum_splits = [0] + list(np.cumsum(splits))

    # load the raw data into RAM
    energy_ds = netCDF4.Dataset(str(energy_nc_path), mode="r")
    energy_np = energy_ds.variables["energy_consumption"][:]           # (B, T)
    with h5py.File(str(weather_tensor_h5_path), mode="r") as h5f:
        root = h5f["/data"]
        if isinstance(root, h5py.Dataset):               # single-dataset layout
            weather_np = root[...]
        else:                                            # per-hour group layout
            hour_keys = sorted(root.keys(), key=int)
            sample = root[hour_keys[0]]
            num_hours = len(hour_keys)
            num_tiers, num_vars, H, W = sample.shape
            weather_np = np.empty(
                (num_hours, num_tiers, num_vars, H, W), dtype=sample.dtype
            )
            for idx, k in enumerate(hour_keys):
                weather_np[idx] = root[k][...]

    # load train dataset
    train_dset = WeatherEnergySet(
        lookback = lookback,
        lookahead = lookahead,
        energy_np = energy_np,
        weather_np = weather_np,
        start_boundary_ratio = cum_splits[0],
        end_boundary_ratio = cum_splits[1],
        device=device
    )

    # if we have ranks we should create our own sampler
    if (global_rank is not None) and (world_size is not None):
        sampler = DistributedSampler(
            train_dset,
            num_replicas = world_size,
            rank = global_rank,
            shuffle = False,
            seed = 42
        )
    else:
        sampler = None

    # define train loader
    train_loader = DataLoader(
        train_dset,
        batch_size = batch_size,
        shuffle = False,
        drop_last = False,
        sampler = sampler,
        pin_memory = True
    )

    # in case of non-distributed settings or when global rank is 0, generate train or validation set
    if (global_rank is None or world_size is None) or (global_rank == 0):
        val_dset = WeatherEnergySet(
            lookback = lookback,
            lookahead = lookahead,
            energy_np = energy_np,
            weather_np = weather_np,
            start_boundary_ratio = cum_splits[1] if is_val else cum_splits[2],
            end_boundary_ratio = cum_splits[2] if is_val else cum_splits[3]
        )
        val_loader = DataLoader(
            val_dset,
            batch_size = batch_size,
            shuffle = False,
            drop_last = False,
            pin_memory = True
        )
    else:
        val_loader = None

    # return the results
    return train_loader, val_loader

def get_train_loss():
    return lambda x,y: F.mse_loss(x,y,reduction="mean")

def get_eval_loss_dict(mean, std):
    try:
        to_float = lambda x: x.item() if hasattr(x,"item") else float(x)
        mean, std = to_float(mean), to_float(std)
    except:
        raise ValueError("The provided mean and std cannot be converted to floats, possibly they contain more than one element.")
    unnorm = lambda x: std * x + mean # function to unnormalize the data
    return {
        "mse": lambda y,yhat: F.mse_loss(y,yhat,reduction="mean").item(),
        "mae": lambda y,yhat: F.l1_loss(y,yhat,reduction="mean").item(),
        "nmse": lambda y,yhat: F.mse_loss(unnorm(y),unnorm(yhat),reduction="mean").item(),
        "nmae": lambda y,yhat: F.l1_loss(unnorm(y),unnorm(yhat),reduction="mean").item(),
        "mape": lambda y,yhat: 100 * torch.mean(torch.abs(unnorm(y) - unnorm(yhat)) / torch.abs(unnorm(yhat))).item(),
        "smape": lambda y,yhat: 200 * torch.mean(torch.abs(unnorm(y) - unnorm(yhat)) / (torch.abs(y) + torch.abs(yhat)))
    }

def train(
        model,
        trainloader,
        testloader,
        args,
        is_tuning = False
):
    
    model_engine, optimizer, _, _ = deepspeed.initialize(
        args = args,
        model = model,
        model_parameters = model.parameters()
    )

    device = model_engine.device
    global_rank = dist.get_rank() if torch.distributed.is_initialized() else 0

    is_main_process = (dist.get_rank() == 0)

    model_engine.train()
    epochs = args.epochs()
    loss_fn = get_train_loss()

    if is_main_process:
        total_steps = epochs * len(trainloader)
        progress_bar = tqdm(range(total_steps), desc="Training")

    for epoch in range(epochs):
        epoch_loss = 0

        for step, (x,y,weather) in enumerate(trainloader):

            x,y,weather = x.to(device), y.to(device), weather.to(device)
            out = model(x,weather=weather)
            loss = loss_fn(out,y)

            model_engine.backward(loss)
            model_engine.step()

            if is_main_process:
                epoch_loss += loss.item()
                progress_bar.update(1)

    if is_main_process:
        avg_epoch_loss = epoch_loss / len(trainloader)
        progress_bar.set_description(f"After epoch {epoch+1}, loss is {avg_epoch_loss:.3f}.")


if __name__ == "__main__":

    args = parse_args()

    print(f"Detected local rank to be {os.getenv('MPI_LOCALRANKID',-1)}.",flush=True)

    # Deepspeed configs
    if 'MPI_LOCALRANKID' in os.environ:
        # MPICH launcher - use MPI environment variables
        args.local_rank = int(os.environ['MPI_LOCALRANKID'])
        os.environ['LOCAL_RANK'] = str(args.local_rank)  # Set for DeepSpeed
    elif 'LOCAL_RANK' in os.environ:
        args.local_rank = int(os.environ['LOCAL_RANK'])
    elif args.local_rank == -1:
        args.local_rank = 0

    if torch.cuda.is_available():
        torch.cuda.set_device(args.local_rank)
    
    world_size = 1
    global_rank = 0
    
    # master_addr = os.getenv("MASTER_ADDR", "10.0.0.1")
    deepspeed.init_distributed(
        # dist_backend = "nccl",
        # init_method = f"tcp://{master_addr}:53477"
    )

    local_rank = args.local_rank
    global_rank = torch.distributed.get_rank()
    gpu_id = torch.cuda.current_device()
    print(f"[Node {os.uname()[1]}] global_rank={global_rank} local_rank={local_rank} â†’ GPU {gpu_id} ({torch.cuda.get_device_name(gpu_id)})")

    if torch.distributed.is_initialized():
        world_size = torch.distributed.get_world_size()
        global_rank = torch.distributed.get_rank()
    args.world_size = world_size
    args.global_rank = global_rank

    # get model and data
    dl_train, dl_val = get_dataloader(global_rank=args.global_rank, world_size=args.world_size, device=torch.device("cpu"))
    model = get_model()

    if global_rank == 0:
        print(f"Training with batch size: {args.per_device_train_batch_size} per GPU")
        print(f"World size: {world_size} GPUs total")
        print(f"Global rank: {global_rank}, Local rank: {args.local_rank}")
        print(f"Effective global batch size: {args.per_device_train_batch_size * world_size}")

    train(model,dl_train,dl_val,args,False)

    if torch.distributed.is_initialized():
        torch.distributed.destroy_process_group()

    

    
    
