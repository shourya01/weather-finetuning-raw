/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled

Lmod is automatically replacing "nvhpc/23.9" with "gcc-native/12.3".


Lmod is automatically replacing "PrgEnv-nvhpc/8.5.0" with "PrgEnv-gnu/8.5.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.28

declare -x APP2_STATE="23.12.0"
declare -x BASH_ENV="/usr/share/lmod/lmod/init/bash"
declare -x C3_RSH="ssh -oConnectTimeout=10 -oForwardX11=no"
declare -x CFLAGS="-I/soft/applications/conda/2024-04-29/mconda3/include"
declare -x COLORTERM="1"
declare -x COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin"
declare -x CONDA_DEFAULT_ENV="base"
declare -x CONDA_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/conda"
declare -x CONDA_PREFIX="/soft/applications/conda/2024-04-29/mconda3"
declare -x CONDA_PROMPT_MODIFIER="(2024-04-29/base) "
declare -x CONDA_PYTHON_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/python"
declare -x CONDA_SHLVL="1"
declare -x CPU="x86_64"
declare -x CRAYPAT_LD_LIBRARY_PATH="/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAYPAT_OPTS_EXECUTABLE="libexec64/opts"
declare -x CRAYPAT_ROOT="/opt/cray/pe/perftools/23.12.0"
declare -x CRAYPE_DIR="/opt/cray/pe/craype/2.7.30"
declare -x CRAYPE_NETWORK_TARGET="ofi"
declare -x CRAYPE_VERSION="2.7.30"
declare -x CRAY_CPU_TARGET="x86-milan"
declare -x CRAY_DSMML_BASEDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_DIR="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_PREFIX="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_ROOTDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_VER="0.2.2"
declare -x CRAY_DSMML_VERSION="0.2.2"
declare -x CRAY_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x CRAY_HDF5_PARALLEL_PREFIX="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x CRAY_HDF5_PARALLEL_VERSION="1.12.2.9"
declare -x CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:/opt/cray/pe/pmi/6.1.13/lib:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:/opt/cray/pe/mpich/8.1.28/gtl/lib:/opt/cray/pe/dsmml/0.2.2/dsmml/lib:/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAY_LMOD_COMPILER="gnu/12.0"
declare -x CRAY_LMOD_CPU="x86-milan/1.0"
declare -x CRAY_LMOD_MPI="cray-mpich/8.0"
declare -x CRAY_LMOD_NET="ofi/1.0"
declare -x CRAY_MPICH_BASEDIR="/opt/cray/pe/mpich/8.1.28/ofi"
declare -x CRAY_MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_PREFIX="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_ROOTDIR="/opt/cray/pe/mpich/8.1.28"
declare -x CRAY_MPICH_VER="8.1.28"
declare -x CRAY_MPICH_VERSION="8.1.28"
declare -x CRAY_PERFTOOLS_PREFIX="/opt/cray/pe/perftools/23.12.0"
declare -x CRAY_PERFTOOLS_VERSION="23.12.0"
declare -x CRAY_PMI_INCLUDE_OPTS="-I/opt/cray/pe/pmi/6.1.13/include"
declare -x CRAY_PMI_POST_LINK_OPTS="-L/opt/cray/pe/pmi/6.1.13/lib"
declare -x CRAY_PMI_PREFIX="/opt/cray/pe/pmi/6.1.13"
declare -x CRAY_PMI_VERSION="6.1.13"
declare -x CSHEDIT="emacs"
declare -x CUDA_HOME="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_TOOLKIT_BASE="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDNN_HOME="/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/"
declare -x ENVIRONMENT="BATCH"
declare -x ENV_NAME="conda/2024-04-29"
declare -x FROM_HEADER=""
declare -x GCC_PATH="/usr/bin"
declare -x GCC_PREFIX="/usr/lib64/gcc/x86_64-suse-linux/12"
declare -x GCC_VERSION="12.3"
declare -x GNU_VERSION="12.3"
declare -x GPG_TTY="not a tty"
declare -x GSETTINGS_SCHEMA_DIR="/soft/applications/conda/2024-04-29/mconda3/share/glib-2.0/schemas"
declare -x GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=""
declare -x G_BROKEN_FILENAMES="1"
declare -x G_FILENAME_ENCODING="@locale,UTF-8,ISO-8859-15,CP1252"
declare -x HDF5_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HDF5_ROOT="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HISTSIZE="1000"
declare -x HOME="/home/shourya01"
declare -x HOST="x3006c0s19b0n0"
declare -x HOSTNAME="x3006c0s19b0n0"
declare -x HOSTTYPE="x86_64"
declare -x HTTPS_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x HTTP_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x LANG="en_US.UTF-8"
declare -x LANGUAGE="en_US.UTF-8"
declare -x LDFLAGS="-L/soft/applications/conda/2024-04-29/mconda3/lib -Wl,--enable-new-dtags,-rpath,/soft/applications/conda/2024-04-29/mconda3/lib"
declare -x LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:/soft/perftools/darshan/darshan-3.4.4/lib:/opt/cray/pe/papi/7.0.1.2/lib64:/opt/cray/libfabric/1.15.2.0/lib64"
declare -x LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so"
declare -x LESS="-M -I -R"
declare -x LESSCLOSE="lessclose.sh %s %s"
declare -x LESSKEY="/etc/lesskey.bin"
declare -x LESSOPEN="lessopen.sh %s"
declare -x LESS_ADVANCED_PREPROCESSOR="no"
declare -x LMOD_CMD="/usr/share/lmod/lmod/libexec/lmod"
declare -x LMOD_DIR="/usr/share/lmod/lmod/libexec"
declare -x LMOD_FAMILY_COMPILER="gcc-native"
declare -x LMOD_FAMILY_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_CRAYPE="craype"
declare -x LMOD_FAMILY_CRAYPE_CPU="craype-x86-milan"
declare -x LMOD_FAMILY_CRAYPE_CPU_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_NETWORK="craype-network-ofi"
declare -x LMOD_FAMILY_CRAYPE_NETWORK_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_VERSION="2.7.30"
declare -x LMOD_FAMILY_GCC_COMPILER="gcc-native"
declare -x LMOD_FAMILY_GCC_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_HDF5="cray-hdf5-parallel"
declare -x LMOD_FAMILY_HDF5_VERSION="1.12.2.9"
declare -x LMOD_FAMILY_MPI="cray-mpich"
declare -x LMOD_FAMILY_MPI_VERSION="8.1.28"
declare -x LMOD_FAMILY_PRGENV="PrgEnv-gnu"
declare -x LMOD_FAMILY_PRGENV_VERSION="8.5.0"
declare -x LMOD_FAMILY_PYTHON="conda"
declare -x LMOD_FAMILY_PYTHON_VERSION="2024-04-29"
declare -x LMOD_PKG="/usr/share/lmod/lmod"
declare -x LMOD_ROOT="/usr/share/lmod"
declare -x LMOD_SETTARG_FULL_SUPPORT="no"
declare -x LMOD_SYSTEM_DEFAULT_MODULES="PrgEnv-nvhpc:craype-network-ofi:perftools-base:darshan:xalt"
declare -x LMOD_VERSION="8.7.34"
declare -x LMOD_sys="Linux"
declare -x LOADEDMODULES="libfabric/1.15.2.0:craype-network-ofi:perftools-base/23.12.0:darshan/3.4.4:xalt/3.0.2-202408282050:gcc-native/12.3:craype/2.7.30:cray-dsmml/0.2.2:cray-mpich/8.1.28:cray-pmi/6.1.13:cray-pals/1.3.4:cray-libpals/1.3.4:craype-x86-milan:PrgEnv-gnu/8.5.0:cray-hdf5-parallel/1.12.2.9:cudnn/9.1.0:conda/2024-04-29"
declare -x LOGNAME="shourya01"
declare -x MACHTYPE="x86_64-suse-linux"
declare -x MAIL="/var/spool/mail/shourya01"
declare -x MANPATH="/opt/cray/pals/1.3.4/man:/opt/cray/pe/pmi/6.1.13/man:/opt/cray/pe/mpich/8.1.28/ofi/man:/opt/cray/pe/mpich/8.1.28/man/mpich:/opt/cray/pe/dsmml/0.2.2/dsmml/man:/opt/cray/pe/craype/2.7.30/man:/opt/cray/pe/perftools/23.12.0/man:/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:/opt/cray/libfabric/1.15.2.0/share/man:/usr/share/lmod/lmod/share/man:/home/shourya01/.local/man:/usr/local/man:/usr/share/man:/usr/man:/opt/c3/man:/opt/pbs/share/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man"
declare -x MINICOM="-c on"
declare -x MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:/opt/cray/pe/lmod/modulefiles/mix_compilers:/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:/soft/modulefiles:/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:/usr/share/modulefiles/Linux:/usr/share/modulefiles/Core:/usr/share/lmod/lmod/modulefiles/Core:/usr/share/lmod/lmod/modulefiles:/opt/cray/pals/lmod/modulefiles/core:/opt/cray/modulefiles:/opt/cray/pe/lmod/modulefiles/core:/opt/cray/pe/lmod/modulefiles/craype-targets/default:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:/soft/xalt/modulefiles"
declare -x MODULEPATH_ROOT="/usr/share/modulefiles"
declare -x MODULESHOME="/usr/share/lmod/lmod"
declare -x MORE="-sl"
declare -x MPI4JAX_USE_CUDA_MPI="1"
declare -x MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x MPICH_GPU_SUPPORT_ENABLED="1"
declare -x NCCL_IB_DISABLE="1"
declare -x NCCL_SOCKET_IFNAME="hsn"
declare -x NCPUS="64"
declare -x OFFLOAD_INIT="on_start"
declare -x OLDPWD
declare -x OMP_NUM_THREADS="4"
declare -x OSCAR_HOME="/opt/oscar"
declare -x OSTYPE="linux"
declare -x PAGER="less"
declare -x PALS_TRANSFER="0"
declare -x PATH="/soft/applications/conda/2024-04-29/mconda3/bin:/soft/applications/conda/2024-04-29/mconda3/condabin:/soft/xalt/3.0.2-202408282050/bin:/soft/compilers/cudatoolkit/cuda-12.4.1/bin:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:/opt/cray/pe/hdf5/1.12.2.9/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/home/shourya01/.local/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/shourya01/bin:/opt/cray/pe/bin"
declare -x PAT_RT_PERFCTR_DISABLE_COMPONENTS="nvml,rocm_smi"
declare -x PBS_ACCOUNT="ParaLLMs"
declare -x PBS_ENVIRONMENT="PBS_BATCH"
declare -x PBS_JOBCOOKIE="3F765E25396414247DB6C475394EE2AF"
declare -x PBS_JOBDIR="/home/shourya01"
declare -x PBS_JOBID="5238364.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_JOBNAME="trail-T"
declare -x PBS_MOMPORT="15003"
declare -x PBS_NODEFILE="/var/spool/pbs/aux/5238364.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_NODENUM="0"
declare -x PBS_O_HOME="/home/shourya01"
declare -x PBS_O_HOST="polaris-login-04.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_O_INTERACTIVE_AUTH_METHOD="resvport"
declare -x PBS_O_LANG="en_US.UTF-8"
declare -x PBS_O_LOGNAME="shourya01"
declare -x PBS_O_MAIL="/var/spool/mail/shourya01"
declare -x PBS_O_PATH="/home/shourya01/.local/bin:/home/shourya01/.vscode-server/cli/servers/Stable-dfaf44141ea9deb3b4096f7cd6d24e00c147a4b1/server/bin/remote-cli:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/soft/xalt/3.0.2-202408282050/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/nvidia/23.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/extras/qd/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/shourya01/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/dbhome/db2cat/sqllib/bin:/dbhome/db2cat/sqllib/adm:/dbhome/db2cat/sqllib/misc:/dbhome/db2cat/sqllib/gskit/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/opt/cray/pe/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.vscode-server/extensions/ms-python.debugpy-2025.8.0/bundled/scripts/noConfigScripts"
declare -x PBS_O_QUEUE="debug"
declare -x PBS_O_SHELL="/bin/bash"
declare -x PBS_O_SYSTEM="Linux"
declare -x PBS_O_WORKDIR="/home/shourya01"
declare -x PBS_QUEUE="debug"
declare -x PBS_TASKNUM="1"
declare -x PELOCAL_PRGENV="true"
declare -x PERFTOOLS_VERSION="23.12.0"
declare -x PE_DSMML_MODULE_NAME="cray-dsmml"
declare -x PE_DSMML_PKGCONFIG_LIBS="dsmml"
declare -x PE_ENV="GNU"
declare -x PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel:mpichf90"
declare -x PE_GCC_EXTERNAL="native"
declare -x PE_GCC_LEVEL="12"
declare -x PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig"
declare -x PE_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x PE_HDF5_PARALLEL_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel"
declare -x PE_HDF5_PARALLEL_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel"
declare -x PE_MPICH_FIXED_PRGENV="GNU"
declare -x PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90"
declare -x PE_MPICH_GENCOMPILERS_GNU="12.3"
declare -x PE_MPICH_GTL_DIR_amd_gfx906="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx908="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx90a="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx940="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx942="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia70="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia80="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia90="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_ponteVecchio="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_LIBS_amd_gfx906="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx908="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx90a="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx940="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx942="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_nvidia70="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia80="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia90="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_ponteVecchio="-lmpi_gtl_ze"
declare -x PE_MPICH_MODULE_NAME="cray-mpich"
declare -x PE_MPICH_PKGCONFIG_LIBS="mpich"
declare -x PE_MPICH_PKGCONFIG_VARIABLES="PE_MPICH_GTL_DIR_@accelerator@:PE_MPICH_GTL_LIBS_@accelerator@"
declare -x PE_PALS_PKGCONFIG_LIBS="libpals"
declare -x PE_PERFTOOLS_MPICH_LIBDIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib"
declare -x PE_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel:mpich:dsmml:darshan-runtime"
declare -x PE_PKGCONFIG_PRODUCTS="PE_PALS:PE_PMI:PE_MPICH:PE_DSMML"
declare -x PE_PMI_PKGCONFIG_LIBS="cray-pmi"
declare -x PE_PRODUCT_LIST="CRAYPE_X86_MILAN"
declare -x PKGCONFIG_ENABLED="1"
declare -x PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:/opt/cray/pals/1.3.4/lib/pkgconfig:/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:/opt/cray/pe/craype/2.7.30/pkg-config:/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig"
declare -x PROFILEREAD="true"
declare -x PWD="/home/shourya01"
declare -x PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages"
declare -x PYTHONUSERBASE="/home/shourya01/.local/polaris/conda/2024-04-29"
declare -x QT_SYSTEM_DIR="/usr/share/desktop-data"
declare -x SHELL="/bin/bash"
declare -x SHLVL="2"
declare -x SLURM_MPI_TYPE="cray_shasta"
declare -x STARSHIP_SESSION_KEY="2099012641105767"
declare -x STARSHIP_SHELL="bash"
declare -x TMPDIR="/var/tmp/pbs.5238364.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x TRITON_DISABLE_AUTOTUNE="1"
declare -x TZ="Etc/UTC"
declare -x USER="shourya01"
declare -x USE_PCM_DB="2"
declare -x WINDOWMANAGER="xterm"
declare -x XALT_DIR="/soft/xalt/3.0.2-202408282050"
declare -x XALT_EXECUTABLE_TRACKING="yes"
declare -x XALT_SAMPLING="no"
declare -x XALT_SCALAR_AND_SPSR_SAMPLING="yes"
declare -x XCURSOR_THEME="DMZ"
declare -x XDG_CONFIG_DIRS="/etc/xdg"
declare -x XDG_DATA_DIRS="/usr/share"
declare -x XKEYSYMDB="/usr/X11R6/lib/X11/XKeysymDB"
declare -x XLA_FLAGS="--xla_gpu_force_compilation_parallelism=1 --xla_gpu_cuda_data_dir=/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x XLA_PYTHON_CLIENT_PREALLOCATE="false"
declare -x XML_CATALOG_FILES="file:///soft/applications/conda/2024-04-29/mconda3/etc/xml/catalog file:///etc/xml/catalog"
declare -x XNLSPATH="/usr/X11R6/lib/X11/nls"
declare -x _CE_CONDA=""
declare -x _CE_M=""
declare -x _LMFILES_="/opt/cray/modulefiles/libfabric/1.15.2.0:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-network-ofi.lua:/opt/cray/pe/lmod/modulefiles/core/perftools-base/23.12.0.lua:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles/darshan/3.4.4:/soft/xalt/modulefiles/xalt/3.0.2-202408282050:/opt/cray/pe/lmod/modulefiles/core/gcc-native/12.3.lua:/opt/cray/pe/lmod/modulefiles/core/craype/2.7.30.lua:/opt/cray/pe/lmod/modulefiles/core/cray-dsmml/0.2.2.lua:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0/cray-mpich/8.1.28.lua:/opt/cray/pe/lmod/modulefiles/core/cray-pmi/6.1.13.lua:/opt/cray/pals/lmod/modulefiles/core/cray-pals/1.3.4.lua:/opt/cray/pals/lmod/modulefiles/core/cray-libpals/1.3.4.lua:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-x86-milan.lua:/opt/cray/pe/lmod/modulefiles/core/PrgEnv-gnu/8.5.0.lua:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2.9.lua:/soft/modulefiles/cudnn/9.1.0.lua:/soft/modulefiles/conda/2024-04-29.lua"
declare -x _ModuleTable001_="X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpQcmdFbnYgPSAiUHJnRW52LWdudSIsCmNvbXBpbGVyID0gImdjYy1uYXRpdmUiLApjcmF5cGUgPSAiY3JheXBlIiwKY3JheXBlX2NwdSA9ICJjcmF5cGUteDg2LW1pbGFuIiwKY3JheXBlX25ldHdvcmsgPSAiY3JheXBlLW5ldHdvcmstb2ZpIiwKZ2NjX2NvbXBpbGVyID0gImdjYy1uYXRpdmUiLApoZGY1ID0gImNyYXktaGRmNS1wYXJhbGxlbCIsCm1waSA9ICJjcmF5LW1waWNoIiwKcHl0aG9uID0gImNvbmRhIiwKfSwKbVQgPSB7ClsiUHJnRW52LWdudSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUv"
declare -x _ModuleTable002_="bG1vZC9tb2R1bGVmaWxlcy9jb3JlL1ByZ0Vudi1nbnUvOC41LjAubHVhIiwKZnVsbE5hbWUgPSAiUHJnRW52LWdudS84LjUuMCIsCmxvYWRPcmRlciA9IDE0LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gIlByZ0Vudi1nbnUiLAp3ViA9ICJeMDAwMDAwMDguMDAwMDAwMDA1Lip6ZmluYWwiLAp9LApjb25kYSA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY29uZGEvMjAyNC0wNC0yOS5sdWEiLApmdWxsTmFtZSA9ICJjb25kYS8yMDI0LTA0LTI5IiwKbG9hZE9yZGVyID0gMTcsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY29uZGEiLAp3ViA9ICJeMDAw"
declare -x _ModuleTable003_="MDIwMjQuKnpmaW5hbC0uMDAwMDAwMDA0Lip6ZmluYWwtLjAwMDAwMDAyOS4qemZpbmFsIiwKfSwKWyJjcmF5LWRzbW1sIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1kc21tbC8wLjIuMi5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWRzbW1sLzAuMi4yIiwKbG9hZE9yZGVyID0gOCwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWRzbW1sIiwKd1YgPSAiXjAwMDAwMDAwLjAwMDAwMDAwMi4wMDAwMDAwMDIuKnpmaW5hbCIsCn0sClsiY3JheS1oZGY1LXBhcmFsbGVsIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9v"
declare -x _ModuleTable004_="ZmkvMS4wL2NyYXktbXBpY2gvOC4wL2NyYXktaGRmNS1wYXJhbGxlbC8xLjEyLjIuOS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLApsb2FkT3JkZXIgPSAxNSwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDEyLjAwMDAwMDAwMi4wMDAwMDAwMDkuKnpmaW5hbCIsCn0sClsiY3JheS1saWJwYWxzIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wYWxzL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5LWxpYnBhbHMvMS4zLjQubHVhIiwKZnVsbE5hbWUgPSAiY3JheS1s"
declare -x _ModuleTable005_="aWJwYWxzLzEuMy40IiwKbG9hZE9yZGVyID0gMTIsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1saWJwYWxzIiwKd1YgPSAiXjAwMDAwMDAxLjAwMDAwMDAwMy4wMDAwMDAwMDQuKnpmaW5hbCIsCn0sClsiY3JheS1tcGljaCJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMC9jcmF5LW1waWNoLzguMS4yOC5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LW1waWNoLzguMS4yOCIsCmxvYWRPcmRlciA9IDksCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1tcGljaCIsCndWID0gIl4w"
declare -x _ModuleTable006_="MDAwMDAwOC4wMDAwMDAwMDEuMDAwMDAwMDI4Lip6ZmluYWwiLAp9LApbImNyYXktcGFscyJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGFscy9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wYWxzLzEuMy40Lmx1YSIsCmZ1bGxOYW1lID0gImNyYXktcGFscy8xLjMuNCIsCmxvYWRPcmRlciA9IDExLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcGFscyIsCndWID0gIl4wMDAwMDAwMS4wMDAwMDAwMDMuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImNyYXktcG1pIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wbWkvNi4xLjEzLmx1YSIsCmZ1bGxOYW1lID0gImNy"
declare -x _ModuleTable007_="YXktcG1pLzYuMS4xMyIsCmxvYWRPcmRlciA9IDEwLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcG1pIiwKd1YgPSAiXjAwMDAwMDA2LjAwMDAwMDAwMS4wMDAwMDAwMTMuKnpmaW5hbCIsCn0sCmNyYXlwZSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5cGUvMi43LjMwLmx1YSIsCmZ1bGxOYW1lID0gImNyYXlwZS8yLjcuMzAiLApsb2FkT3JkZXIgPSA3LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZSIsCndWID0gIl4wMDAwMDAwMi4wMDAwMDAwMDcuMDAwMDAwMDMwLip6ZmluYWwiLAp9LApb"
declare -x _ModuleTable008_="ImNyYXlwZS1uZXR3b3JrLW9maSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS1uZXR3b3JrLW9maS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5cGUtbmV0d29yay1vZmkiLApsb2FkT3JkZXIgPSAyLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS1uZXR3b3JrLW9maSIsCndWID0gIk0uKnpmaW5hbCIsCn0sClsiY3JheXBlLXg4Ni1taWxhbiJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS14ODYtbWlsYW4ubHVhIiwKZnVsbE5hbWUgPSAiY3JheXBl"
declare -x _ModuleTable009_="LXg4Ni1taWxhbiIsCmxvYWRPcmRlciA9IDEzLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS14ODYtbWlsYW4iLAp3ViA9ICJNLip6ZmluYWwiLAp9LApjdWRubiA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY3Vkbm4vOS4xLjAubHVhIiwKZnVsbE5hbWUgPSAiY3Vkbm4vOS4xLjAiLApsb2FkT3JkZXIgPSAxNiwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjdWRubi85LjEuMCIsCndWID0gIjAwMDAwMDAwOS4wMDAwMDAwMDEuKnpmaW5hbCIsCn0sCmRhcnNoYW4gPSB7CmZuID0gIi9zb2Z0L3BlcmZ0b29scy9k"
declare -x _ModuleTable010_="YXJzaGFuL2RhcnNoYW4tMy40LjQvc2hhcmUvY3JheXBlLTIueC9tb2R1bGVmaWxlcy9kYXJzaGFuLzMuNC40IiwKZnVsbE5hbWUgPSAiZGFyc2hhbi8zLjQuNCIsCmxvYWRPcmRlciA9IDQsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiZGFyc2hhbiIsCndWID0gIjAwMDAwMDAwMy4wMDAwMDAwMDQuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImdjYy1uYXRpdmUiXSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9nY2MtbmF0aXZlLzEyLjMubHVhIiwKZnVsbE5hbWUgPSAiZ2NjLW5hdGl2ZS8xMi4zIiwKbG9hZE9yZGVyID0gNiwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0"
declare -x _ModuleTable011_="YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJnY2MtbmF0aXZlIiwKd1YgPSAiXjAwMDAwMDEyLjAwMDAwMDAwMy4qemZpbmFsIiwKfSwKbGliZmFicmljID0gewpmbiA9ICIvb3B0L2NyYXkvbW9kdWxlZmlsZXMvbGliZmFicmljLzEuMTUuMi4wIiwKZnVsbE5hbWUgPSAibGliZmFicmljLzEuMTUuMi4wIiwKbG9hZE9yZGVyID0gMSwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJsaWJmYWJyaWMiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDE1LjAwMDAwMDAwMi4qemZpbmFsIiwKfSwKWyJwZXJmdG9vbHMtYmFzZSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb3JlL3BlcmZ0b29s"
declare -x _ModuleTable012_="cy1iYXNlLzIzLjEyLjAubHVhIiwKZnVsbE5hbWUgPSAicGVyZnRvb2xzLWJhc2UvMjMuMTIuMCIsCmxvYWRPcmRlciA9IDMsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAicGVyZnRvb2xzLWJhc2UiLAp3ViA9ICJeMDAwMDAwMjMuMDAwMDAwMDEyLip6ZmluYWwiLAp9LAp4YWx0ID0gewpmbiA9ICIvc29mdC94YWx0L21vZHVsZWZpbGVzL3hhbHQvMy4wLjItMjAyNDA4MjgyMDUwIiwKZnVsbE5hbWUgPSAieGFsdC8zLjAuMi0yMDI0MDgyODIwNTAiLApsb2FkT3JkZXIgPSA1LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gInhhbHQiLAp3ViA9ICJeMDAwMDAw"
declare -x _ModuleTable013_="MDMuMDAwMDAwMDAwLjAwMDAwMDAwMi4qemZpbmFsLS4yMDI0MDgyODIwNTAuKnpmaW5hbCIsCn0sCn0sCm1wYXRoQSA9IHsKCiIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9oZGY1LXBhcmFsbGVsL2dudS8xMi4wL29maS8xLjAvY3JheS1tcGljaC84LjAvY3JheS1oZGY1LXBhcmFsbGVsLzEuMTIuMiIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3B1L3g4Ni1taWxhbi8xLjAiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9vZmkvMS4wL2NyYXktbXBpY2gvOC4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMCIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxl"
declare -x _ModuleTable014_="ZmlsZXMvbWl4X2NvbXBpbGVycyIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29tcGlsZXIvZ251LzEyLjAiLCAiL3NvZnQvbW9kdWxlZmlsZXMiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL3BlcmZ0b29scy8yMy4xMi4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9uZXQvb2ZpLzEuMCIsICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0xpbnV4IgosICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmUiLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZSIKLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BhbHMvbG1vZC9tb2R1bGVmaWxlcy9jb3JlIgosICIvb3B0L2Ny"
declare -x _ModuleTable015_="YXkvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZSIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdCIKLCAiL3NvZnQvcGVyZnRvb2xzL2RhcnNoYW4vZGFyc2hhbi0zLjQuNC9zaGFyZS9jcmF5cGUtMi54L21vZHVsZWZpbGVzIiwgIi9zb2Z0L3hhbHQvbW9kdWxlZmlsZXMiLAp9LApzeXN0ZW1CYXNlTVBBVEggPSAiL3Vzci9zaGFyZS9tb2R1bGVmaWxlcy9MaW51eDovdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmU6L3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZTovdXNyL3NoYXJlL2xtb2QvbG1vZC9tb2R1bGVmaWxlczovb3B0L2NyYXkvcGFscy9sbW9kL21vZHVs"
declare -x _ModuleTable016_="ZWZpbGVzL2NvcmU6L29wdC9jcmF5L21vZHVsZWZpbGVzOi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmU6L29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdDovc29mdC9wZXJmdG9vbHMvZGFyc2hhbi9kYXJzaGFuLTMuNC40L3NoYXJlL2NyYXlwZS0yLngvbW9kdWxlZmlsZXM6L3NvZnQveGFsdC9tb2R1bGVmaWxlcyIsCn0K"
declare -x _ModuleTable_Sz_="16"
declare -x __LMOD_Priority_PATH="/soft/xalt/3.0.2-202408282050/bin:-100"
declare -x __LMOD_REF_COUNT_COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin:1"
declare -x __LMOD_REF_COUNT_CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:1;/opt/cray/pe/pmi/6.1.13/lib:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:1;/opt/cray/pe/mpich/8.1.28/gtl/lib:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib:1;/opt/cray/pe/perftools/23.12.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:1;/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:1;/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:1;/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:1;/soft/perftools/darshan/darshan-3.4.4/lib:1;/opt/cray/pe/papi/7.0.1.2/lib64:1;/opt/cray/libfabric/1.15.2.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so:1"
declare -x __LMOD_REF_COUNT_MANPATH="/opt/cray/pals/1.3.4/man:2;/opt/cray/pe/pmi/6.1.13/man:1;/opt/cray/pe/mpich/8.1.28/ofi/man:1;/opt/cray/pe/mpich/8.1.28/man/mpich:1;/opt/cray/pe/dsmml/0.2.2/dsmml/man:1;/opt/cray/pe/craype/2.7.30/man:1;/opt/cray/pe/perftools/23.12.0/man:1;/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:1;/opt/cray/libfabric/1.15.2.0/share/man:1;/usr/share/lmod/lmod/share/man:1;/home/shourya01/.local/man:1;/usr/local/man:1;/usr/share/man:1;/usr/man:1;/opt/c3/man:1;/opt/pbs/share/man:1;/opt/clmgr/man:1;/opt/sgi/share/man:1;/opt/clmgr/share/man:1;/opt/clmgr/lib/cm-cli/man:1"
declare -x __LMOD_REF_COUNT_MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:1;/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:1;/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:1;/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:1;/opt/cray/pe/lmod/modulefiles/mix_compilers:1;/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:1;/soft/modulefiles:1;/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:1;/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:1;/usr/share/modulefiles/Linux:1;/usr/share/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles:1;/opt/cray/pals/lmod/modulefiles/core:1;/opt/cray/modulefiles:1;/opt/cray/pe/lmod/modulefiles/core:1;/opt/cray/pe/lmod/modulefiles/craype-targets/default:1;/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:1;/soft/xalt/modulefiles:1"
declare -x __LMOD_REF_COUNT_PATH="/soft/xalt/3.0.2-202408282050/bin:1;/soft/compilers/cudatoolkit/cuda-12.4.1/bin:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:1;/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:1;/opt/cray/pe/hdf5/1.12.2.9/bin:1;/opt/cray/pals/1.3.4/bin:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:1;/opt/cray/pe/mpich/8.1.28/bin:1;/opt/cray/pe/craype/2.7.30/bin:1;/home/shourya01/.local/bin:4;/soft/perftools/darshan/darshan-3.4.4/bin:1;/opt/cray/pe/perftools/23.12.0/bin:1;/opt/cray/pe/papi/7.0.1.2/bin:1;/opt/cray/libfabric/1.15.2.0/bin:1;/opt/clmgr/sbin:1;/opt/clmgr/bin:1;/opt/sgi/sbin:1;/opt/sgi/bin:1;/usr/local/bin:1;/usr/bin:1;/bin:2;/opt/c3/bin:1;/usr/lib/mit/bin:1;/usr/lib/mit/sbin:1;/opt/pbs/bin:1;/sbin:1;/home/shourya01/bin:1;/opt/cray/pe/bin:1"
declare -x __LMOD_REF_COUNT_PE_DSMML_PKGCONFIG_LIBS="dsmml:1"
declare -x __LMOD_REF_COUNT_PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:1;hdf5_fortran_parallel:1;mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FIXED_PRGENV="GNU:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_GENCOMPILERS_GNU="12.3:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_PKGCONFIG_LIBS="mpich:1"
declare -x __LMOD_REF_COUNT_PE_PALS_PKGCONFIG_LIBS="libpals:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_LIBS="hdf5_hl_parallel:1;hdf5_parallel:1;mpich:1;dsmml:1;darshan-runtime:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_PRODUCTS="PE_PALS:1;PE_PMI:1;PE_MPICH:1;PE_DSMML:1"
declare -x __LMOD_REF_COUNT_PE_PMI_PKGCONFIG_LIBS="cray-pmi:1"
declare -x __LMOD_REF_COUNT_PE_PRODUCT_LIST="CRAYPE_X86_MILAN:1;PERFTOOLS:1;CRAYPAT:1"
declare -x __LMOD_REF_COUNT_PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:1;/opt/cray/pals/1.3.4/lib/pkgconfig:1;/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:1;/opt/cray/pe/craype/2.7.30/pkg-config:1;/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:1;/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages:1"
declare -x ftp_proxy="http://proxy.alcf.anl.gov:3128"
declare -x http_proxy="http://proxy.alcf.anl.gov:3128"
declare -x https_proxy="http://proxy.alcf.anl.gov:3128"
declare -x no_proxy="admin,polaris-adminvm-01,localhost,*.cm.polaris.alcf.anl.gov,polaris-*,*.polaris.alcf.anl.gov,*.alcf.anl.gov"
Running on 2 nodes
Total number of GPUs: 8
Connected to tcp://x3006c0s19b0n0.hsn.cm.polaris.alcf.anl.gov:7919
Found executable /soft/applications/conda/2024-04-29/mconda3/bin/python
Launching application 17f4a9f2-ac14-47ba-add6-eb30a7f47e95
Using PMI port 40488,40489
[2025-06-25 00:18:47,528] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:18:47,528] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:18:47,528] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:18:47,528] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:18:48,919] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:18:48,919] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:18:48,919] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:18:48,919] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:18:53,232] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:18:53,232] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:18:53,233] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:18:53,233] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:18:53,233] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-25 00:18:53,233] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:18:53,233] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:18:53,233] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:18:54,488] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:18:54,489] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=8, master_addr=10.140.57.105, master_port=29500
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:18:54,488] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:18:54,488] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-25 00:18:54,489] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=8, master_addr=10.140.57.105, master_port=29500
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:18:54,488] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:18:54,488] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-25 00:18:54,489] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=2, world_size=8, master_addr=10.140.57.105, master_port=29500
[2025-06-25 00:18:54,489] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=8, master_addr=10.140.57.105, master_port=29500
[2025-06-25 00:18:54,489] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=8, master_addr=10.140.57.105, master_port=29500
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:18:54,488] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:18:54,488] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-25 00:18:54,489] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=3, world_size=8, master_addr=10.140.57.105, master_port=29500
[2025-06-25 00:18:54,489] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-06-25 00:18:54,488] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-25 00:18:54,489] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=0, world_size=8, master_addr=10.140.57.105, master_port=29500
[2025-06-25 00:18:54,489] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=1, world_size=8, master_addr=10.140.57.105, master_port=29500
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
Initialized deepspeed on global rank 0, local rank 0 with world size 8.
[2025-06-25 00:18:59,870] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2+5f631abc, git-hash=5f631abc, git-branch=HEAD
[2025-06-25 00:19:03,993] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-06-25 00:19:03,995] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-06-25 00:19:03,995] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
Initialized deepspeed on global rank 2, local rank 2 with world size 8.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/shourya01/stormer_deepspeed/train_tfm_parallel.py", line 273, in <module>
[rank2]:     _ = train_one_epoch_tfm_parallel(
[rank2]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]: TypeError: train_one_epoch_tfm_parallel() got an unexpected keyword argument 'two_args_for_model'
Initialized deepspeed on global rank 3, local rank 3 with world size 8.
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/shourya01/stormer_deepspeed/train_tfm_parallel.py", line 273, in <module>
[rank3]:     _ = train_one_epoch_tfm_parallel(
[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]: TypeError: train_one_epoch_tfm_parallel() got an unexpected keyword argument 'two_args_for_model'
[2025-06-25 00:19:04,064] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-06-25 00:19:04,064] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-06-25 00:19:04,064] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-06-25 00:19:04,064] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-06-25 00:19:04,064] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0005], mom=[(0.9, 0.999)]
[2025-06-25 00:19:04,065] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   amp_params ................... False
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x152a75591bd0>
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2025-06-25 00:19:04,065] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   dump_state ................... False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
Initialized deepspeed on global rank 5, local rank 1 with world size 8.
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   global_rank .................. 0
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 0.0005, 'weight_decay': 0.01}
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   pld_params ................... False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   steps_per_print .............. 100000
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   train_batch_size ............. 1024
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   world_size ................... 8
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[rank5]: Traceback (most recent call last):
[rank5]:   File "/home/shourya01/stormer_deepspeed/train_tfm_parallel.py", line 273, in <module>
[rank5]:     _ = train_one_epoch_tfm_parallel(
[rank5]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]: TypeError: train_one_epoch_tfm_parallel() got an unexpected keyword argument 'two_args_for_model'
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2025-06-25 00:19:04,066] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2025-06-25 00:19:04,066] [INFO] [config.py:986:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 128, 
    "train_batch_size": 1.024000e+03, 
    "steps_per_print": 1.000000e+05, 
    "gradient_accumulation_steps": 1, 
    "fp16": {
        "enabled": false
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0005, 
            "weight_decay": 0.01
        }
    }, 
    "comms_logger": {
        "enabled": true, 
        "verbose": false
    }, 
    "zero_optimization": {
        "stage": 0
    }
}
Initialized deepspeed on global rank 6, local rank 2 with world size 8.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/shourya01/stormer_deepspeed/train_tfm_parallel.py", line 273, in <module>
[rank0]:     _ = train_one_epoch_tfm_parallel(
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: TypeError: train_one_epoch_tfm_parallel() got an unexpected keyword argument 'two_args_for_model'
Initialized deepspeed on global rank 7, local rank 3 with world size 8.
Initialized deepspeed on global rank 4, local rank 0 with world size 8.
[rank4]: Traceback (most recent call last):
[rank4]:   File "/home/shourya01/stormer_deepspeed/train_tfm_parallel.py", line 273, in <module>
[rank4]:     _ = train_one_epoch_tfm_parallel(
[rank4]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]: TypeError: train_one_epoch_tfm_parallel() got an unexpected keyword argument 'two_args_for_model'
[rank6]: Traceback (most recent call last):
[rank6]:   File "/home/shourya01/stormer_deepspeed/train_tfm_parallel.py", line 273, in <module>
[rank6]:     _ = train_one_epoch_tfm_parallel(
[rank6]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]: TypeError: train_one_epoch_tfm_parallel() got an unexpected keyword argument 'two_args_for_model'
[rank7]: Traceback (most recent call last):
[rank7]:   File "/home/shourya01/stormer_deepspeed/train_tfm_parallel.py", line 273, in <module>
[rank7]:     _ = train_one_epoch_tfm_parallel(
[rank7]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]: TypeError: train_one_epoch_tfm_parallel() got an unexpected keyword argument 'two_args_for_model'
Initialized deepspeed on global rank 1, local rank 1 with world size 8.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/shourya01/stormer_deepspeed/train_tfm_parallel.py", line 273, in <module>
[rank1]:     _ = train_one_epoch_tfm_parallel(
[rank1]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: TypeError: train_one_epoch_tfm_parallel() got an unexpected keyword argument 'two_args_for_model'
x3006c0s19b0n0.hsn.cm.polaris.alcf.anl.gov: rank 0 exited with code 1
x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov: rank 4 exited with code 1
x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov: rank 7 exited with code 1
x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov: rank 5 exited with code 1
x3006c0s19b0n0.hsn.cm.polaris.alcf.anl.gov: rank 1 exited with code 1
x3006c0s19b0n0.hsn.cm.polaris.alcf.anl.gov: rank 3 exited with code 1
x3006c0s19b0n0.hsn.cm.polaris.alcf.anl.gov: rank 2 exited with code 1
x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov: rank 6 exited with code 1
Application 17f4a9f2 resources: utime=79s stime=53s maxrss=4879020KB inblock=4309992 oublock=144 minflt=3730922 majflt=14153 nvcsw=507472 nivcsw=111822
Training completed
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled

Lmod is automatically replacing "nvhpc/23.9" with "gcc-native/12.3".


Lmod is automatically replacing "PrgEnv-nvhpc/8.5.0" with "PrgEnv-gnu/8.5.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.28

declare -x APP2_STATE="23.12.0"
declare -x BASH_ENV="/usr/share/lmod/lmod/init/bash"
declare -x C3_RSH="ssh -oConnectTimeout=10 -oForwardX11=no"
declare -x CFLAGS="-I/soft/applications/conda/2024-04-29/mconda3/include"
declare -x COLORTERM="1"
declare -x COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin"
declare -x CONDA_DEFAULT_ENV="base"
declare -x CONDA_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/conda"
declare -x CONDA_PREFIX="/soft/applications/conda/2024-04-29/mconda3"
declare -x CONDA_PROMPT_MODIFIER="(2024-04-29/base) "
declare -x CONDA_PYTHON_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/python"
declare -x CONDA_SHLVL="1"
declare -x CPU="x86_64"
declare -x CRAYPAT_LD_LIBRARY_PATH="/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAYPAT_OPTS_EXECUTABLE="libexec64/opts"
declare -x CRAYPAT_ROOT="/opt/cray/pe/perftools/23.12.0"
declare -x CRAYPE_DIR="/opt/cray/pe/craype/2.7.30"
declare -x CRAYPE_NETWORK_TARGET="ofi"
declare -x CRAYPE_VERSION="2.7.30"
declare -x CRAY_CPU_TARGET="x86-milan"
declare -x CRAY_DSMML_BASEDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_DIR="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_PREFIX="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_ROOTDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_VER="0.2.2"
declare -x CRAY_DSMML_VERSION="0.2.2"
declare -x CRAY_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x CRAY_HDF5_PARALLEL_PREFIX="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x CRAY_HDF5_PARALLEL_VERSION="1.12.2.9"
declare -x CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:/opt/cray/pe/pmi/6.1.13/lib:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:/opt/cray/pe/mpich/8.1.28/gtl/lib:/opt/cray/pe/dsmml/0.2.2/dsmml/lib:/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAY_LMOD_COMPILER="gnu/12.0"
declare -x CRAY_LMOD_CPU="x86-milan/1.0"
declare -x CRAY_LMOD_MPI="cray-mpich/8.0"
declare -x CRAY_LMOD_NET="ofi/1.0"
declare -x CRAY_MPICH_BASEDIR="/opt/cray/pe/mpich/8.1.28/ofi"
declare -x CRAY_MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_PREFIX="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_ROOTDIR="/opt/cray/pe/mpich/8.1.28"
declare -x CRAY_MPICH_VER="8.1.28"
declare -x CRAY_MPICH_VERSION="8.1.28"
declare -x CRAY_PERFTOOLS_PREFIX="/opt/cray/pe/perftools/23.12.0"
declare -x CRAY_PERFTOOLS_VERSION="23.12.0"
declare -x CRAY_PMI_INCLUDE_OPTS="-I/opt/cray/pe/pmi/6.1.13/include"
declare -x CRAY_PMI_POST_LINK_OPTS="-L/opt/cray/pe/pmi/6.1.13/lib"
declare -x CRAY_PMI_PREFIX="/opt/cray/pe/pmi/6.1.13"
declare -x CRAY_PMI_VERSION="6.1.13"
declare -x CSHEDIT="emacs"
declare -x CUDA_HOME="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_TOOLKIT_BASE="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDNN_HOME="/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/"
declare -x ENVIRONMENT="BATCH"
declare -x ENV_NAME="conda/2024-04-29"
declare -x FROM_HEADER=""
declare -x GCC_PATH="/usr/bin"
declare -x GCC_PREFIX="/usr/lib64/gcc/x86_64-suse-linux/12"
declare -x GCC_VERSION="12.3"
declare -x GNU_VERSION="12.3"
declare -x GPG_TTY="not a tty"
declare -x GSETTINGS_SCHEMA_DIR="/soft/applications/conda/2024-04-29/mconda3/share/glib-2.0/schemas"
declare -x GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=""
declare -x G_BROKEN_FILENAMES="1"
declare -x G_FILENAME_ENCODING="@locale,UTF-8,ISO-8859-15,CP1252"
declare -x HDF5_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HDF5_ROOT="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HISTSIZE="1000"
declare -x HOME="/home/shourya01"
declare -x HOST="x3006c0s19b0n0"
declare -x HOSTNAME="x3006c0s19b0n0"
declare -x HOSTTYPE="x86_64"
declare -x HTTPS_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x HTTP_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x LANG="en_US.UTF-8"
declare -x LANGUAGE="en_US.UTF-8"
declare -x LDFLAGS="-L/soft/applications/conda/2024-04-29/mconda3/lib -Wl,--enable-new-dtags,-rpath,/soft/applications/conda/2024-04-29/mconda3/lib"
declare -x LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:/soft/perftools/darshan/darshan-3.4.4/lib:/opt/cray/pe/papi/7.0.1.2/lib64:/opt/cray/libfabric/1.15.2.0/lib64"
declare -x LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so"
declare -x LESS="-M -I -R"
declare -x LESSCLOSE="lessclose.sh %s %s"
declare -x LESSKEY="/etc/lesskey.bin"
declare -x LESSOPEN="lessopen.sh %s"
declare -x LESS_ADVANCED_PREPROCESSOR="no"
declare -x LMOD_CMD="/usr/share/lmod/lmod/libexec/lmod"
declare -x LMOD_DIR="/usr/share/lmod/lmod/libexec"
declare -x LMOD_FAMILY_COMPILER="gcc-native"
declare -x LMOD_FAMILY_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_CRAYPE="craype"
declare -x LMOD_FAMILY_CRAYPE_CPU="craype-x86-milan"
declare -x LMOD_FAMILY_CRAYPE_CPU_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_NETWORK="craype-network-ofi"
declare -x LMOD_FAMILY_CRAYPE_NETWORK_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_VERSION="2.7.30"
declare -x LMOD_FAMILY_GCC_COMPILER="gcc-native"
declare -x LMOD_FAMILY_GCC_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_HDF5="cray-hdf5-parallel"
declare -x LMOD_FAMILY_HDF5_VERSION="1.12.2.9"
declare -x LMOD_FAMILY_MPI="cray-mpich"
declare -x LMOD_FAMILY_MPI_VERSION="8.1.28"
declare -x LMOD_FAMILY_PRGENV="PrgEnv-gnu"
declare -x LMOD_FAMILY_PRGENV_VERSION="8.5.0"
declare -x LMOD_FAMILY_PYTHON="conda"
declare -x LMOD_FAMILY_PYTHON_VERSION="2024-04-29"
declare -x LMOD_PKG="/usr/share/lmod/lmod"
declare -x LMOD_ROOT="/usr/share/lmod"
declare -x LMOD_SETTARG_FULL_SUPPORT="no"
declare -x LMOD_SYSTEM_DEFAULT_MODULES="PrgEnv-nvhpc:craype-network-ofi:perftools-base:darshan:xalt"
declare -x LMOD_VERSION="8.7.34"
declare -x LMOD_sys="Linux"
declare -x LOADEDMODULES="libfabric/1.15.2.0:craype-network-ofi:perftools-base/23.12.0:darshan/3.4.4:xalt/3.0.2-202408282050:gcc-native/12.3:craype/2.7.30:cray-dsmml/0.2.2:cray-mpich/8.1.28:cray-pmi/6.1.13:cray-pals/1.3.4:cray-libpals/1.3.4:craype-x86-milan:PrgEnv-gnu/8.5.0:cray-hdf5-parallel/1.12.2.9:cudnn/9.1.0:conda/2024-04-29"
declare -x LOGNAME="shourya01"
declare -x MACHTYPE="x86_64-suse-linux"
declare -x MAIL="/var/spool/mail/shourya01"
declare -x MANPATH="/opt/cray/pals/1.3.4/man:/opt/cray/pe/pmi/6.1.13/man:/opt/cray/pe/mpich/8.1.28/ofi/man:/opt/cray/pe/mpich/8.1.28/man/mpich:/opt/cray/pe/dsmml/0.2.2/dsmml/man:/opt/cray/pe/craype/2.7.30/man:/opt/cray/pe/perftools/23.12.0/man:/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:/opt/cray/libfabric/1.15.2.0/share/man:/usr/share/lmod/lmod/share/man:/home/shourya01/.local/man:/usr/local/man:/usr/share/man:/usr/man:/opt/c3/man:/opt/pbs/share/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man"
declare -x MINICOM="-c on"
declare -x MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:/opt/cray/pe/lmod/modulefiles/mix_compilers:/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:/soft/modulefiles:/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:/usr/share/modulefiles/Linux:/usr/share/modulefiles/Core:/usr/share/lmod/lmod/modulefiles/Core:/usr/share/lmod/lmod/modulefiles:/opt/cray/pals/lmod/modulefiles/core:/opt/cray/modulefiles:/opt/cray/pe/lmod/modulefiles/core:/opt/cray/pe/lmod/modulefiles/craype-targets/default:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:/soft/xalt/modulefiles"
declare -x MODULEPATH_ROOT="/usr/share/modulefiles"
declare -x MODULESHOME="/usr/share/lmod/lmod"
declare -x MORE="-sl"
declare -x MPI4JAX_USE_CUDA_MPI="1"
declare -x MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x MPICH_GPU_SUPPORT_ENABLED="1"
declare -x NCCL_IB_DISABLE="1"
declare -x NCCL_SOCKET_IFNAME="hsn"
declare -x NCPUS="64"
declare -x OFFLOAD_INIT="on_start"
declare -x OLDPWD
declare -x OMP_NUM_THREADS="4"
declare -x OSCAR_HOME="/opt/oscar"
declare -x OSTYPE="linux"
declare -x PAGER="less"
declare -x PALS_TRANSFER="0"
declare -x PATH="/soft/applications/conda/2024-04-29/mconda3/bin:/soft/applications/conda/2024-04-29/mconda3/condabin:/soft/xalt/3.0.2-202408282050/bin:/soft/compilers/cudatoolkit/cuda-12.4.1/bin:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:/opt/cray/pe/hdf5/1.12.2.9/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/home/shourya01/.local/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/shourya01/bin:/opt/cray/pe/bin"
declare -x PAT_RT_PERFCTR_DISABLE_COMPONENTS="nvml,rocm_smi"
declare -x PBS_ACCOUNT="ParaLLMs"
declare -x PBS_ENVIRONMENT="PBS_BATCH"
declare -x PBS_JOBCOOKIE="4FD685BB75E4B65462F47F5F564047EE"
declare -x PBS_JOBDIR="/home/shourya01"
declare -x PBS_JOBID="5238386.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_JOBNAME="parallel-tfm-trial"
declare -x PBS_MOMPORT="15003"
declare -x PBS_NODEFILE="/var/spool/pbs/aux/5238386.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_NODENUM="0"
declare -x PBS_O_HOME="/home/shourya01"
declare -x PBS_O_HOST="polaris-login-04.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_O_INTERACTIVE_AUTH_METHOD="resvport"
declare -x PBS_O_LANG="en_US.UTF-8"
declare -x PBS_O_LOGNAME="shourya01"
declare -x PBS_O_MAIL="/var/spool/mail/shourya01"
declare -x PBS_O_PATH="/home/shourya01/.local/bin:/home/shourya01/.vscode-server/cli/servers/Stable-dfaf44141ea9deb3b4096f7cd6d24e00c147a4b1/server/bin/remote-cli:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/soft/xalt/3.0.2-202408282050/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/nvidia/23.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/extras/qd/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/shourya01/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/dbhome/db2cat/sqllib/bin:/dbhome/db2cat/sqllib/adm:/dbhome/db2cat/sqllib/misc:/dbhome/db2cat/sqllib/gskit/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/opt/cray/pe/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.vscode-server/extensions/ms-python.debugpy-2025.8.0/bundled/scripts/noConfigScripts"
declare -x PBS_O_QUEUE="debug"
declare -x PBS_O_SHELL="/bin/bash"
declare -x PBS_O_SYSTEM="Linux"
declare -x PBS_O_WORKDIR="/home/shourya01"
declare -x PBS_QUEUE="debug"
declare -x PBS_TASKNUM="1"
declare -x PELOCAL_PRGENV="true"
declare -x PERFTOOLS_VERSION="23.12.0"
declare -x PE_DSMML_MODULE_NAME="cray-dsmml"
declare -x PE_DSMML_PKGCONFIG_LIBS="dsmml"
declare -x PE_ENV="GNU"
declare -x PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel:mpichf90"
declare -x PE_GCC_EXTERNAL="native"
declare -x PE_GCC_LEVEL="12"
declare -x PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig"
declare -x PE_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x PE_HDF5_PARALLEL_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel"
declare -x PE_HDF5_PARALLEL_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel"
declare -x PE_MPICH_FIXED_PRGENV="GNU"
declare -x PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90"
declare -x PE_MPICH_GENCOMPILERS_GNU="12.3"
declare -x PE_MPICH_GTL_DIR_amd_gfx906="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx908="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx90a="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx940="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx942="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia70="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia80="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia90="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_ponteVecchio="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_LIBS_amd_gfx906="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx908="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx90a="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx940="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx942="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_nvidia70="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia80="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia90="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_ponteVecchio="-lmpi_gtl_ze"
declare -x PE_MPICH_MODULE_NAME="cray-mpich"
declare -x PE_MPICH_PKGCONFIG_LIBS="mpich"
declare -x PE_MPICH_PKGCONFIG_VARIABLES="PE_MPICH_GTL_DIR_@accelerator@:PE_MPICH_GTL_LIBS_@accelerator@"
declare -x PE_PALS_PKGCONFIG_LIBS="libpals"
declare -x PE_PERFTOOLS_MPICH_LIBDIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib"
declare -x PE_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel:mpich:dsmml:darshan-runtime"
declare -x PE_PKGCONFIG_PRODUCTS="PE_PALS:PE_PMI:PE_MPICH:PE_DSMML"
declare -x PE_PMI_PKGCONFIG_LIBS="cray-pmi"
declare -x PE_PRODUCT_LIST="CRAYPE_X86_MILAN"
declare -x PKGCONFIG_ENABLED="1"
declare -x PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:/opt/cray/pals/1.3.4/lib/pkgconfig:/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:/opt/cray/pe/craype/2.7.30/pkg-config:/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig"
declare -x PROFILEREAD="true"
declare -x PWD="/home/shourya01"
declare -x PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages"
declare -x PYTHONUSERBASE="/home/shourya01/.local/polaris/conda/2024-04-29"
declare -x QT_SYSTEM_DIR="/usr/share/desktop-data"
declare -x SHELL="/bin/bash"
declare -x SHLVL="2"
declare -x SLURM_MPI_TYPE="cray_shasta"
declare -x STARSHIP_SESSION_KEY="8889228631484024"
declare -x STARSHIP_SHELL="bash"
declare -x TMPDIR="/var/tmp/pbs.5238386.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x TRITON_DISABLE_AUTOTUNE="1"
declare -x TZ="Etc/UTC"
declare -x USER="shourya01"
declare -x USE_PCM_DB="2"
declare -x WINDOWMANAGER="xterm"
declare -x XALT_DIR="/soft/xalt/3.0.2-202408282050"
declare -x XALT_EXECUTABLE_TRACKING="yes"
declare -x XALT_SAMPLING="no"
declare -x XALT_SCALAR_AND_SPSR_SAMPLING="yes"
declare -x XCURSOR_THEME="DMZ"
declare -x XDG_CONFIG_DIRS="/etc/xdg"
declare -x XDG_DATA_DIRS="/usr/share"
declare -x XKEYSYMDB="/usr/X11R6/lib/X11/XKeysymDB"
declare -x XLA_FLAGS="--xla_gpu_force_compilation_parallelism=1 --xla_gpu_cuda_data_dir=/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x XLA_PYTHON_CLIENT_PREALLOCATE="false"
declare -x XML_CATALOG_FILES="file:///soft/applications/conda/2024-04-29/mconda3/etc/xml/catalog file:///etc/xml/catalog"
declare -x XNLSPATH="/usr/X11R6/lib/X11/nls"
declare -x _CE_CONDA=""
declare -x _CE_M=""
declare -x _LMFILES_="/opt/cray/modulefiles/libfabric/1.15.2.0:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-network-ofi.lua:/opt/cray/pe/lmod/modulefiles/core/perftools-base/23.12.0.lua:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles/darshan/3.4.4:/soft/xalt/modulefiles/xalt/3.0.2-202408282050:/opt/cray/pe/lmod/modulefiles/core/gcc-native/12.3.lua:/opt/cray/pe/lmod/modulefiles/core/craype/2.7.30.lua:/opt/cray/pe/lmod/modulefiles/core/cray-dsmml/0.2.2.lua:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0/cray-mpich/8.1.28.lua:/opt/cray/pe/lmod/modulefiles/core/cray-pmi/6.1.13.lua:/opt/cray/pals/lmod/modulefiles/core/cray-pals/1.3.4.lua:/opt/cray/pals/lmod/modulefiles/core/cray-libpals/1.3.4.lua:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-x86-milan.lua:/opt/cray/pe/lmod/modulefiles/core/PrgEnv-gnu/8.5.0.lua:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2.9.lua:/soft/modulefiles/cudnn/9.1.0.lua:/soft/modulefiles/conda/2024-04-29.lua"
declare -x _ModuleTable001_="X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpQcmdFbnYgPSAiUHJnRW52LWdudSIsCmNvbXBpbGVyID0gImdjYy1uYXRpdmUiLApjcmF5cGUgPSAiY3JheXBlIiwKY3JheXBlX2NwdSA9ICJjcmF5cGUteDg2LW1pbGFuIiwKY3JheXBlX25ldHdvcmsgPSAiY3JheXBlLW5ldHdvcmstb2ZpIiwKZ2NjX2NvbXBpbGVyID0gImdjYy1uYXRpdmUiLApoZGY1ID0gImNyYXktaGRmNS1wYXJhbGxlbCIsCm1waSA9ICJjcmF5LW1waWNoIiwKcHl0aG9uID0gImNvbmRhIiwKfSwKbVQgPSB7ClsiUHJnRW52LWdudSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUv"
declare -x _ModuleTable002_="bG1vZC9tb2R1bGVmaWxlcy9jb3JlL1ByZ0Vudi1nbnUvOC41LjAubHVhIiwKZnVsbE5hbWUgPSAiUHJnRW52LWdudS84LjUuMCIsCmxvYWRPcmRlciA9IDE0LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gIlByZ0Vudi1nbnUiLAp3ViA9ICJeMDAwMDAwMDguMDAwMDAwMDA1Lip6ZmluYWwiLAp9LApjb25kYSA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY29uZGEvMjAyNC0wNC0yOS5sdWEiLApmdWxsTmFtZSA9ICJjb25kYS8yMDI0LTA0LTI5IiwKbG9hZE9yZGVyID0gMTcsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY29uZGEiLAp3ViA9ICJeMDAw"
declare -x _ModuleTable003_="MDIwMjQuKnpmaW5hbC0uMDAwMDAwMDA0Lip6ZmluYWwtLjAwMDAwMDAyOS4qemZpbmFsIiwKfSwKWyJjcmF5LWRzbW1sIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1kc21tbC8wLjIuMi5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWRzbW1sLzAuMi4yIiwKbG9hZE9yZGVyID0gOCwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWRzbW1sIiwKd1YgPSAiXjAwMDAwMDAwLjAwMDAwMDAwMi4wMDAwMDAwMDIuKnpmaW5hbCIsCn0sClsiY3JheS1oZGY1LXBhcmFsbGVsIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9v"
declare -x _ModuleTable004_="ZmkvMS4wL2NyYXktbXBpY2gvOC4wL2NyYXktaGRmNS1wYXJhbGxlbC8xLjEyLjIuOS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLApsb2FkT3JkZXIgPSAxNSwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDEyLjAwMDAwMDAwMi4wMDAwMDAwMDkuKnpmaW5hbCIsCn0sClsiY3JheS1saWJwYWxzIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wYWxzL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5LWxpYnBhbHMvMS4zLjQubHVhIiwKZnVsbE5hbWUgPSAiY3JheS1s"
declare -x _ModuleTable005_="aWJwYWxzLzEuMy40IiwKbG9hZE9yZGVyID0gMTIsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1saWJwYWxzIiwKd1YgPSAiXjAwMDAwMDAxLjAwMDAwMDAwMy4wMDAwMDAwMDQuKnpmaW5hbCIsCn0sClsiY3JheS1tcGljaCJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMC9jcmF5LW1waWNoLzguMS4yOC5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LW1waWNoLzguMS4yOCIsCmxvYWRPcmRlciA9IDksCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1tcGljaCIsCndWID0gIl4w"
declare -x _ModuleTable006_="MDAwMDAwOC4wMDAwMDAwMDEuMDAwMDAwMDI4Lip6ZmluYWwiLAp9LApbImNyYXktcGFscyJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGFscy9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wYWxzLzEuMy40Lmx1YSIsCmZ1bGxOYW1lID0gImNyYXktcGFscy8xLjMuNCIsCmxvYWRPcmRlciA9IDExLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcGFscyIsCndWID0gIl4wMDAwMDAwMS4wMDAwMDAwMDMuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImNyYXktcG1pIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wbWkvNi4xLjEzLmx1YSIsCmZ1bGxOYW1lID0gImNy"
declare -x _ModuleTable007_="YXktcG1pLzYuMS4xMyIsCmxvYWRPcmRlciA9IDEwLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcG1pIiwKd1YgPSAiXjAwMDAwMDA2LjAwMDAwMDAwMS4wMDAwMDAwMTMuKnpmaW5hbCIsCn0sCmNyYXlwZSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5cGUvMi43LjMwLmx1YSIsCmZ1bGxOYW1lID0gImNyYXlwZS8yLjcuMzAiLApsb2FkT3JkZXIgPSA3LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZSIsCndWID0gIl4wMDAwMDAwMi4wMDAwMDAwMDcuMDAwMDAwMDMwLip6ZmluYWwiLAp9LApb"
declare -x _ModuleTable008_="ImNyYXlwZS1uZXR3b3JrLW9maSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS1uZXR3b3JrLW9maS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5cGUtbmV0d29yay1vZmkiLApsb2FkT3JkZXIgPSAyLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS1uZXR3b3JrLW9maSIsCndWID0gIk0uKnpmaW5hbCIsCn0sClsiY3JheXBlLXg4Ni1taWxhbiJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS14ODYtbWlsYW4ubHVhIiwKZnVsbE5hbWUgPSAiY3JheXBl"
declare -x _ModuleTable009_="LXg4Ni1taWxhbiIsCmxvYWRPcmRlciA9IDEzLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS14ODYtbWlsYW4iLAp3ViA9ICJNLip6ZmluYWwiLAp9LApjdWRubiA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY3Vkbm4vOS4xLjAubHVhIiwKZnVsbE5hbWUgPSAiY3Vkbm4vOS4xLjAiLApsb2FkT3JkZXIgPSAxNiwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjdWRubi85LjEuMCIsCndWID0gIjAwMDAwMDAwOS4wMDAwMDAwMDEuKnpmaW5hbCIsCn0sCmRhcnNoYW4gPSB7CmZuID0gIi9zb2Z0L3BlcmZ0b29scy9k"
declare -x _ModuleTable010_="YXJzaGFuL2RhcnNoYW4tMy40LjQvc2hhcmUvY3JheXBlLTIueC9tb2R1bGVmaWxlcy9kYXJzaGFuLzMuNC40IiwKZnVsbE5hbWUgPSAiZGFyc2hhbi8zLjQuNCIsCmxvYWRPcmRlciA9IDQsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiZGFyc2hhbiIsCndWID0gIjAwMDAwMDAwMy4wMDAwMDAwMDQuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImdjYy1uYXRpdmUiXSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9nY2MtbmF0aXZlLzEyLjMubHVhIiwKZnVsbE5hbWUgPSAiZ2NjLW5hdGl2ZS8xMi4zIiwKbG9hZE9yZGVyID0gNiwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0"
declare -x _ModuleTable011_="YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJnY2MtbmF0aXZlIiwKd1YgPSAiXjAwMDAwMDEyLjAwMDAwMDAwMy4qemZpbmFsIiwKfSwKbGliZmFicmljID0gewpmbiA9ICIvb3B0L2NyYXkvbW9kdWxlZmlsZXMvbGliZmFicmljLzEuMTUuMi4wIiwKZnVsbE5hbWUgPSAibGliZmFicmljLzEuMTUuMi4wIiwKbG9hZE9yZGVyID0gMSwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJsaWJmYWJyaWMiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDE1LjAwMDAwMDAwMi4qemZpbmFsIiwKfSwKWyJwZXJmdG9vbHMtYmFzZSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb3JlL3BlcmZ0b29s"
declare -x _ModuleTable012_="cy1iYXNlLzIzLjEyLjAubHVhIiwKZnVsbE5hbWUgPSAicGVyZnRvb2xzLWJhc2UvMjMuMTIuMCIsCmxvYWRPcmRlciA9IDMsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAicGVyZnRvb2xzLWJhc2UiLAp3ViA9ICJeMDAwMDAwMjMuMDAwMDAwMDEyLip6ZmluYWwiLAp9LAp4YWx0ID0gewpmbiA9ICIvc29mdC94YWx0L21vZHVsZWZpbGVzL3hhbHQvMy4wLjItMjAyNDA4MjgyMDUwIiwKZnVsbE5hbWUgPSAieGFsdC8zLjAuMi0yMDI0MDgyODIwNTAiLApsb2FkT3JkZXIgPSA1LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gInhhbHQiLAp3ViA9ICJeMDAwMDAw"
declare -x _ModuleTable013_="MDMuMDAwMDAwMDAwLjAwMDAwMDAwMi4qemZpbmFsLS4yMDI0MDgyODIwNTAuKnpmaW5hbCIsCn0sCn0sCm1wYXRoQSA9IHsKCiIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9oZGY1LXBhcmFsbGVsL2dudS8xMi4wL29maS8xLjAvY3JheS1tcGljaC84LjAvY3JheS1oZGY1LXBhcmFsbGVsLzEuMTIuMiIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3B1L3g4Ni1taWxhbi8xLjAiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9vZmkvMS4wL2NyYXktbXBpY2gvOC4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMCIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxl"
declare -x _ModuleTable014_="ZmlsZXMvbWl4X2NvbXBpbGVycyIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29tcGlsZXIvZ251LzEyLjAiLCAiL3NvZnQvbW9kdWxlZmlsZXMiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL3BlcmZ0b29scy8yMy4xMi4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9uZXQvb2ZpLzEuMCIsICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0xpbnV4IgosICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmUiLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZSIKLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BhbHMvbG1vZC9tb2R1bGVmaWxlcy9jb3JlIgosICIvb3B0L2Ny"
declare -x _ModuleTable015_="YXkvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZSIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdCIKLCAiL3NvZnQvcGVyZnRvb2xzL2RhcnNoYW4vZGFyc2hhbi0zLjQuNC9zaGFyZS9jcmF5cGUtMi54L21vZHVsZWZpbGVzIiwgIi9zb2Z0L3hhbHQvbW9kdWxlZmlsZXMiLAp9LApzeXN0ZW1CYXNlTVBBVEggPSAiL3Vzci9zaGFyZS9tb2R1bGVmaWxlcy9MaW51eDovdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmU6L3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZTovdXNyL3NoYXJlL2xtb2QvbG1vZC9tb2R1bGVmaWxlczovb3B0L2NyYXkvcGFscy9sbW9kL21vZHVs"
declare -x _ModuleTable016_="ZWZpbGVzL2NvcmU6L29wdC9jcmF5L21vZHVsZWZpbGVzOi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmU6L29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdDovc29mdC9wZXJmdG9vbHMvZGFyc2hhbi9kYXJzaGFuLTMuNC40L3NoYXJlL2NyYXlwZS0yLngvbW9kdWxlZmlsZXM6L3NvZnQveGFsdC9tb2R1bGVmaWxlcyIsCn0K"
declare -x _ModuleTable_Sz_="16"
declare -x __LMOD_Priority_PATH="/soft/xalt/3.0.2-202408282050/bin:-100"
declare -x __LMOD_REF_COUNT_COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin:1"
declare -x __LMOD_REF_COUNT_CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:1;/opt/cray/pe/pmi/6.1.13/lib:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:1;/opt/cray/pe/mpich/8.1.28/gtl/lib:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib:1;/opt/cray/pe/perftools/23.12.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:1;/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:1;/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:1;/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:1;/soft/perftools/darshan/darshan-3.4.4/lib:1;/opt/cray/pe/papi/7.0.1.2/lib64:1;/opt/cray/libfabric/1.15.2.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so:1"
declare -x __LMOD_REF_COUNT_MANPATH="/opt/cray/pals/1.3.4/man:2;/opt/cray/pe/pmi/6.1.13/man:1;/opt/cray/pe/mpich/8.1.28/ofi/man:1;/opt/cray/pe/mpich/8.1.28/man/mpich:1;/opt/cray/pe/dsmml/0.2.2/dsmml/man:1;/opt/cray/pe/craype/2.7.30/man:1;/opt/cray/pe/perftools/23.12.0/man:1;/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:1;/opt/cray/libfabric/1.15.2.0/share/man:1;/usr/share/lmod/lmod/share/man:1;/home/shourya01/.local/man:1;/usr/local/man:1;/usr/share/man:1;/usr/man:1;/opt/c3/man:1;/opt/pbs/share/man:1;/opt/clmgr/man:1;/opt/sgi/share/man:1;/opt/clmgr/share/man:1;/opt/clmgr/lib/cm-cli/man:1"
declare -x __LMOD_REF_COUNT_MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:1;/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:1;/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:1;/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:1;/opt/cray/pe/lmod/modulefiles/mix_compilers:1;/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:1;/soft/modulefiles:1;/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:1;/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:1;/usr/share/modulefiles/Linux:1;/usr/share/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles:1;/opt/cray/pals/lmod/modulefiles/core:1;/opt/cray/modulefiles:1;/opt/cray/pe/lmod/modulefiles/core:1;/opt/cray/pe/lmod/modulefiles/craype-targets/default:1;/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:1;/soft/xalt/modulefiles:1"
declare -x __LMOD_REF_COUNT_PATH="/soft/xalt/3.0.2-202408282050/bin:1;/soft/compilers/cudatoolkit/cuda-12.4.1/bin:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:1;/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:1;/opt/cray/pe/hdf5/1.12.2.9/bin:1;/opt/cray/pals/1.3.4/bin:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:1;/opt/cray/pe/mpich/8.1.28/bin:1;/opt/cray/pe/craype/2.7.30/bin:1;/home/shourya01/.local/bin:4;/soft/perftools/darshan/darshan-3.4.4/bin:1;/opt/cray/pe/perftools/23.12.0/bin:1;/opt/cray/pe/papi/7.0.1.2/bin:1;/opt/cray/libfabric/1.15.2.0/bin:1;/opt/clmgr/sbin:1;/opt/clmgr/bin:1;/opt/sgi/sbin:1;/opt/sgi/bin:1;/usr/local/bin:1;/usr/bin:1;/bin:2;/opt/c3/bin:1;/usr/lib/mit/bin:1;/usr/lib/mit/sbin:1;/opt/pbs/bin:1;/sbin:1;/home/shourya01/bin:1;/opt/cray/pe/bin:1"
declare -x __LMOD_REF_COUNT_PE_DSMML_PKGCONFIG_LIBS="dsmml:1"
declare -x __LMOD_REF_COUNT_PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:1;hdf5_fortran_parallel:1;mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FIXED_PRGENV="GNU:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_GENCOMPILERS_GNU="12.3:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_PKGCONFIG_LIBS="mpich:1"
declare -x __LMOD_REF_COUNT_PE_PALS_PKGCONFIG_LIBS="libpals:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_LIBS="hdf5_hl_parallel:1;hdf5_parallel:1;mpich:1;dsmml:1;darshan-runtime:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_PRODUCTS="PE_PALS:1;PE_PMI:1;PE_MPICH:1;PE_DSMML:1"
declare -x __LMOD_REF_COUNT_PE_PMI_PKGCONFIG_LIBS="cray-pmi:1"
declare -x __LMOD_REF_COUNT_PE_PRODUCT_LIST="CRAYPE_X86_MILAN:1;PERFTOOLS:1;CRAYPAT:1"
declare -x __LMOD_REF_COUNT_PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:1;/opt/cray/pals/1.3.4/lib/pkgconfig:1;/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:1;/opt/cray/pe/craype/2.7.30/pkg-config:1;/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:1;/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages:1"
declare -x ftp_proxy="http://proxy.alcf.anl.gov:3128"
declare -x http_proxy="http://proxy.alcf.anl.gov:3128"
declare -x https_proxy="http://proxy.alcf.anl.gov:3128"
declare -x no_proxy="admin,polaris-adminvm-01,localhost,*.cm.polaris.alcf.anl.gov,polaris-*,*.polaris.alcf.anl.gov,*.alcf.anl.gov"
Running on 2 nodes
Total number of GPUs: 8
Connected to tcp://x3006c0s19b0n0.hsn.cm.polaris.alcf.anl.gov:7919
Found executable /soft/applications/conda/2024-04-29/mconda3/bin/python
Launching application 1e3392b0-3e73-4c1e-a23f-5149577a86e3
Using PMI port 50910,50911
[2025-06-25 00:34:11,384] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:34:11,384] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:34:11,384] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:34:11,384] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:34:11,640] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:34:11,641] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:34:11,641] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:34:11,641] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:34:15,622] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:34:15,622] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:34:15,623] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:34:15,623] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:34:15,623] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-25 00:34:15,623] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:34:15,623] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:34:15,623] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:34:16,213] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:34:16,213] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=8, master_addr=10.140.57.105, master_port=29500
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:34:16,213] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:34:16,213] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:34:16,213] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:34:16,213] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-25 00:34:16,213] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=2, world_size=8, master_addr=10.140.57.105, master_port=29500
[2025-06-25 00:34:16,213] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=8, master_addr=10.140.57.105, master_port=29500
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:34:16,213] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:34:16,213] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-25 00:34:16,213] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=3, world_size=8, master_addr=10.140.57.105, master_port=29500
[2025-06-25 00:34:16,213] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=8, master_addr=10.140.57.105, master_port=29500
[2025-06-25 00:34:16,213] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-25 00:34:16,213] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=0, world_size=8, master_addr=10.140.57.105, master_port=29500
[2025-06-25 00:34:16,213] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=8, master_addr=10.140.57.105, master_port=29500
[2025-06-25 00:34:16,213] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-06-25 00:34:16,213] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=1, world_size=8, master_addr=10.140.57.105, master_port=29500
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
Initialized deepspeed on global rank 0, local rank 0 with world size 8.
[2025-06-25 00:34:21,047] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2+5f631abc, git-hash=5f631abc, git-branch=HEAD
[2025-06-25 00:34:25,256] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-06-25 00:34:25,258] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-06-25 00:34:25,258] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-06-25 00:34:25,315] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-06-25 00:34:25,315] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-06-25 00:34:25,315] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-06-25 00:34:25,315] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-06-25 00:34:25,315] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0005], mom=[(0.9, 0.999)]
[2025-06-25 00:34:25,316] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2025-06-25 00:34:25,316] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-06-25 00:34:25,316] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-06-25 00:34:25,316] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2025-06-25 00:34:25,316] [INFO] [config.py:1000:print]   amp_params ................... False
[2025-06-25 00:34:25,316] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x14f950063e90>
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   dump_state ................... False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   global_rank .................. 0
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 0.0005, 'weight_decay': 0.01}
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   pld_params ................... False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   steps_per_print .............. 100000
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   train_batch_size ............. 1024
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2025-06-25 00:34:25,317] [INFO] [config.py:1000:print]   world_size ................... 8
[2025-06-25 00:34:25,318] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2025-06-25 00:34:25,318] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-06-25 00:34:25,318] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2025-06-25 00:34:25,318] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2025-06-25 00:34:25,318] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2025-06-25 00:34:25,318] [INFO] [config.py:986:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 128, 
    "train_batch_size": 1.024000e+03, 
    "steps_per_print": 1.000000e+05, 
    "gradient_accumulation_steps": 1, 
    "fp16": {
        "enabled": false
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0005, 
            "weight_decay": 0.01
        }
    }, 
    "comms_logger": {
        "enabled": true, 
        "verbose": false
    }, 
    "zero_optimization": {
        "stage": 0
    }
}
Validating lr=0.0005, train epoch 0.:   0%|          | 0/535 [00:00<?, ?it/s]Initialized deepspeed on global rank 1, local rank 1 with world size 8.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/shourya01/stormer_deepspeed/train_tfm_parallel.py", line 273, in <module>
[rank1]:     _ = train_one_epoch_tfm_parallel(
[rank1]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 169, in train_one_epoch_tfm_parallel
[rank1]:     forecast_out, weather_out = model_engine(x, weather_past)
[rank1]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank1]:     ret_val = func(*args, **kwargs)
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank1]:     loss = self.module(*inputs, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1199, in forward
[rank1]:     out_weather = self.timesfm2.decode(padded_inp_weather, padding_weather, self.lookahead)[0]
[rank1]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: TypeError: PatchedTimeSeriesDecoder.decode() missing 1 required positional argument: 'horizon_len'
Validating lr=0.0005, train epoch 0.:   0%|          | 0/535 [00:00<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/shourya01/stormer_deepspeed/train_tfm_parallel.py", line 273, in <module>
[rank0]:     _ = train_one_epoch_tfm_parallel(
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 169, in train_one_epoch_tfm_parallel
[rank0]:     forecast_out, weather_out = model_engine(x, weather_past)
[rank0]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1199, in forward
[rank0]:     out_weather = self.timesfm2.decode(padded_inp_weather, padding_weather, self.lookahead)[0]
[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: TypeError: PatchedTimeSeriesDecoder.decode() missing 1 required positional argument: 'horizon_len'
Initialized deepspeed on global rank 3, local rank 3 with world size 8.
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/shourya01/stormer_deepspeed/train_tfm_parallel.py", line 273, in <module>
[rank3]:     _ = train_one_epoch_tfm_parallel(
[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 169, in train_one_epoch_tfm_parallel
[rank3]:     forecast_out, weather_out = model_engine(x, weather_past)
[rank3]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank3]:     ret_val = func(*args, **kwargs)
[rank3]:               ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank3]:     loss = self.module(*inputs, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1199, in forward
[rank3]:     out_weather = self.timesfm2.decode(padded_inp_weather, padding_weather, self.lookahead)[0]
[rank3]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]: TypeError: PatchedTimeSeriesDecoder.decode() missing 1 required positional argument: 'horizon_len'
Initialized deepspeed on global rank 2, local rank 2 with world size 8.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/shourya01/stormer_deepspeed/train_tfm_parallel.py", line 273, in <module>
[rank2]:     _ = train_one_epoch_tfm_parallel(
[rank2]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 169, in train_one_epoch_tfm_parallel
[rank2]:     forecast_out, weather_out = model_engine(x, weather_past)
[rank2]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank2]:     ret_val = func(*args, **kwargs)
[rank2]:               ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank2]:     loss = self.module(*inputs, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1199, in forward
[rank2]:     out_weather = self.timesfm2.decode(padded_inp_weather, padding_weather, self.lookahead)[0]
[rank2]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]: TypeError: PatchedTimeSeriesDecoder.decode() missing 1 required positional argument: 'horizon_len'
Initialized deepspeed on global rank 7, local rank 3 with world size 8.
Initialized deepspeed on global rank 4, local rank 0 with world size 8.
Initialized deepspeed on global rank 5, local rank 1 with world size 8.
Initialized deepspeed on global rank 6, local rank 2 with world size 8.
[rank4]: Traceback (most recent call last):
[rank4]:   File "/home/shourya01/stormer_deepspeed/train_tfm_parallel.py", line 273, in <module>
[rank4]:     _ = train_one_epoch_tfm_parallel(
[rank4]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 169, in train_one_epoch_tfm_parallel
[rank4]:     forecast_out, weather_out = model_engine(x, weather_past)
[rank4]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank4]:     ret_val = func(*args, **kwargs)
[rank4]:               ^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank4]:     loss = self.module(*inputs, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1199, in forward
[ra[rank5]: Traceback (most recent call last):
[rank5]:   File "/home/shourya01/stormer_deepspeed/train_tfm_parallel.py", line 273, in <module>
[rank5]:     _ = train_one_epoch_tfm_parallel(
[rank5]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 169, in train_one_epoch_tfm_parallel
[rank5]:     forecast_out, weather_out = model_engine(x, weather_past)
[rank5]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank5]:     ret_val = func(*args, **kwargs)
[rank5]:               ^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank5]:     loss = self.module(*inputs, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1199, in forward
[ra[rank7]: Traceback (most recent call last):
[rank7]:   File "/home/shourya01/stormer_deepspeed/train_tfm_parallel.py", line 273, in <module>
[rank7]:     _ = train_one_epoch_tfm_parallel(
[rank7]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 169, in train_one_epoch_tfm_parallel
[rank7]:     forecast_out, weather_out = model_engine(x, weather_past)
[rank7]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank7]:     ret_val = func(*args, **kwargs)
[rank7]:               ^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank7]:     loss = self.module(*inputs, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1199, in forward
[rank4]:     out_weather = self.timesfm2.decode(padded_inp_weather, padding_weather, self.lookahead)[0]
[rank4]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]: TypeError: PatchedTimeSeriesDecoder.decode() missing 1 required positional argument: 'horizon_len'
nk5]:     out_weather = self.timesfm2.decode(padded_inp_weather, padding_weather, self.lookahead)[0]
[rank5]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]: TypeError: PatchedTimeSeriesDecoder.decode() missing 1 required positional argument: 'horizon_len'
[rank6]: Traceback (most recent call last):
[rank6]:   File "/home/shourya01/stormer_deepspeed/train_tfm_parallel.py", line 273, in <module>
[rank6]:     _ = train_one_epoch_tfm_parallel(
[rank6]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/home/shourya01/stormer_deepspeed/utils.py", line 169, in train_one_epoch_tfm_parallel
[rank6]:     forecast_out, weather_out = model_engine(x, weather_past)
[rank6]:                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
[rank6]:     ret_val = func(*args, **kwargs)
[rank6]:               ^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 1855, in forward
[rank6]:     loss = self.module(*inputs, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/soft/applications/conda/2024-04-29/mconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/home/shourya01/stormer_deepspeed/TimesFM.py", line 1199, in forward
[rank7]:     out_weather = self.timesfm2.decode(padded_inp_weather, padding_weather, self.lookahead)[0]
[rank7]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]: TypeError: PatchedTimeSeriesDecoder.decode() missing 1 required positional argument: 'horizon_len'
nk6]:     out_weather = self.timesfm2.decode(padded_inp_weather, padding_weather, self.lookahead)[0]
[rank6]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]: TypeError: PatchedTimeSeriesDecoder.decode() missing 1 required positional argument: 'horizon_len'
x3006c0s19b0n0.hsn.cm.polaris.alcf.anl.gov: rank 0 exited with code 1
x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov: rank 4 exited with code 1
x3006c0s19b0n0.hsn.cm.polaris.alcf.anl.gov: rank 2 exited with code 1
x3006c0s19b0n0.hsn.cm.polaris.alcf.anl.gov: rank 3 exited with code 1
x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov: rank 7 exited with code 1
x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov: rank 6 exited with code 1
x3006c0s19b0n0.hsn.cm.polaris.alcf.anl.gov: rank 1 exited with code 1
x3006c0s19b1n0.hsn.cm.polaris.alcf.anl.gov: rank 5 exited with code 1
Application 1e3392b0 resources: utime=80s stime=51s maxrss=4947532KB inblock=122560 oublock=144 minflt=3761637 majflt=6693 nvcsw=468273 nivcsw=132145
Training completed
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 163: bind: warning: line editing not enabled
/home/shourya01/.bashrc: line 164: bind: warning: line editing not enabled

Lmod is automatically replacing "nvhpc/23.9" with "gcc-native/12.3".


Lmod is automatically replacing "PrgEnv-nvhpc/8.5.0" with "PrgEnv-gnu/8.5.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.28

declare -x APP2_STATE="23.12.0"
declare -x BASH_ENV="/usr/share/lmod/lmod/init/bash"
declare -x C3_RSH="ssh -oConnectTimeout=10 -oForwardX11=no"
declare -x CFLAGS="-I/soft/applications/conda/2024-04-29/mconda3/include"
declare -x COLORTERM="1"
declare -x COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin"
declare -x CONDA_DEFAULT_ENV="base"
declare -x CONDA_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/conda"
declare -x CONDA_PREFIX="/soft/applications/conda/2024-04-29/mconda3"
declare -x CONDA_PROMPT_MODIFIER="(2024-04-29/base) "
declare -x CONDA_PYTHON_EXE="/soft/applications/conda/2024-04-29/mconda3/bin/python"
declare -x CONDA_SHLVL="1"
declare -x CPU="x86_64"
declare -x CRAYPAT_LD_LIBRARY_PATH="/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAYPAT_OPTS_EXECUTABLE="libexec64/opts"
declare -x CRAYPAT_ROOT="/opt/cray/pe/perftools/23.12.0"
declare -x CRAYPE_DIR="/opt/cray/pe/craype/2.7.30"
declare -x CRAYPE_NETWORK_TARGET="ofi"
declare -x CRAYPE_VERSION="2.7.30"
declare -x CRAY_CPU_TARGET="x86-milan"
declare -x CRAY_DSMML_BASEDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_DIR="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_PREFIX="/opt/cray/pe/dsmml/0.2.2/dsmml"
declare -x CRAY_DSMML_ROOTDIR="/opt/cray/pe/dsmml/0.2.2"
declare -x CRAY_DSMML_VER="0.2.2"
declare -x CRAY_DSMML_VERSION="0.2.2"
declare -x CRAY_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x CRAY_HDF5_PARALLEL_PREFIX="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x CRAY_HDF5_PARALLEL_VERSION="1.12.2.9"
declare -x CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:/opt/cray/pe/pmi/6.1.13/lib:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:/opt/cray/pe/mpich/8.1.28/gtl/lib:/opt/cray/pe/dsmml/0.2.2/dsmml/lib:/opt/cray/pe/perftools/23.12.0/lib64"
declare -x CRAY_LMOD_COMPILER="gnu/12.0"
declare -x CRAY_LMOD_CPU="x86-milan/1.0"
declare -x CRAY_LMOD_MPI="cray-mpich/8.0"
declare -x CRAY_LMOD_NET="ofi/1.0"
declare -x CRAY_MPICH_BASEDIR="/opt/cray/pe/mpich/8.1.28/ofi"
declare -x CRAY_MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_PREFIX="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x CRAY_MPICH_ROOTDIR="/opt/cray/pe/mpich/8.1.28"
declare -x CRAY_MPICH_VER="8.1.28"
declare -x CRAY_MPICH_VERSION="8.1.28"
declare -x CRAY_PERFTOOLS_PREFIX="/opt/cray/pe/perftools/23.12.0"
declare -x CRAY_PERFTOOLS_VERSION="23.12.0"
declare -x CRAY_PMI_INCLUDE_OPTS="-I/opt/cray/pe/pmi/6.1.13/include"
declare -x CRAY_PMI_POST_LINK_OPTS="-L/opt/cray/pe/pmi/6.1.13/lib"
declare -x CRAY_PMI_PREFIX="/opt/cray/pe/pmi/6.1.13"
declare -x CRAY_PMI_VERSION="6.1.13"
declare -x CSHEDIT="emacs"
declare -x CUDA_HOME="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDA_TOOLKIT_BASE="/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x CUDNN_HOME="/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/"
declare -x ENVIRONMENT="BATCH"
declare -x ENV_NAME="conda/2024-04-29"
declare -x FROM_HEADER=""
declare -x GCC_PATH="/usr/bin"
declare -x GCC_PREFIX="/usr/lib64/gcc/x86_64-suse-linux/12"
declare -x GCC_VERSION="12.3"
declare -x GNU_VERSION="12.3"
declare -x GPG_TTY="not a tty"
declare -x GSETTINGS_SCHEMA_DIR="/soft/applications/conda/2024-04-29/mconda3/share/glib-2.0/schemas"
declare -x GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=""
declare -x G_BROKEN_FILENAMES="1"
declare -x G_FILENAME_ENCODING="@locale,UTF-8,ISO-8859-15,CP1252"
declare -x HDF5_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HDF5_ROOT="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3"
declare -x HISTSIZE="1000"
declare -x HOME="/home/shourya01"
declare -x HOST="x3003c0s13b1n0"
declare -x HOSTNAME="x3003c0s13b1n0"
declare -x HOSTTYPE="x86_64"
declare -x HTTPS_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x HTTP_PROXY="http://proxy.alcf.anl.gov:3128"
declare -x LANG="en_US.UTF-8"
declare -x LANGUAGE="en_US.UTF-8"
declare -x LDFLAGS="-L/soft/applications/conda/2024-04-29/mconda3/lib -Wl,--enable-new-dtags,-rpath,/soft/applications/conda/2024-04-29/mconda3/lib"
declare -x LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:/soft/perftools/darshan/darshan-3.4.4/lib:/opt/cray/pe/papi/7.0.1.2/lib64:/opt/cray/libfabric/1.15.2.0/lib64"
declare -x LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so"
declare -x LESS="-M -I -R"
declare -x LESSCLOSE="lessclose.sh %s %s"
declare -x LESSKEY="/etc/lesskey.bin"
declare -x LESSOPEN="lessopen.sh %s"
declare -x LESS_ADVANCED_PREPROCESSOR="no"
declare -x LMOD_CMD="/usr/share/lmod/lmod/libexec/lmod"
declare -x LMOD_DIR="/usr/share/lmod/lmod/libexec"
declare -x LMOD_FAMILY_COMPILER="gcc-native"
declare -x LMOD_FAMILY_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_CRAYPE="craype"
declare -x LMOD_FAMILY_CRAYPE_CPU="craype-x86-milan"
declare -x LMOD_FAMILY_CRAYPE_CPU_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_NETWORK="craype-network-ofi"
declare -x LMOD_FAMILY_CRAYPE_NETWORK_VERSION="false"
declare -x LMOD_FAMILY_CRAYPE_VERSION="2.7.30"
declare -x LMOD_FAMILY_GCC_COMPILER="gcc-native"
declare -x LMOD_FAMILY_GCC_COMPILER_VERSION="12.3"
declare -x LMOD_FAMILY_HDF5="cray-hdf5-parallel"
declare -x LMOD_FAMILY_HDF5_VERSION="1.12.2.9"
declare -x LMOD_FAMILY_MPI="cray-mpich"
declare -x LMOD_FAMILY_MPI_VERSION="8.1.28"
declare -x LMOD_FAMILY_PRGENV="PrgEnv-gnu"
declare -x LMOD_FAMILY_PRGENV_VERSION="8.5.0"
declare -x LMOD_FAMILY_PYTHON="conda"
declare -x LMOD_FAMILY_PYTHON_VERSION="2024-04-29"
declare -x LMOD_PKG="/usr/share/lmod/lmod"
declare -x LMOD_ROOT="/usr/share/lmod"
declare -x LMOD_SETTARG_FULL_SUPPORT="no"
declare -x LMOD_SYSTEM_DEFAULT_MODULES="PrgEnv-nvhpc:craype-network-ofi:perftools-base:darshan:xalt"
declare -x LMOD_VERSION="8.7.34"
declare -x LMOD_sys="Linux"
declare -x LOADEDMODULES="libfabric/1.15.2.0:craype-network-ofi:perftools-base/23.12.0:darshan/3.4.4:xalt/3.0.2-202408282050:gcc-native/12.3:craype/2.7.30:cray-dsmml/0.2.2:cray-mpich/8.1.28:cray-pmi/6.1.13:cray-pals/1.3.4:cray-libpals/1.3.4:craype-x86-milan:PrgEnv-gnu/8.5.0:cray-hdf5-parallel/1.12.2.9:cudnn/9.1.0:conda/2024-04-29"
declare -x LOGNAME="shourya01"
declare -x MACHTYPE="x86_64-suse-linux"
declare -x MAIL="/var/spool/mail/shourya01"
declare -x MANPATH="/opt/cray/pals/1.3.4/man:/opt/cray/pe/pmi/6.1.13/man:/opt/cray/pe/mpich/8.1.28/ofi/man:/opt/cray/pe/mpich/8.1.28/man/mpich:/opt/cray/pe/dsmml/0.2.2/dsmml/man:/opt/cray/pe/craype/2.7.30/man:/opt/cray/pe/perftools/23.12.0/man:/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:/opt/cray/libfabric/1.15.2.0/share/man:/usr/share/lmod/lmod/share/man:/home/shourya01/.local/man:/usr/local/man:/usr/share/man:/usr/man:/opt/c3/man:/opt/pbs/share/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man"
declare -x MINICOM="-c on"
declare -x MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:/opt/cray/pe/lmod/modulefiles/mix_compilers:/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:/soft/modulefiles:/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:/usr/share/modulefiles/Linux:/usr/share/modulefiles/Core:/usr/share/lmod/lmod/modulefiles/Core:/usr/share/lmod/lmod/modulefiles:/opt/cray/pals/lmod/modulefiles/core:/opt/cray/modulefiles:/opt/cray/pe/lmod/modulefiles/core:/opt/cray/pe/lmod/modulefiles/craype-targets/default:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:/soft/xalt/modulefiles"
declare -x MODULEPATH_ROOT="/usr/share/modulefiles"
declare -x MODULESHOME="/usr/share/lmod/lmod"
declare -x MORE="-sl"
declare -x MPI4JAX_USE_CUDA_MPI="1"
declare -x MPICH_DIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3"
declare -x MPICH_GPU_SUPPORT_ENABLED="1"
declare -x NCCL_IB_DISABLE="1"
declare -x NCCL_SOCKET_IFNAME="hsn"
declare -x NCPUS="64"
declare -x OFFLOAD_INIT="on_start"
declare -x OLDPWD
declare -x OMP_NUM_THREADS="4"
declare -x OSCAR_HOME="/opt/oscar"
declare -x OSTYPE="linux"
declare -x PAGER="less"
declare -x PALS_TRANSFER="0"
declare -x PATH="/soft/applications/conda/2024-04-29/mconda3/bin:/soft/applications/conda/2024-04-29/mconda3/condabin:/soft/xalt/3.0.2-202408282050/bin:/soft/compilers/cudatoolkit/cuda-12.4.1/bin:/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:/opt/cray/pe/hdf5/1.12.2.9/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/home/shourya01/.local/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/home/shourya01/bin:/opt/cray/pe/bin"
declare -x PAT_RT_PERFCTR_DISABLE_COMPONENTS="nvml,rocm_smi"
declare -x PBS_ACCOUNT="ParaLLMs"
declare -x PBS_ENVIRONMENT="PBS_BATCH"
declare -x PBS_JOBCOOKIE="7D4EEFD950773FA355C0F26C0C6FFD4C"
declare -x PBS_JOBDIR="/home/shourya01"
declare -x PBS_JOBID="5238402.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_JOBNAME="tfm-parallel-trial"
declare -x PBS_MOMPORT="15003"
declare -x PBS_NODEFILE="/var/spool/pbs/aux/5238402.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_NODENUM="0"
declare -x PBS_O_HOME="/home/shourya01"
declare -x PBS_O_HOST="polaris-login-04.hsn.cm.polaris.alcf.anl.gov"
declare -x PBS_O_INTERACTIVE_AUTH_METHOD="resvport"
declare -x PBS_O_LANG="en_US.UTF-8"
declare -x PBS_O_LOGNAME="shourya01"
declare -x PBS_O_MAIL="/var/spool/mail/shourya01"
declare -x PBS_O_PATH="/home/shourya01/.local/bin:/home/shourya01/.vscode-server/cli/servers/Stable-dfaf44141ea9deb3b4096f7cd6d24e00c147a4b1/server/bin/remote-cli:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/home/shourya01/.local/bin:/soft/xalt/3.0.2-202408282050/bin:/soft/perftools/darshan/darshan-3.4.4/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/cray/pals/1.3.4/bin:/opt/cray/pe/mpich/8.1.28/ofi/nvidia/23.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/extras/qd/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/compilers/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/shourya01/.local/bin:/usr/local/bin:/usr/bin:/bin:/opt/c3/bin:/dbhome/db2cat/sqllib/bin:/dbhome/db2cat/sqllib/adm:/dbhome/db2cat/sqllib/misc:/dbhome/db2cat/sqllib/gskit/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/opt/cray/pe/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.local/bin:/home/shourya01/bin:/home/shourya01/.vscode-server/extensions/ms-python.debugpy-2025.8.0/bundled/scripts/noConfigScripts"
declare -x PBS_O_QUEUE="debug"
declare -x PBS_O_SHELL="/bin/bash"
declare -x PBS_O_SYSTEM="Linux"
declare -x PBS_O_WORKDIR="/home/shourya01"
declare -x PBS_QUEUE="debug"
declare -x PBS_TASKNUM="1"
declare -x PELOCAL_PRGENV="true"
declare -x PERFTOOLS_VERSION="23.12.0"
declare -x PE_DSMML_MODULE_NAME="cray-dsmml"
declare -x PE_DSMML_PKGCONFIG_LIBS="dsmml"
declare -x PE_ENV="GNU"
declare -x PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel:mpichf90"
declare -x PE_GCC_EXTERNAL="native"
declare -x PE_GCC_LEVEL="12"
declare -x PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig"
declare -x PE_HDF5_PARALLEL_DIR="/opt/cray/pe/hdf5-parallel/1.12.2.9"
declare -x PE_HDF5_PARALLEL_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:hdf5_fortran_parallel"
declare -x PE_HDF5_PARALLEL_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel"
declare -x PE_MPICH_FIXED_PRGENV="GNU"
declare -x PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90"
declare -x PE_MPICH_GENCOMPILERS_GNU="12.3"
declare -x PE_MPICH_GTL_DIR_amd_gfx906="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx908="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx90a="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx940="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_amd_gfx942="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia70="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia80="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_nvidia90="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_DIR_ponteVecchio="-L/opt/cray/pe/mpich/8.1.28/gtl/lib"
declare -x PE_MPICH_GTL_LIBS_amd_gfx906="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx908="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx90a="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx940="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_amd_gfx942="-lmpi_gtl_hsa"
declare -x PE_MPICH_GTL_LIBS_nvidia70="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia80="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_nvidia90="-lmpi_gtl_cuda"
declare -x PE_MPICH_GTL_LIBS_ponteVecchio="-lmpi_gtl_ze"
declare -x PE_MPICH_MODULE_NAME="cray-mpich"
declare -x PE_MPICH_PKGCONFIG_LIBS="mpich"
declare -x PE_MPICH_PKGCONFIG_VARIABLES="PE_MPICH_GTL_DIR_@accelerator@:PE_MPICH_GTL_LIBS_@accelerator@"
declare -x PE_PALS_PKGCONFIG_LIBS="libpals"
declare -x PE_PERFTOOLS_MPICH_LIBDIR="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib"
declare -x PE_PKGCONFIG_LIBS="hdf5_hl_parallel:hdf5_parallel:mpich:dsmml:darshan-runtime"
declare -x PE_PKGCONFIG_PRODUCTS="PE_PALS:PE_PMI:PE_MPICH:PE_DSMML"
declare -x PE_PMI_PKGCONFIG_LIBS="cray-pmi"
declare -x PE_PRODUCT_LIST="CRAYPE_X86_MILAN"
declare -x PKGCONFIG_ENABLED="1"
declare -x PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:/opt/cray/pals/1.3.4/lib/pkgconfig:/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:/opt/cray/pe/craype/2.7.30/pkg-config:/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig"
declare -x PROFILEREAD="true"
declare -x PWD="/home/shourya01"
declare -x PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages"
declare -x PYTHONUSERBASE="/home/shourya01/.local/polaris/conda/2024-04-29"
declare -x QT_SYSTEM_DIR="/usr/share/desktop-data"
declare -x SHELL="/bin/bash"
declare -x SHLVL="2"
declare -x SLURM_MPI_TYPE="cray_shasta"
declare -x STARSHIP_SESSION_KEY="3166913064103871"
declare -x STARSHIP_SHELL="bash"
declare -x TMPDIR="/var/tmp/pbs.5238402.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov"
declare -x TRITON_DISABLE_AUTOTUNE="1"
declare -x TZ="Etc/UTC"
declare -x USER="shourya01"
declare -x USE_PCM_DB="2"
declare -x WINDOWMANAGER="xterm"
declare -x XALT_DIR="/soft/xalt/3.0.2-202408282050"
declare -x XALT_EXECUTABLE_TRACKING="yes"
declare -x XALT_SAMPLING="no"
declare -x XALT_SCALAR_AND_SPSR_SAMPLING="yes"
declare -x XCURSOR_THEME="DMZ"
declare -x XDG_CONFIG_DIRS="/etc/xdg"
declare -x XDG_DATA_DIRS="/usr/share"
declare -x XKEYSYMDB="/usr/X11R6/lib/X11/XKeysymDB"
declare -x XLA_FLAGS="--xla_gpu_force_compilation_parallelism=1 --xla_gpu_cuda_data_dir=/soft/compilers/cudatoolkit/cuda-12.4.1/"
declare -x XLA_PYTHON_CLIENT_PREALLOCATE="false"
declare -x XML_CATALOG_FILES="file:///soft/applications/conda/2024-04-29/mconda3/etc/xml/catalog file:///etc/xml/catalog"
declare -x XNLSPATH="/usr/X11R6/lib/X11/nls"
declare -x _CE_CONDA=""
declare -x _CE_M=""
declare -x _LMFILES_="/opt/cray/modulefiles/libfabric/1.15.2.0:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-network-ofi.lua:/opt/cray/pe/lmod/modulefiles/core/perftools-base/23.12.0.lua:/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles/darshan/3.4.4:/soft/xalt/modulefiles/xalt/3.0.2-202408282050:/opt/cray/pe/lmod/modulefiles/core/gcc-native/12.3.lua:/opt/cray/pe/lmod/modulefiles/core/craype/2.7.30.lua:/opt/cray/pe/lmod/modulefiles/core/cray-dsmml/0.2.2.lua:/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0/cray-mpich/8.1.28.lua:/opt/cray/pe/lmod/modulefiles/core/cray-pmi/6.1.13.lua:/opt/cray/pals/lmod/modulefiles/core/cray-pals/1.3.4.lua:/opt/cray/pals/lmod/modulefiles/core/cray-libpals/1.3.4.lua:/opt/cray/pe/lmod/modulefiles/craype-targets/default/craype-x86-milan.lua:/opt/cray/pe/lmod/modulefiles/core/PrgEnv-gnu/8.5.0.lua:/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2.9.lua:/soft/modulefiles/cudnn/9.1.0.lua:/soft/modulefiles/conda/2024-04-29.lua"
declare -x _ModuleTable001_="X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpQcmdFbnYgPSAiUHJnRW52LWdudSIsCmNvbXBpbGVyID0gImdjYy1uYXRpdmUiLApjcmF5cGUgPSAiY3JheXBlIiwKY3JheXBlX2NwdSA9ICJjcmF5cGUteDg2LW1pbGFuIiwKY3JheXBlX25ldHdvcmsgPSAiY3JheXBlLW5ldHdvcmstb2ZpIiwKZ2NjX2NvbXBpbGVyID0gImdjYy1uYXRpdmUiLApoZGY1ID0gImNyYXktaGRmNS1wYXJhbGxlbCIsCm1waSA9ICJjcmF5LW1waWNoIiwKcHl0aG9uID0gImNvbmRhIiwKfSwKbVQgPSB7ClsiUHJnRW52LWdudSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUv"
declare -x _ModuleTable002_="bG1vZC9tb2R1bGVmaWxlcy9jb3JlL1ByZ0Vudi1nbnUvOC41LjAubHVhIiwKZnVsbE5hbWUgPSAiUHJnRW52LWdudS84LjUuMCIsCmxvYWRPcmRlciA9IDE0LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gIlByZ0Vudi1nbnUiLAp3ViA9ICJeMDAwMDAwMDguMDAwMDAwMDA1Lip6ZmluYWwiLAp9LApjb25kYSA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY29uZGEvMjAyNC0wNC0yOS5sdWEiLApmdWxsTmFtZSA9ICJjb25kYS8yMDI0LTA0LTI5IiwKbG9hZE9yZGVyID0gMTcsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY29uZGEiLAp3ViA9ICJeMDAw"
declare -x _ModuleTable003_="MDIwMjQuKnpmaW5hbC0uMDAwMDAwMDA0Lip6ZmluYWwtLjAwMDAwMDAyOS4qemZpbmFsIiwKfSwKWyJjcmF5LWRzbW1sIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1kc21tbC8wLjIuMi5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWRzbW1sLzAuMi4yIiwKbG9hZE9yZGVyID0gOCwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWRzbW1sIiwKd1YgPSAiXjAwMDAwMDAwLjAwMDAwMDAwMi4wMDAwMDAwMDIuKnpmaW5hbCIsCn0sClsiY3JheS1oZGY1LXBhcmFsbGVsIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9v"
declare -x _ModuleTable004_="ZmkvMS4wL2NyYXktbXBpY2gvOC4wL2NyYXktaGRmNS1wYXJhbGxlbC8xLjEyLjIuOS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLApsb2FkT3JkZXIgPSAxNSwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjcmF5LWhkZjUtcGFyYWxsZWwvMS4xMi4yLjkiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDEyLjAwMDAwMDAwMi4wMDAwMDAwMDkuKnpmaW5hbCIsCn0sClsiY3JheS1saWJwYWxzIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wYWxzL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5LWxpYnBhbHMvMS4zLjQubHVhIiwKZnVsbE5hbWUgPSAiY3JheS1s"
declare -x _ModuleTable005_="aWJwYWxzLzEuMy40IiwKbG9hZE9yZGVyID0gMTIsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1saWJwYWxzIiwKd1YgPSAiXjAwMDAwMDAxLjAwMDAwMDAwMy4wMDAwMDAwMDQuKnpmaW5hbCIsCn0sClsiY3JheS1tcGljaCJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMC9jcmF5LW1waWNoLzguMS4yOC5sdWEiLApmdWxsTmFtZSA9ICJjcmF5LW1waWNoLzguMS4yOCIsCmxvYWRPcmRlciA9IDksCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAyLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY3JheS1tcGljaCIsCndWID0gIl4w"
declare -x _ModuleTable006_="MDAwMDAwOC4wMDAwMDAwMDEuMDAwMDAwMDI4Lip6ZmluYWwiLAp9LApbImNyYXktcGFscyJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGFscy9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wYWxzLzEuMy40Lmx1YSIsCmZ1bGxOYW1lID0gImNyYXktcGFscy8xLjMuNCIsCmxvYWRPcmRlciA9IDExLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcGFscyIsCndWID0gIl4wMDAwMDAwMS4wMDAwMDAwMDMuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImNyYXktcG1pIl0gPSB7CmZuID0gIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmUvY3JheS1wbWkvNi4xLjEzLmx1YSIsCmZ1bGxOYW1lID0gImNy"
declare -x _ModuleTable007_="YXktcG1pLzYuMS4xMyIsCmxvYWRPcmRlciA9IDEwLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXktcG1pIiwKd1YgPSAiXjAwMDAwMDA2LjAwMDAwMDAwMS4wMDAwMDAwMTMuKnpmaW5hbCIsCn0sCmNyYXlwZSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9jcmF5cGUvMi43LjMwLmx1YSIsCmZ1bGxOYW1lID0gImNyYXlwZS8yLjcuMzAiLApsb2FkT3JkZXIgPSA3LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZSIsCndWID0gIl4wMDAwMDAwMi4wMDAwMDAwMDcuMDAwMDAwMDMwLip6ZmluYWwiLAp9LApb"
declare -x _ModuleTable008_="ImNyYXlwZS1uZXR3b3JrLW9maSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS1uZXR3b3JrLW9maS5sdWEiLApmdWxsTmFtZSA9ICJjcmF5cGUtbmV0d29yay1vZmkiLApsb2FkT3JkZXIgPSAyLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS1uZXR3b3JrLW9maSIsCndWID0gIk0uKnpmaW5hbCIsCn0sClsiY3JheXBlLXg4Ni1taWxhbiJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jcmF5cGUtdGFyZ2V0cy9kZWZhdWx0L2NyYXlwZS14ODYtbWlsYW4ubHVhIiwKZnVsbE5hbWUgPSAiY3JheXBl"
declare -x _ModuleTable009_="LXg4Ni1taWxhbiIsCmxvYWRPcmRlciA9IDEzLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMiwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNyYXlwZS14ODYtbWlsYW4iLAp3ViA9ICJNLip6ZmluYWwiLAp9LApjdWRubiA9IHsKZm4gPSAiL3NvZnQvbW9kdWxlZmlsZXMvY3Vkbm4vOS4xLjAubHVhIiwKZnVsbE5hbWUgPSAiY3Vkbm4vOS4xLjAiLApsb2FkT3JkZXIgPSAxNiwKcHJvcFQgPSB7fSwKcmVmX2NvdW50ID0gMSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjdWRubi85LjEuMCIsCndWID0gIjAwMDAwMDAwOS4wMDAwMDAwMDEuKnpmaW5hbCIsCn0sCmRhcnNoYW4gPSB7CmZuID0gIi9zb2Z0L3BlcmZ0b29scy9k"
declare -x _ModuleTable010_="YXJzaGFuL2RhcnNoYW4tMy40LjQvc2hhcmUvY3JheXBlLTIueC9tb2R1bGVmaWxlcy9kYXJzaGFuLzMuNC40IiwKZnVsbE5hbWUgPSAiZGFyc2hhbi8zLjQuNCIsCmxvYWRPcmRlciA9IDQsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiZGFyc2hhbiIsCndWID0gIjAwMDAwMDAwMy4wMDAwMDAwMDQuMDAwMDAwMDA0Lip6ZmluYWwiLAp9LApbImdjYy1uYXRpdmUiXSA9IHsKZm4gPSAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZS9nY2MtbmF0aXZlLzEyLjMubHVhIiwKZnVsbE5hbWUgPSAiZ2NjLW5hdGl2ZS8xMi4zIiwKbG9hZE9yZGVyID0gNiwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDIsCnN0"
declare -x _ModuleTable011_="YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJnY2MtbmF0aXZlIiwKd1YgPSAiXjAwMDAwMDEyLjAwMDAwMDAwMy4qemZpbmFsIiwKfSwKbGliZmFicmljID0gewpmbiA9ICIvb3B0L2NyYXkvbW9kdWxlZmlsZXMvbGliZmFicmljLzEuMTUuMi4wIiwKZnVsbE5hbWUgPSAibGliZmFicmljLzEuMTUuMi4wIiwKbG9hZE9yZGVyID0gMSwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDEsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJsaWJmYWJyaWMiLAp3ViA9ICJeMDAwMDAwMDEuMDAwMDAwMDE1LjAwMDAwMDAwMi4qemZpbmFsIiwKfSwKWyJwZXJmdG9vbHMtYmFzZSJdID0gewpmbiA9ICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb3JlL3BlcmZ0b29s"
declare -x _ModuleTable012_="cy1iYXNlLzIzLjEyLjAubHVhIiwKZnVsbE5hbWUgPSAicGVyZnRvb2xzLWJhc2UvMjMuMTIuMCIsCmxvYWRPcmRlciA9IDMsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAicGVyZnRvb2xzLWJhc2UiLAp3ViA9ICJeMDAwMDAwMjMuMDAwMDAwMDEyLip6ZmluYWwiLAp9LAp4YWx0ID0gewpmbiA9ICIvc29mdC94YWx0L21vZHVsZWZpbGVzL3hhbHQvMy4wLjItMjAyNDA4MjgyMDUwIiwKZnVsbE5hbWUgPSAieGFsdC8zLjAuMi0yMDI0MDgyODIwNTAiLApsb2FkT3JkZXIgPSA1LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gInhhbHQiLAp3ViA9ICJeMDAwMDAw"
declare -x _ModuleTable013_="MDMuMDAwMDAwMDAwLjAwMDAwMDAwMi4qemZpbmFsLS4yMDI0MDgyODIwNTAuKnpmaW5hbCIsCn0sCn0sCm1wYXRoQSA9IHsKCiIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9oZGY1LXBhcmFsbGVsL2dudS8xMi4wL29maS8xLjAvY3JheS1tcGljaC84LjAvY3JheS1oZGY1LXBhcmFsbGVsLzEuMTIuMiIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3B1L3g4Ni1taWxhbi8xLjAiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL21waS9nbnUvMTIuMC9vZmkvMS4wL2NyYXktbXBpY2gvOC4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9jb21uZXQvZ251LzEyLjAvb2ZpLzEuMCIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxl"
declare -x _ModuleTable014_="ZmlsZXMvbWl4X2NvbXBpbGVycyIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29tcGlsZXIvZ251LzEyLjAiLCAiL3NvZnQvbW9kdWxlZmlsZXMiCiwgIi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL3BlcmZ0b29scy8yMy4xMi4wIgosICIvb3B0L2NyYXkvcGUvbG1vZC9tb2R1bGVmaWxlcy9uZXQvb2ZpLzEuMCIsICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0xpbnV4IgosICIvdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmUiLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZSIKLCAiL3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BhbHMvbG1vZC9tb2R1bGVmaWxlcy9jb3JlIgosICIvb3B0L2Ny"
declare -x _ModuleTable015_="YXkvbW9kdWxlZmlsZXMiLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY29yZSIKLCAiL29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdCIKLCAiL3NvZnQvcGVyZnRvb2xzL2RhcnNoYW4vZGFyc2hhbi0zLjQuNC9zaGFyZS9jcmF5cGUtMi54L21vZHVsZWZpbGVzIiwgIi9zb2Z0L3hhbHQvbW9kdWxlZmlsZXMiLAp9LApzeXN0ZW1CYXNlTVBBVEggPSAiL3Vzci9zaGFyZS9tb2R1bGVmaWxlcy9MaW51eDovdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmU6L3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZTovdXNyL3NoYXJlL2xtb2QvbG1vZC9tb2R1bGVmaWxlczovb3B0L2NyYXkvcGFscy9sbW9kL21vZHVs"
declare -x _ModuleTable016_="ZWZpbGVzL2NvcmU6L29wdC9jcmF5L21vZHVsZWZpbGVzOi9vcHQvY3JheS9wZS9sbW9kL21vZHVsZWZpbGVzL2NvcmU6L29wdC9jcmF5L3BlL2xtb2QvbW9kdWxlZmlsZXMvY3JheXBlLXRhcmdldHMvZGVmYXVsdDovc29mdC9wZXJmdG9vbHMvZGFyc2hhbi9kYXJzaGFuLTMuNC40L3NoYXJlL2NyYXlwZS0yLngvbW9kdWxlZmlsZXM6L3NvZnQveGFsdC9tb2R1bGVmaWxlcyIsCn0K"
declare -x _ModuleTable_Sz_="16"
declare -x __LMOD_Priority_PATH="/soft/xalt/3.0.2-202408282050/bin:-100"
declare -x __LMOD_REF_COUNT_COMPILER_PATH="/soft/xalt/3.0.2-202408282050/bin:1"
declare -x __LMOD_REF_COUNT_CRAY_LD_LIBRARY_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib:1;/opt/cray/pe/pmi/6.1.13/lib:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib:1;/opt/cray/pe/mpich/8.1.28/gtl/lib:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib:1;/opt/cray/pe/perftools/23.12.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_LIBRARY_PATH="/soft/compilers/cudatoolkit/cuda-12.4.1/extras/CUPTI/lib64:1;/soft/compilers/cudatoolkit/cuda-12.4.1/lib64:1;/soft/libraries/trt/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0/lib:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/lib:1;/soft/libraries/cudnn/cudnn-cuda12-linux-x64-v9.1.0.70/lib:1;/soft/perftools/darshan/darshan-3.4.4/lib:1;/opt/cray/pe/papi/7.0.1.2/lib64:1;/opt/cray/libfabric/1.15.2.0/lib64:1"
declare -x __LMOD_REF_COUNT_LD_PRELOAD="/soft/xalt/3.0.2-202408282050/lib64/libxalt_init.so:1"
declare -x __LMOD_REF_COUNT_MANPATH="/opt/cray/pals/1.3.4/man:2;/opt/cray/pe/pmi/6.1.13/man:1;/opt/cray/pe/mpich/8.1.28/ofi/man:1;/opt/cray/pe/mpich/8.1.28/man/mpich:1;/opt/cray/pe/dsmml/0.2.2/dsmml/man:1;/opt/cray/pe/craype/2.7.30/man:1;/opt/cray/pe/perftools/23.12.0/man:1;/opt/cray/pe/papi/7.0.1.2/share/pdoc/man:1;/opt/cray/libfabric/1.15.2.0/share/man:1;/usr/share/lmod/lmod/share/man:1;/home/shourya01/.local/man:1;/usr/local/man:1;/usr/share/man:1;/usr/man:1;/opt/c3/man:1;/opt/pbs/share/man:1;/opt/clmgr/man:1;/opt/sgi/share/man:1;/opt/clmgr/share/man:1;/opt/clmgr/lib/cm-cli/man:1"
declare -x __LMOD_REF_COUNT_MODULEPATH="/opt/cray/pe/lmod/modulefiles/hdf5-parallel/gnu/12.0/ofi/1.0/cray-mpich/8.0/cray-hdf5-parallel/1.12.2:1;/opt/cray/pe/lmod/modulefiles/cpu/x86-milan/1.0:1;/opt/cray/pe/lmod/modulefiles/mpi/gnu/12.0/ofi/1.0/cray-mpich/8.0:1;/opt/cray/pe/lmod/modulefiles/comnet/gnu/12.0/ofi/1.0:1;/opt/cray/pe/lmod/modulefiles/mix_compilers:1;/opt/cray/pe/lmod/modulefiles/compiler/gnu/12.0:1;/soft/modulefiles:1;/opt/cray/pe/lmod/modulefiles/perftools/23.12.0:1;/opt/cray/pe/lmod/modulefiles/net/ofi/1.0:1;/usr/share/modulefiles/Linux:1;/usr/share/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles:1;/opt/cray/pals/lmod/modulefiles/core:1;/opt/cray/modulefiles:1;/opt/cray/pe/lmod/modulefiles/core:1;/opt/cray/pe/lmod/modulefiles/craype-targets/default:1;/soft/perftools/darshan/darshan-3.4.4/share/craype-2.x/modulefiles:1;/soft/xalt/modulefiles:1"
declare -x __LMOD_REF_COUNT_PATH="/soft/xalt/3.0.2-202408282050/bin:1;/soft/compilers/cudatoolkit/cuda-12.4.1/bin:1;/soft/libraries/nccl/nccl_2.21.5-1+cuda12.4_x86_64/include:1;/opt/cray/pe/hdf5-parallel/1.12.2.9/bin:1;/opt/cray/pe/hdf5/1.12.2.9/bin:1;/opt/cray/pals/1.3.4/bin:1;/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:1;/opt/cray/pe/mpich/8.1.28/bin:1;/opt/cray/pe/craype/2.7.30/bin:1;/home/shourya01/.local/bin:4;/soft/perftools/darshan/darshan-3.4.4/bin:1;/opt/cray/pe/perftools/23.12.0/bin:1;/opt/cray/pe/papi/7.0.1.2/bin:1;/opt/cray/libfabric/1.15.2.0/bin:1;/opt/clmgr/sbin:1;/opt/clmgr/bin:1;/opt/sgi/sbin:1;/opt/sgi/bin:1;/usr/local/bin:1;/usr/bin:1;/bin:2;/opt/c3/bin:1;/usr/lib/mit/bin:1;/usr/lib/mit/sbin:1;/opt/pbs/bin:1;/sbin:1;/home/shourya01/bin:1;/opt/cray/pe/bin:1"
declare -x __LMOD_REF_COUNT_PE_DSMML_PKGCONFIG_LIBS="dsmml:1"
declare -x __LMOD_REF_COUNT_PE_FORTRAN_PKGCONFIG_LIBS="hdf5hl_fortran_parallel:1;hdf5_fortran_parallel:1;mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_GNU_FIXED_PKGCONFIG_PATH="/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/lib/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FIXED_PRGENV="GNU:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_FORTRAN_PKGCONFIG_LIBS="mpichf90:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_GENCOMPILERS_GNU="12.3:1"
declare -x __LMOD_REF_COUNT_PE_MPICH_PKGCONFIG_LIBS="mpich:1"
declare -x __LMOD_REF_COUNT_PE_PALS_PKGCONFIG_LIBS="libpals:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_LIBS="hdf5_hl_parallel:1;hdf5_parallel:1;mpich:1;dsmml:1;darshan-runtime:1"
declare -x __LMOD_REF_COUNT_PE_PKGCONFIG_PRODUCTS="PE_PALS:1;PE_PMI:1;PE_MPICH:1;PE_DSMML:1"
declare -x __LMOD_REF_COUNT_PE_PMI_PKGCONFIG_LIBS="cray-pmi:1"
declare -x __LMOD_REF_COUNT_PE_PRODUCT_LIST="CRAYPE_X86_MILAN:1;PERFTOOLS:1;CRAYPAT:1"
declare -x __LMOD_REF_COUNT_PKG_CONFIG_PATH="/opt/cray/pe/hdf5-parallel/1.12.2.9/gnu/12.3/lib/pkgconfig:1;/opt/cray/pals/1.3.4/lib/pkgconfig:1;/opt/cray/pe/pmi/6.1.13/lib/pkgconfig:1;/opt/cray/pe/dsmml/0.2.2/dsmml/lib/pkgconfig:1;/opt/cray/pe/craype/2.7.30/pkg-config:1;/soft/perftools/darshan/darshan-3.4.4/lib/pkgconfig:1;/opt/cray/libfabric/1.15.2.0/lib64/pkgconfig:1"
declare -x __LMOD_REF_COUNT_PYTHONPATH="/soft/xalt/3.0.2-202408282050/site_packages:1"
declare -x ftp_proxy="http://proxy.alcf.anl.gov:3128"
declare -x http_proxy="http://proxy.alcf.anl.gov:3128"
declare -x https_proxy="http://proxy.alcf.anl.gov:3128"
declare -x no_proxy="admin,polaris-adminvm-01,localhost,*.cm.polaris.alcf.anl.gov,polaris-*,*.polaris.alcf.anl.gov,*.alcf.anl.gov"
Running on 2 nodes
Total number of GPUs: 8
Connected to tcp://x3003c0s13b1n0.hsn.cm.polaris.alcf.anl.gov:7919
Found executable /soft/applications/conda/2024-04-29/mconda3/bin/python
Launching application 5dfb86c4-5d31-42fc-b76c-e9b2b19bdbf7
Using PMI port 42977,42978
[2025-06-25 00:51:27,608] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:51:27,608] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:51:27,608] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:51:27,608] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:51:30,357] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:51:30,357] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:51:30,357] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-25 00:51:30,357] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:51:31,186] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:51:31,186] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:51:31,186] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:51:31,186] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:51:31,186] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:51:31,186] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:51:31,186] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-25 00:51:31,186] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:51:37,326] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:51:37,326] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:51:37,327] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:51:37,326] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:51:37,327] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2025-06-25 00:51:37,326] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-06-25 00:51:37,327] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-25 00:51:37,327] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-06-25 00:51:37,329] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=0, world_size=8, master_addr=10.140.57.25, master_port=29500
[2025-06-25 00:51:37,329] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=1, world_size=8, master_addr=10.140.57.25, master_port=29500
[2025-06-25 00:51:37,329] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=8, master_addr=10.140.57.25, master_port=29500
[2025-06-25 00:51:37,329] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=2, world_size=8, master_addr=10.140.57.25, master_port=29500
[2025-06-25 00:51:37,329] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=3, world_size=8, master_addr=10.140.57.25, master_port=29500
[2025-06-25 00:51:37,329] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=8, master_addr=10.140.57.25, master_port=29500
[2025-06-25 00:51:37,329] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=8, master_addr=10.140.57.25, master_port=29500
[2025-06-25 00:51:37,329] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=8, master_addr=10.140.57.25, master_port=29500
[2025-06-25 00:51:37,330] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  mean_w = torch.tensor(w.mean(axis=(0,1), keepdims=True), dtype=dtype)
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
/home/shourya01/stormer_deepspeed/stormer_specific_data_utils.py:268: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  std_w = torch.tensor(w.std(axis=(0,1), keepdims=True), dtype=dtype) + 1e-6
Initialized deepspeed on global rank 0, local rank 0 with world size 8.
[2025-06-25 00:51:42,556] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2+5f631abc, git-hash=5f631abc, git-branch=HEAD
[2025-06-25 00:51:46,933] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-06-25 00:51:46,934] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-06-25 00:51:46,934] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-06-25 00:51:46,994] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-06-25 00:51:46,994] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2025-06-25 00:51:46,994] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-06-25 00:51:46,994] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-06-25 00:51:46,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0005], mom=[(0.9, 0.999)]
[2025-06-25 00:51:46,995] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   amp_params ................... False
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x14999e862510>
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2025-06-25 00:51:46,995] [INFO] [config.py:1000:print]   dump_state ................... False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   global_rank .................. 0
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 0.0005, 'weight_decay': 0.01}
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   pld_params ................... False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   steps_per_print .............. 100000
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   train_batch_size ............. 1024
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  128
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   world_size ................... 8
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2025-06-25 00:51:46,996] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2025-06-25 00:51:46,996] [INFO] [config.py:986:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 128, 
    "train_batch_size": 1.024000e+03, 
    "steps_per_print": 1.000000e+05, 
    "gradient_accumulation_steps": 1, 
    "fp16": {
        "enabled": false
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0005, 
            "weight_decay": 0.01
        }
    }, 
    "comms_logger": {
        "enabled": true, 
        "verbose": false
    }, 
    "zero_optimization": {
        "stage": 0
    }
}
Validating lr=0.0005, train epoch 0.:   0%|          | 0/535 [00:00<?, ?it/s]Validating lr=0.0005, train epoch 0.:   0%|          | 1/535 [00:02<20:45,  2.33s/it]Validating lr=0.0005, train epoch 0.:   0%|          | 2/535 [00:03<14:37,  1.65s/it]Validating lr=0.0005, train epoch 0.:   1%|          | 3/535 [00:04<12:37,  1.42s/it]Validating lr=0.0005, train epoch 0.:   1%|          | 4/535 [00:05<11:42,  1.32s/it]Validating lr=0.0005, train epoch 0.:   1%|          | 5/535 [00:07<11:14,  1.27s/it]Validating lr=0.0005, train epoch 0.:   1%|          | 6/535 [00:08<10:51,  1.23s/it]Validating lr=0.0005, train epoch 0.:   1%|▏         | 7/535 [00:09<10:39,  1.21s/it]Validating lr=0.0005, train epoch 0.:   1%|▏         | 8/535 [00:10<10:33,  1.20s/it]Validating lr=0.0005, train epoch 0.:   2%|▏         | 9/535 [00:11<10:23,  1.19s/it]Validating lr=0.0005, train epoch 0.:   2%|▏         | 10/535 [00:12<10:22,  1.19s/it]Validating lr=0.0005, train epoch 0.:   2%|▏         | 11/535 [00:14<10:17,  1.18s/it]Validating lr=0.0005, train epoch 0.:   2%|▏         | 12/535 [00:15<10:15,  1.18s/it]Validating lr=0.0005, train epoch 0.:   2%|▏         | 13/535 [00:16<10:12,  1.17s/it]Validating lr=0.0005, train epoch 0.:   3%|▎         | 14/535 [00:17<10:14,  1.18s/it]Validating lr=0.0005, train epoch 0.:   3%|▎         | 15/535 [00:18<10:12,  1.18s/it]Validating lr=0.0005, train epoch 0.:   3%|▎         | 16/535 [00:19<10:12,  1.18s/it]Validating lr=0.0005, train epoch 0.:   3%|▎         | 17/535 [00:21<10:11,  1.18s/it]Validating lr=0.0005, train epoch 0.:   3%|▎         | 18/535 [00:22<10:12,  1.19s/it]Validating lr=0.0005, train epoch 0.:   4%|▎         | 19/535 [00:23<10:07,  1.18s/it]Validating lr=0.0005, train epoch 0.:   4%|▎         | 20/535 [00:24<10:06,  1.18s/it]Validating lr=0.0005, train epoch 0.:   4%|▍         | 21/535 [00:25<10:03,  1.17s/it]Validating lr=0.0005, train epoch 0.:   4%|▍         | 22/535 [00:26<10:00,  1.17s/it]Validating lr=0.0005, train epoch 0.:   4%|▍         | 23/535 [00:28<10:01,  1.17s/it]Validating lr=0.0005, train epoch 0.:   4%|▍         | 24/535 [00:29<10:01,  1.18s/it]Validating lr=0.0005, train epoch 0.:   5%|▍         | 25/535 [00:30<09:59,  1.18s/it]Validating lr=0.0005, train epoch 0.:   5%|▍         | 26/535 [00:31<09:57,  1.17s/it]Validating lr=0.0005, train epoch 0.:   5%|▌         | 27/535 [00:32<09:56,  1.18s/it]Validating lr=0.0005, train epoch 0.:   5%|▌         | 28/535 [00:34<09:56,  1.18s/it]Validating lr=0.0005, train epoch 0.:   5%|▌         | 29/535 [00:35<09:56,  1.18s/it]Validating lr=0.0005, train epoch 0.:   6%|▌         | 30/535 [00:36<09:50,  1.17s/it]Validating lr=0.0005, train epoch 0.:   6%|▌         | 31/535 [00:37<09:48,  1.17s/it]Validating lr=0.0005, train epoch 0.:   6%|▌         | 32/535 [00:38<09:47,  1.17s/it]Validating lr=0.0005, train epoch 0.:   6%|▌         | 33/535 [00:39<09:46,  1.17s/it]Validating lr=0.0005, train epoch 0.:   6%|▋         | 34/535 [00:41<09:46,  1.17s/it]Validating lr=0.0005, train epoch 0.:   7%|▋         | 35/535 [00:42<09:46,  1.17s/it]Validating lr=0.0005, train epoch 0.:   7%|▋         | 36/535 [00:43<09:45,  1.17s/it]Validating lr=0.0005, train epoch 0.:   7%|▋         | 37/535 [00:44<09:44,  1.17s/it]Validating lr=0.0005, train epoch 0.:   7%|▋         | 38/535 [00:45<09:42,  1.17s/it]Validating lr=0.0005, train epoch 0.:   7%|▋         | 39/535 [00:46<09:40,  1.17s/it]Validating lr=0.0005, train epoch 0.:   7%|▋         | 40/535 [00:48<09:40,  1.17s/it]Validating lr=0.0005, train epoch 0.:   8%|▊         | 41/535 [00:49<09:41,  1.18s/it]Validating lr=0.0005, train epoch 0.:   8%|▊         | 42/535 [00:50<09:40,  1.18s/it]Validating lr=0.0005, train epoch 0.:   8%|▊         | 43/535 [00:51<09:37,  1.17s/it]Validating lr=0.0005, train epoch 0.:   8%|▊         | 44/535 [00:52<09:34,  1.17s/it]Validating lr=0.0005, train epoch 0.:   8%|▊         | 45/535 [00:53<09:32,  1.17s/it]Validating lr=0.0005, train epoch 0.:   9%|▊         | 46/535 [00:55<09:32,  1.17s/it]Validating lr=0.0005, train epoch 0.:   9%|▉         | 47/535 [00:56<09:31,  1.17s/it]Validating lr=0.0005, train epoch 0.:   9%|▉         | 48/535 [00:57<09:30,  1.17s/it]Validating lr=0.0005, train epoch 0.:   9%|▉         | 49/535 [00:58<09:30,  1.17s/it]Validating lr=0.0005, train epoch 0.:   9%|▉         | 50/535 [00:59<09:28,  1.17s/it]Validating lr=0.0005, train epoch 0.:  10%|▉         | 51/535 [01:00<09:28,  1.17s/it]Validating lr=0.0005, train epoch 0.:  10%|▉         | 52/535 [01:02<09:28,  1.18s/it]Validating lr=0.0005, train epoch 0.:  10%|▉         | 53/535 [01:03<09:28,  1.18s/it]Validating lr=0.0005, train epoch 0.:  10%|█         | 54/535 [01:04<09:29,  1.18s/it]Validating lr=0.0005, train epoch 0.:  10%|█         | 55/535 [01:05<09:23,  1.17s/it]Validating lr=0.0005, train epoch 0.:  10%|█         | 56/535 [01:06<09:21,  1.17s/it]Validating lr=0.0005, train epoch 0.:  11%|█         | 57/535 [01:08<09:19,  1.17s/it]Validating lr=0.0005, train epoch 0.:  11%|█         | 58/535 [01:09<09:20,  1.17s/it]Validating lr=0.0005, train epoch 0.:  11%|█         | 59/535 [01:10<09:16,  1.17s/it]Validating lr=0.0005, train epoch 0.:  11%|█         | 60/535 [01:11<09:17,  1.17s/it]Validating lr=0.0005, train epoch 0.:  11%|█▏        | 61/535 [01:12<09:17,  1.18s/it]Validating lr=0.0005, train epoch 0.:  12%|█▏        | 62/535 [01:13<09:14,  1.17s/it]Validating lr=0.0005, train epoch 0.:  12%|█▏        | 63/535 [01:15<09:14,  1.17s/it]Validating lr=0.0005, train epoch 0.:  12%|█▏        | 64/535 [01:16<09:12,  1.17s/it]Validating lr=0.0005, train epoch 0.:  12%|█▏        | 65/535 [01:17<09:09,  1.17s/it]Validating lr=0.0005, train epoch 0.:  12%|█▏        | 66/535 [01:18<09:08,  1.17s/it]Validating lr=0.0005, train epoch 0.:  13%|█▎        | 67/535 [01:19<09:06,  1.17s/it]Validating lr=0.0005, train epoch 0.:  13%|█▎        | 68/535 [01:20<09:04,  1.17s/it]Validating lr=0.0005, train epoch 0.:  13%|█▎        | 69/535 [01:22<09:05,  1.17s/it]Validating lr=0.0005, train epoch 0.:  13%|█▎        | 70/535 [01:23<09:04,  1.17s/it]Validating lr=0.0005, train epoch 0.:  13%|█▎        | 71/535 [01:24<09:04,  1.17s/it]Validating lr=0.0005, train epoch 0.:  13%|█▎        | 72/535 [01:25<09:03,  1.17s/it]Validating lr=0.0005, train epoch 0.:  14%|█▎        | 73/535 [01:26<09:02,  1.17s/it]Validating lr=0.0005, train epoch 0.:  14%|█▍        | 74/535 [01:27<09:01,  1.17s/it]Validating lr=0.0005, train epoch 0.:  14%|█▍        | 75/535 [01:29<09:01,  1.18s/it]Validating lr=0.0005, train epoch 0.:  14%|█▍        | 76/535 [01:30<08:56,  1.17s/it]Validating lr=0.0005, train epoch 0.:  14%|█▍        | 77/535 [01:31<08:56,  1.17s/it]Validating lr=0.0005, train epoch 0.:  15%|█▍        | 78/535 [01:32<08:56,  1.17s/it]Validating lr=0.0005, train epoch 0.:  15%|█▍        | 79/535 [01:33<08:52,  1.17s/it]Validating lr=0.0005, train epoch 0.:  15%|█▍        | 80/535 [01:34<08:51,  1.17s/it]Validating lr=0.0005, train epoch 0.:  15%|█▌        | 81/535 [01:36<08:51,  1.17s/it]Validating lr=0.0005, train epoch 0.:  15%|█▌        | 82/535 [01:37<08:50,  1.17s/it]Validating lr=0.0005, train epoch 0.:  16%|█▌        | 83/535 [01:38<08:49,  1.17s/it]Validating lr=0.0005, train epoch 0.:  16%|█▌        | 84/535 [01:39<08:46,  1.17s/it]Validating lr=0.0005, train epoch 0.:  16%|█▌        | 85/535 [01:40<08:44,  1.16s/it]Validating lr=0.0005, train epoch 0.:  16%|█▌        | 86/535 [01:41<08:41,  1.16s/it]Validating lr=0.0005, train epoch 0.:  16%|█▋        | 87/535 [01:43<08:42,  1.17s/it]Validating lr=0.0005, train epoch 0.:  16%|█▋        | 88/535 [01:44<08:40,  1.16s/it]Validating lr=0.0005, train epoch 0.:  17%|█▋        | 89/535 [01:45<08:41,  1.17s/it]Validating lr=0.0005, train epoch 0.:  17%|█▋        | 90/535 [01:46<08:41,  1.17s/it]Validating lr=0.0005, train epoch 0.:  17%|█▋        | 91/535 [01:47<08:40,  1.17s/it]Validating lr=0.0005, train epoch 0.:  17%|█▋        | 92/535 [01:48<08:39,  1.17s/it]Validating lr=0.0005, train epoch 0.:  17%|█▋        | 93/535 [01:50<08:38,  1.17s/it]Validating lr=0.0005, train epoch 0.:  18%|█▊        | 94/535 [01:51<08:38,  1.18s/it]Validating lr=0.0005, train epoch 0.:  18%|█▊        | 95/535 [01:52<08:38,  1.18s/it]Validating lr=0.0005, train epoch 0.:  18%|█▊        | 96/535 [01:53<08:36,  1.18s/it]Validating lr=0.0005, train epoch 0.:  18%|█▊        | 97/535 [01:54<08:35,  1.18s/it]Validating lr=0.0005, train epoch 0.:  18%|█▊        | 98/535 [01:56<08:35,  1.18s/it]Validating lr=0.0005, train epoch 0.:  19%|█▊        | 99/535 [01:57<08:33,  1.18s/it]Validating lr=0.0005, train epoch 0.:  19%|█▊        | 100/535 [01:58<08:34,  1.18s/it]Validating lr=0.0005, train epoch 0.:  19%|█▉        | 101/535 [01:59<08:30,  1.18s/it]Validating lr=0.0005, train epoch 0.:  19%|█▉        | 102/535 [02:00<08:27,  1.17s/it]Validating lr=0.0005, train epoch 0.:  19%|█▉        | 103/535 [02:01<08:25,  1.17s/it]Validating lr=0.0005, train epoch 0.:  19%|█▉        | 104/535 [02:03<08:25,  1.17s/it]Validating lr=0.0005, train epoch 0.:  20%|█▉        | 105/535 [02:04<08:26,  1.18s/it]Validating lr=0.0005, train epoch 0.:  20%|█▉        | 106/535 [02:05<08:24,  1.18s/it]Validating lr=0.0005, train epoch 0.:  20%|██        | 107/535 [02:06<08:24,  1.18s/it]Validating lr=0.0005, train epoch 0.:  20%|██        | 108/535 [02:07<08:24,  1.18s/it]Validating lr=0.0005, train epoch 0.:  20%|██        | 109/535 [02:09<08:22,  1.18s/it]Validating lr=0.0005, train epoch 0.:  21%|██        | 110/535 [02:10<08:22,  1.18s/it]Validating lr=0.0005, train epoch 0.:  21%|██        | 111/535 [02:11<08:19,  1.18s/it]Validating lr=0.0005, train epoch 0.:  21%|██        | 112/535 [02:12<08:18,  1.18s/it]Validating lr=0.0005, train epoch 0.:  21%|██        | 113/535 [02:13<08:15,  1.17s/it]Validating lr=0.0005, train epoch 0.:  21%|██▏       | 114/535 [02:14<08:15,  1.18s/it]Validating lr=0.0005, train epoch 0.:  21%|██▏       | 115/535 [02:16<08:13,  1.18s/it]Validating lr=0.0005, train epoch 0.:  22%|██▏       | 116/535 [02:17<08:13,  1.18s/it]Validating lr=0.0005, train epoch 0.:  22%|██▏       | 117/535 [02:18<08:11,  1.18s/it]Validating lr=0.0005, train epoch 0.:  22%|██▏       | 118/535 [02:19<08:07,  1.17s/it]Validating lr=0.0005, train epoch 0.:  22%|██▏       | 119/535 [02:20<08:05,  1.17s/it]Validating lr=0.0005, train epoch 0.:  22%|██▏       | 120/535 [02:21<08:05,  1.17s/it]Validating lr=0.0005, train epoch 0.:  23%|██▎       | 121/535 [02:23<08:06,  1.18s/it]Validating lr=0.0005, train epoch 0.:  23%|██▎       | 122/535 [02:24<08:05,  1.18s/it]Validating lr=0.0005, train epoch 0.:  23%|██▎       | 123/535 [02:25<08:05,  1.18s/it]Validating lr=0.0005, train epoch 0.:  23%|██▎       | 124/535 [02:26<08:05,  1.18s/it]Validating lr=0.0005, train epoch 0.:  23%|██▎       | 125/535 [02:27<08:03,  1.18s/it]Validating lr=0.0005, train epoch 0.:  24%|██▎       | 126/535 [02:29<08:02,  1.18s/it]Validating lr=0.0005, train epoch 0.:  24%|██▎       | 127/535 [02:30<08:02,  1.18s/it]Validating lr=0.0005, train epoch 0.:  24%|██▍       | 128/535 [02:31<08:08,  1.20s/it]Validating lr=0.0005, train epoch 0.:  24%|██▍       | 129/535 [02:32<08:17,  1.23s/it]Validating lr=0.0005, train epoch 0.:  24%|██▍       | 130/535 [02:33<08:11,  1.21s/it]Validating lr=0.0005, train epoch 0.:  24%|██▍       | 131/535 [02:35<08:05,  1.20s/it]Validating lr=0.0005, train epoch 0.:  25%|██▍       | 132/535 [02:36<07:59,  1.19s/it]Validating lr=0.0005, train epoch 0.:  25%|██▍       | 133/535 [02:37<07:55,  1.18s/it]Validating lr=0.0005, train epoch 0.:  25%|██▌       | 134/535 [02:38<07:51,  1.18s/it]Validating lr=0.0005, train epoch 0.:  25%|██▌       | 135/535 [02:39<07:49,  1.17s/it]Validating lr=0.0005, train epoch 0.:  25%|██▌       | 136/535 [02:40<07:49,  1.18s/it]Validating lr=0.0005, train epoch 0.:  26%|██▌       | 137/535 [02:42<07:46,  1.17s/it]Validating lr=0.0005, train epoch 0.:  26%|██▌       | 138/535 [02:43<07:46,  1.17s/it]Validating lr=0.0005, train epoch 0.:  26%|██▌       | 139/535 [02:44<07:46,  1.18s/it]Validating lr=0.0005, train epoch 0.:  26%|██▌       | 140/535 [02:45<07:45,  1.18s/it]Validating lr=0.0005, train epoch 0.:  26%|██▋       | 141/535 [02:46<07:43,  1.18s/it]Validating lr=0.0005, train epoch 0.:  27%|██▋       | 142/535 [02:47<07:42,  1.18s/it]Validating lr=0.0005, train epoch 0.:  27%|██▋       | 143/535 [02:49<07:41,  1.18s/it]Validating lr=0.0005, train epoch 0.:  27%|██▋       | 144/535 [02:50<07:38,  1.17s/it]Validating lr=0.0005, train epoch 0.:  27%|██▋       | 145/535 [02:51<07:35,  1.17s/it]Validating lr=0.0005, train epoch 0.:  27%|██▋       | 146/535 [02:52<07:35,  1.17s/it]Validating lr=0.0005, train epoch 0.:  27%|██▋       | 147/535 [02:53<07:34,  1.17s/it]Validating lr=0.0005, train epoch 0.:  28%|██▊       | 148/535 [02:55<07:32,  1.17s/it]Validating lr=0.0005, train epoch 0.:  28%|██▊       | 149/535 [02:56<07:33,  1.17s/it]Validating lr=0.0005, train epoch 0.:  28%|██▊       | 150/535 [02:57<07:31,  1.17s/it]Validating lr=0.0005, train epoch 0.:  28%|██▊       | 151/535 [02:58<07:27,  1.17s/it]Validating lr=0.0005, train epoch 0.:  28%|██▊       | 152/535 [02:59<07:25,  1.16s/it]Validating lr=0.0005, train epoch 0.:  29%|██▊       | 153/535 [03:00<07:28,  1.17s/it]Validating lr=0.0005, train epoch 0.:  29%|██▉       | 154/535 [03:02<07:27,  1.18s/it]Validating lr=0.0005, train epoch 0.:  29%|██▉       | 155/535 [03:03<07:25,  1.17s/it]Validating lr=0.0005, train epoch 0.:  29%|██▉       | 156/535 [03:04<07:24,  1.17s/it]Validating lr=0.0005, train epoch 0.:  29%|██▉       | 157/535 [03:05<07:23,  1.17s/it]Validating lr=0.0005, train epoch 0.:  30%|██▉       | 158/535 [03:06<07:24,  1.18s/it]Validating lr=0.0005, train epoch 0.:  30%|██▉       | 159/535 [03:07<07:22,  1.18s/it]Validating lr=0.0005, train epoch 0.:  30%|██▉       | 160/535 [03:09<07:20,  1.17s/it]Validating lr=0.0005, train epoch 0.:  30%|███       | 161/535 [03:10<07:20,  1.18s/it]Validating lr=0.0005, train epoch 0.:  30%|███       | 162/535 [03:11<07:18,  1.18s/it]Validating lr=0.0005, train epoch 0.:  30%|███       | 163/535 [03:12<07:18,  1.18s/it]Validating lr=0.0005, train epoch 0.:  31%|███       | 164/535 [03:13<07:16,  1.18s/it]Validating lr=0.0005, train epoch 0.:  31%|███       | 165/535 [03:14<07:15,  1.18s/it]Validating lr=0.0005, train epoch 0.:  31%|███       | 166/535 [03:16<07:13,  1.17s/it]Validating lr=0.0005, train epoch 0.:  31%|███       | 167/535 [03:17<07:13,  1.18s/it]Validating lr=0.0005, train epoch 0.:  31%|███▏      | 168/535 [03:18<07:10,  1.17s/it]Validating lr=0.0005, train epoch 0.:  32%|███▏      | 169/535 [03:19<07:10,  1.18s/it]Validating lr=0.0005, train epoch 0.:  32%|███▏      | 170/535 [03:20<07:11,  1.18s/it]Validating lr=0.0005, train epoch 0.:  32%|███▏      | 171/535 [03:22<07:09,  1.18s/it]Validating lr=0.0005, train epoch 0.:  32%|███▏      | 172/535 [03:23<07:09,  1.18s/it]Validating lr=0.0005, train epoch 0.:  32%|███▏      | 173/535 [03:24<07:10,  1.19s/it]Validating lr=0.0005, train epoch 0.:  33%|███▎      | 174/535 [03:25<07:09,  1.19s/it]Validating lr=0.0005, train epoch 0.:  33%|███▎      | 175/535 [03:26<07:04,  1.18s/it]Validating lr=0.0005, train epoch 0.:  33%|███▎      | 176/535 [03:27<07:01,  1.17s/it]Validating lr=0.0005, train epoch 0.:  33%|███▎      | 177/535 [03:29<06:59,  1.17s/it]Validating lr=0.0005, train epoch 0.:  33%|███▎      | 178/535 [03:30<06:58,  1.17s/it]Validating lr=0.0005, train epoch 0.:  33%|███▎      | 179/535 [03:31<06:58,  1.18s/it]Validating lr=0.0005, train epoch 0.:  34%|███▎      | 180/535 [03:32<06:58,  1.18s/it]Validating lr=0.0005, train epoch 0.:  34%|███▍      | 181/535 [03:33<06:57,  1.18s/it]Validating lr=0.0005, train epoch 0.:  34%|███▍      | 182/535 [03:35<06:56,  1.18s/it]Validating lr=0.0005, train epoch 0.:  34%|███▍      | 183/535 [03:36<06:52,  1.17s/it]Validating lr=0.0005, train epoch 0.:  34%|███▍      | 184/535 [03:37<06:51,  1.17s/it]Validating lr=0.0005, train epoch 0.:  35%|███▍      | 185/535 [03:38<06:51,  1.18s/it]Validating lr=0.0005, train epoch 0.:  35%|███▍      | 186/535 [03:39<06:51,  1.18s/it]Validating lr=0.0005, train epoch 0.:  35%|███▍      | 187/535 [03:40<06:49,  1.18s/it]Validating lr=0.0005, train epoch 0.:  35%|███▌      | 188/535 [03:42<06:47,  1.18s/it]Validating lr=0.0005, train epoch 0.:  35%|███▌      | 189/535 [03:43<06:49,  1.18s/it]Validating lr=0.0005, train epoch 0.:  36%|███▌      | 190/535 [03:44<06:47,  1.18s/it]Validating lr=0.0005, train epoch 0.:  36%|███▌      | 191/535 [03:45<06:46,  1.18s/it]Validating lr=0.0005, train epoch 0.:  36%|███▌      | 192/535 [03:46<06:42,  1.17s/it]Validating lr=0.0005, train epoch 0.:  36%|███▌      | 193/535 [03:47<06:40,  1.17s/it]Validating lr=0.0005, train epoch 0.:  36%|███▋      | 194/535 [03:49<06:40,  1.17s/it]Validating lr=0.0005, train epoch 0.:  36%|███▋      | 195/535 [03:50<06:39,  1.18s/it]Validating lr=0.0005, train epoch 0.:  37%|███▋      | 196/535 [03:51<06:38,  1.18s/it]Validating lr=0.0005, train epoch 0.:  37%|███▋      | 197/535 [03:52<06:37,  1.18s/it]Validating lr=0.0005, train epoch 0.:  37%|███▋      | 198/535 [03:53<06:37,  1.18s/it]Validating lr=0.0005, train epoch 0.:  37%|███▋      | 199/535 [03:55<06:36,  1.18s/it]Validating lr=0.0005, train epoch 0.:  37%|███▋      | 200/535 [03:56<06:36,  1.18s/it]Validating lr=0.0005, train epoch 0.:  38%|███▊      | 201/535 [03:57<06:33,  1.18s/it]Validating lr=0.0005, train epoch 0.:  38%|███▊      | 202/535 [03:58<06:33,  1.18s/it]Validating lr=0.0005, train epoch 0.:  38%|███▊      | 203/535 [03:59<06:31,  1.18s/it]Validating lr=0.0005, train epoch 0.:  38%|███▊      | 204/535 [04:00<06:28,  1.17s/it]Validating lr=0.0005, train epoch 0.:  38%|███▊      | 205/535 [04:02<06:28,  1.18s/it]Validating lr=0.0005, train epoch 0.:  39%|███▊      | 206/535 [04:03<06:27,  1.18s/it]Validating lr=0.0005, train epoch 0.:  39%|███▊      | 207/535 [04:04<06:26,  1.18s/it]Validating lr=0.0005, train epoch 0.:  39%|███▉      | 208/535 [04:05<06:26,  1.18s/it]Validating lr=0.0005, train epoch 0.:  39%|███▉      | 209/535 [04:06<06:26,  1.18s/it]Validating lr=0.0005, train epoch 0.:  39%|███▉      | 210/535 [04:07<06:23,  1.18s/it]Validating lr=0.0005, train epoch 0.:  39%|███▉      | 211/535 [04:09<06:23,  1.18s/it]Validating lr=0.0005, train epoch 0.:  40%|███▉      | 212/535 [04:10<06:20,  1.18s/it]Validating lr=0.0005, train epoch 0.:  40%|███▉      | 213/535 [04:11<06:19,  1.18s/it]Validating lr=0.0005, train epoch 0.:  40%|████      | 214/535 [04:12<06:18,  1.18s/it]Validating lr=0.0005, train epoch 0.:  40%|████      | 215/535 [04:13<06:17,  1.18s/it]Validating lr=0.0005, train epoch 0.:  40%|████      | 216/535 [04:15<06:16,  1.18s/it]Validating lr=0.0005, train epoch 0.:  41%|████      | 217/535 [04:16<06:15,  1.18s/it]Validating lr=0.0005, train epoch 0.:  41%|████      | 218/535 [04:17<06:13,  1.18s/it]Validating lr=0.0005, train epoch 0.:  41%|████      | 219/535 [04:18<06:12,  1.18s/it]Validating lr=0.0005, train epoch 0.:  41%|████      | 220/535 [04:19<06:11,  1.18s/it]Validating lr=0.0005, train epoch 0.:  41%|████▏     | 221/535 [04:20<06:11,  1.18s/it]Validating lr=0.0005, train epoch 0.:  41%|████▏     | 222/535 [04:22<06:09,  1.18s/it]Validating lr=0.0005, train epoch 0.:  42%|████▏     | 223/535 [04:23<06:09,  1.18s/it]Validating lr=0.0005, train epoch 0.:  42%|████▏     | 224/535 [04:24<06:07,  1.18s/it]Validating lr=0.0005, train epoch 0.:  42%|████▏     | 225/535 [04:25<06:07,  1.19s/it]Validating lr=0.0005, train epoch 0.:  42%|████▏     | 226/535 [04:26<06:05,  1.18s/it]Validating lr=0.0005, train epoch 0.:  42%|████▏     | 227/535 [04:28<06:04,  1.18s/it]Validating lr=0.0005, train epoch 0.:  43%|████▎     | 228/535 [04:29<06:02,  1.18s/it]Validating lr=0.0005, train epoch 0.:  43%|████▎     | 229/535 [04:30<06:02,  1.18s/it]Validating lr=0.0005, train epoch 0.:  43%|████▎     | 230/535 [04:31<05:59,  1.18s/it]Validating lr=0.0005, train epoch 0.:  43%|████▎     | 231/535 [04:32<05:58,  1.18s/it]Validating lr=0.0005, train epoch 0.:  43%|████▎     | 232/535 [04:33<05:57,  1.18s/it]Validating lr=0.0005, train epoch 0.:  44%|████▎     | 233/535 [04:35<05:55,  1.18s/it]Validating lr=0.0005, train epoch 0.:  44%|████▎     | 234/535 [04:36<05:56,  1.18s/it]Validating lr=0.0005, train epoch 0.:  44%|████▍     | 235/535 [04:37<05:54,  1.18s/it]Validating lr=0.0005, train epoch 0.:  44%|████▍     | 236/535 [04:38<05:53,  1.18s/it]Validating lr=0.0005, train epoch 0.:  44%|████▍     | 237/535 [04:39<05:52,  1.18s/it]Validating lr=0.0005, train epoch 0.:  44%|████▍     | 238/535 [04:41<05:49,  1.18s/it]Validating lr=0.0005, train epoch 0.:  45%|████▍     | 239/535 [04:42<05:48,  1.18s/it]Validating lr=0.0005, train epoch 0.:  45%|████▍     | 240/535 [04:43<05:47,  1.18s/it]Validating lr=0.0005, train epoch 0.:  45%|████▌     | 241/535 [04:44<05:46,  1.18s/it]Validating lr=0.0005, train epoch 0.:  45%|████▌     | 242/535 [04:45<05:44,  1.18s/it]Validating lr=0.0005, train epoch 0.:  45%|████▌     | 243/535 [04:46<05:42,  1.17s/it]Validating lr=0.0005, train epoch 0.:  46%|████▌     | 244/535 [04:48<05:41,  1.17s/it]Validating lr=0.0005, train epoch 0.:  46%|████▌     | 245/535 [04:49<05:42,  1.18s/it]Validating lr=0.0005, train epoch 0.:  46%|████▌     | 246/535 [04:50<05:42,  1.18s/it]Validating lr=0.0005, train epoch 0.:  46%|████▌     | 247/535 [04:51<05:41,  1.18s/it]Validating lr=0.0005, train epoch 0.:  46%|████▋     | 248/535 [04:52<05:39,  1.18s/it]Validating lr=0.0005, train epoch 0.:  47%|████▋     | 249/535 [04:54<05:38,  1.18s/it]Validating lr=0.0005, train epoch 0.:  47%|████▋     | 250/535 [04:55<05:38,  1.19s/it]Validating lr=0.0005, train epoch 0.:  47%|████▋     | 251/535 [04:56<05:37,  1.19s/it]Validating lr=0.0005, train epoch 0.:  47%|████▋     | 252/535 [04:57<05:35,  1.18s/it]Validating lr=0.0005, train epoch 0.:  47%|████▋     | 253/535 [04:58<05:33,  1.18s/it]Validating lr=0.0005, train epoch 0.:  47%|████▋     | 254/535 [04:59<05:32,  1.18s/it]Validating lr=0.0005, train epoch 0.:  48%|████▊     | 255/535 [05:01<05:31,  1.18s/it]Validating lr=0.0005, train epoch 0.:  48%|████▊     | 256/535 [05:02<05:30,  1.19s/it]Validating lr=0.0005, train epoch 0.:  48%|████▊     | 257/535 [05:03<05:28,  1.18s/it]Validating lr=0.0005, train epoch 0.:  48%|████▊     | 258/535 [05:04<05:28,  1.19s/it]Validating lr=0.0005, train epoch 0.:  48%|████▊     | 259/535 [05:05<05:28,  1.19s/it]Validating lr=0.0005, train epoch 0.:  49%|████▊     | 260/535 [05:07<05:28,  1.19s/it]Validating lr=0.0005, train epoch 0.:  49%|████▉     | 261/535 [05:08<05:25,  1.19s/it]Validating lr=0.0005, train epoch 0.:  49%|████▉     | 262/535 [05:09<05:24,  1.19s/it]Validating lr=0.0005, train epoch 0.:  49%|████▉     | 263/535 [05:10<05:20,  1.18s/it]Validating lr=0.0005, train epoch 0.:  49%|████▉     | 264/535 [05:11<05:18,  1.18s/it]Validating lr=0.0005, train epoch 0.:  50%|████▉     | 265/535 [05:12<05:17,  1.18s/it]Validating lr=0.0005, train epoch 0.:  50%|████▉     | 266/535 [05:14<05:15,  1.17s/it]Validating lr=0.0005, train epoch 0.:  50%|████▉     | 267/535 [05:15<05:15,  1.18s/it]Validating lr=0.0005, train epoch 0.:  50%|█████     | 268/535 [05:16<05:13,  1.18s/it]Validating lr=0.0005, train epoch 0.:  50%|█████     | 269/535 [05:17<05:13,  1.18s/it]Validating lr=0.0005, train epoch 0.:  50%|█████     | 270/535 [05:18<05:10,  1.17s/it]Validating lr=0.0005, train epoch 0.:  51%|█████     | 271/535 [05:20<05:10,  1.17s/it]Validating lr=0.0005, train epoch 0.:  51%|█████     | 272/535 [05:21<05:08,  1.17s/it]Validating lr=0.0005, train epoch 0.:  51%|█████     | 273/535 [05:22<05:08,  1.18s/it]Validating lr=0.0005, train epoch 0.:  51%|█████     | 274/535 [05:23<05:08,  1.18s/it]Validating lr=0.0005, train epoch 0.:  51%|█████▏    | 275/535 [05:24<05:05,  1.18s/it]Validating lr=0.0005, train epoch 0.:  52%|█████▏    | 276/535 [05:25<05:05,  1.18s/it]Validating lr=0.0005, train epoch 0.:  52%|█████▏    | 277/535 [05:27<05:03,  1.18s/it]Validating lr=0.0005, train epoch 0.:  52%|█████▏    | 278/535 [05:28<05:01,  1.17s/it]Validating lr=0.0005, train epoch 0.:  52%|█████▏    | 279/535 [05:29<05:00,  1.17s/it]Validating lr=0.0005, train epoch 0.:  52%|█████▏    | 280/535 [05:30<04:59,  1.18s/it]Validating lr=0.0005, train epoch 0.:  53%|█████▎    | 281/535 [05:31<04:59,  1.18s/it]Validating lr=0.0005, train epoch 0.:  53%|█████▎    | 282/535 [05:32<04:57,  1.17s/it]Validating lr=0.0005, train epoch 0.:  53%|█████▎    | 283/535 [05:34<04:56,  1.18s/it]Validating lr=0.0005, train epoch 0.:  53%|█████▎    | 284/535 [05:35<04:55,  1.18s/it]Validating lr=0.0005, train epoch 0.:  53%|█████▎    | 285/535 [05:36<04:53,  1.18s/it]Validating lr=0.0005, train epoch 0.:  53%|█████▎    | 286/535 [05:37<04:52,  1.17s/it]Validating lr=0.0005, train epoch 0.:  54%|█████▎    | 287/535 [05:38<04:50,  1.17s/it]Validating lr=0.0005, train epoch 0.:  54%|█████▍    | 288/535 [05:40<04:50,  1.18s/it]Validating lr=0.0005, train epoch 0.:  54%|█████▍    | 289/535 [05:41<04:48,  1.17s/it]Validating lr=0.0005, train epoch 0.:  54%|█████▍    | 290/535 [05:42<04:47,  1.17s/it]Validating lr=0.0005, train epoch 0.:  54%|█████▍    | 291/535 [05:43<04:47,  1.18s/it]Validating lr=0.0005, train epoch 0.:  55%|█████▍    | 292/535 [05:44<04:46,  1.18s/it]Validating lr=0.0005, train epoch 0.:  55%|█████▍    | 293/535 [05:45<04:45,  1.18s/it]Validating lr=0.0005, train epoch 0.:  55%|█████▍    | 294/535 [05:47<04:44,  1.18s/it]Validating lr=0.0005, train epoch 0.:  55%|█████▌    | 295/535 [05:48<04:42,  1.18s/it]Validating lr=0.0005, train epoch 0.:  55%|█████▌    | 296/535 [05:49<04:41,  1.18s/it]Validating lr=0.0005, train epoch 0.:  56%|█████▌    | 297/535 [05:50<04:40,  1.18s/it]Validating lr=0.0005, train epoch 0.:  56%|█████▌    | 298/535 [05:51<04:39,  1.18s/it]Validating lr=0.0005, train epoch 0.:  56%|█████▌    | 299/535 [05:52<04:38,  1.18s/it]Validating lr=0.0005, train epoch 0.:  56%|█████▌    | 300/535 [05:54<04:37,  1.18s/it]Validating lr=0.0005, train epoch 0.:  56%|█████▋    | 301/535 [05:55<04:34,  1.17s/it]Validating lr=0.0005, train epoch 0.:  56%|█████▋    | 302/535 [05:56<04:34,  1.18s/it]Validating lr=0.0005, train epoch 0.:  57%|█████▋    | 303/535 [05:57<04:33,  1.18s/it]Validating lr=0.0005, train epoch 0.:  57%|█████▋    | 304/535 [05:58<04:30,  1.17s/it]Validating lr=0.0005, train epoch 0.:  57%|█████▋    | 305/535 [06:00<04:29,  1.17s/it]Validating lr=0.0005, train epoch 0.:  57%|█████▋    | 306/535 [06:01<04:29,  1.18s/it]Validating lr=0.0005, train epoch 0.:  57%|█████▋    | 307/535 [06:02<04:26,  1.17s/it]Validating lr=0.0005, train epoch 0.:  58%|█████▊    | 308/535 [06:03<04:26,  1.17s/it]Validating lr=0.0005, train epoch 0.:  58%|█████▊    | 309/535 [06:04<04:24,  1.17s/it]Validating lr=0.0005, train epoch 0.:  58%|█████▊    | 310/535 [06:05<04:23,  1.17s/it]Validating lr=0.0005, train epoch 0.:  58%|█████▊    | 311/535 [06:07<04:22,  1.17s/it]Validating lr=0.0005, train epoch 0.:  58%|█████▊    | 312/535 [06:08<04:21,  1.17s/it]Validating lr=0.0005, train epoch 0.:  59%|█████▊    | 313/535 [06:09<04:19,  1.17s/it]Validating lr=0.0005, train epoch 0.:  59%|█████▊    | 314/535 [06:10<04:18,  1.17s/it]Validating lr=0.0005, train epoch 0.:  59%|█████▉    | 315/535 [06:11<04:18,  1.18s/it]Validating lr=0.0005, train epoch 0.:  59%|█████▉    | 316/535 [06:12<04:16,  1.17s/it]Validating lr=0.0005, train epoch 0.:  59%|█████▉    | 317/535 [06:14<04:16,  1.18s/it]Validating lr=0.0005, train epoch 0.:  59%|█████▉    | 318/535 [06:15<04:15,  1.18s/it]Validating lr=0.0005, train epoch 0.:  60%|█████▉    | 319/535 [06:16<04:14,  1.18s/it]Validating lr=0.0005, train epoch 0.:  60%|█████▉    | 320/535 [06:17<04:12,  1.18s/it]Validating lr=0.0005, train epoch 0.:  60%|██████    | 321/535 [06:18<04:12,  1.18s/it]Validating lr=0.0005, train epoch 0.:  60%|██████    | 322/535 [06:20<04:12,  1.18s/it]Validating lr=0.0005, train epoch 0.:  60%|██████    | 323/535 [06:21<04:09,  1.18s/it]Validating lr=0.0005, train epoch 0.:  61%|██████    | 324/535 [06:22<04:08,  1.18s/it]Validating lr=0.0005, train epoch 0.:  61%|██████    | 325/535 [06:23<04:07,  1.18s/it]Validating lr=0.0005, train epoch 0.:  61%|██████    | 326/535 [06:24<04:04,  1.17s/it]Validating lr=0.0005, train epoch 0.:  61%|██████    | 327/535 [06:25<04:03,  1.17s/it]Validating lr=0.0005, train epoch 0.:  61%|██████▏   | 328/535 [06:27<04:03,  1.18s/it]Validating lr=0.0005, train epoch 0.:  61%|██████▏   | 329/535 [06:28<04:02,  1.18s/it]Validating lr=0.0005, train epoch 0.:  62%|██████▏   | 330/535 [06:29<04:01,  1.18s/it]Validating lr=0.0005, train epoch 0.:  62%|██████▏   | 331/535 [06:30<04:00,  1.18s/it]Validating lr=0.0005, train epoch 0.:  62%|██████▏   | 332/535 [06:31<03:59,  1.18s/it]Validating lr=0.0005, train epoch 0.:  62%|██████▏   | 333/535 [06:32<03:58,  1.18s/it]Validating lr=0.0005, train epoch 0.:  62%|██████▏   | 334/535 [06:34<03:55,  1.17s/it]Validating lr=0.0005, train epoch 0.:  63%|██████▎   | 335/535 [06:35<03:54,  1.17s/it]Validating lr=0.0005, train epoch 0.:  63%|██████▎   | 336/535 [06:36<03:54,  1.18s/it]Validating lr=0.0005, train epoch 0.:  63%|██████▎   | 337/535 [06:37<03:52,  1.18s/it]Validating lr=0.0005, train epoch 0.:  63%|██████▎   | 338/535 [06:38<03:50,  1.17s/it]Validating lr=0.0005, train epoch 0.:  63%|██████▎   | 339/535 [06:40<03:51,  1.18s/it]Validating lr=0.0005, train epoch 0.:  64%|██████▎   | 340/535 [06:41<03:49,  1.18s/it]Validating lr=0.0005, train epoch 0.:  64%|██████▎   | 341/535 [06:42<03:48,  1.18s/it]Validating lr=0.0005, train epoch 0.:  64%|██████▍   | 342/535 [06:43<03:46,  1.17s/it]Validating lr=0.0005, train epoch 0.:  64%|██████▍   | 343/535 [06:44<03:46,  1.18s/it]Validating lr=0.0005, train epoch 0.:  64%|██████▍   | 344/535 [06:45<03:45,  1.18s/it]Validating lr=0.0005, train epoch 0.:  64%|██████▍   | 345/535 [06:47<03:44,  1.18s/it]Validating lr=0.0005, train epoch 0.:  65%|██████▍   | 346/535 [06:48<03:43,  1.19s/it]Validating lr=0.0005, train epoch 0.:  65%|██████▍   | 347/535 [06:49<03:42,  1.18s/it]Validating lr=0.0005, train epoch 0.:  65%|██████▌   | 348/535 [06:50<03:41,  1.18s/it]Validating lr=0.0005, train epoch 0.:  65%|██████▌   | 349/535 [06:51<03:40,  1.18s/it]Validating lr=0.0005, train epoch 0.:  65%|██████▌   | 350/535 [06:53<03:39,  1.19s/it]Validating lr=0.0005, train epoch 0.:  66%|██████▌   | 351/535 [06:54<03:38,  1.19s/it]Validating lr=0.0005, train epoch 0.:  66%|██████▌   | 352/535 [06:55<03:37,  1.19s/it]Validating lr=0.0005, train epoch 0.:  66%|██████▌   | 353/535 [06:56<03:35,  1.18s/it]Validating lr=0.0005, train epoch 0.:  66%|██████▌   | 354/535 [06:57<03:34,  1.19s/it]Validating lr=0.0005, train epoch 0.:  66%|██████▋   | 355/535 [06:58<03:32,  1.18s/it]Validating lr=0.0005, train epoch 0.:  67%|██████▋   | 356/535 [07:00<03:31,  1.18s/it]Validating lr=0.0005, train epoch 0.:  67%|██████▋   | 357/535 [07:01<03:30,  1.18s/it]Validating lr=0.0005, train epoch 0.:  67%|██████▋   | 358/535 [07:02<03:29,  1.18s/it]Validating lr=0.0005, train epoch 0.:  67%|██████▋   | 359/535 [07:03<03:28,  1.18s/it]Validating lr=0.0005, train epoch 0.:  67%|██████▋   | 360/535 [07:04<03:26,  1.18s/it]Validating lr=0.0005, train epoch 0.:  67%|██████▋   | 361/535 [07:06<03:25,  1.18s/it]Validating lr=0.0005, train epoch 0.:  68%|██████▊   | 362/535 [07:07<03:24,  1.18s/it]Validating lr=0.0005, train epoch 0.:  68%|██████▊   | 363/535 [07:08<03:22,  1.18s/it]Validating lr=0.0005, train epoch 0.:  68%|██████▊   | 364/535 [07:09<03:20,  1.17s/it]Validating lr=0.0005, train epoch 0.:  68%|██████▊   | 365/535 [07:10<03:20,  1.18s/it]Validating lr=0.0005, train epoch 0.:  68%|██████▊   | 366/535 [07:11<03:19,  1.18s/it]Validating lr=0.0005, train epoch 0.:  69%|██████▊   | 367/535 [07:13<03:17,  1.18s/it]Validating lr=0.0005, train epoch 0.:  69%|██████▉   | 368/535 [07:14<03:16,  1.18s/it]Validating lr=0.0005, train epoch 0.:  69%|██████▉   | 369/535 [07:15<03:14,  1.17s/it]Validating lr=0.0005, train epoch 0.:  69%|██████▉   | 370/535 [07:16<03:14,  1.18s/it]Validating lr=0.0005, train epoch 0.:  69%|██████▉   | 371/535 [07:17<03:13,  1.18s/it]Validating lr=0.0005, train epoch 0.:  70%|██████▉   | 372/535 [07:18<03:13,  1.19s/it]Validating lr=0.0005, train epoch 0.:  70%|██████▉   | 373/535 [07:20<03:12,  1.19s/it]Validating lr=0.0005, train epoch 0.:  70%|██████▉   | 374/535 [07:21<03:11,  1.19s/it]Validating lr=0.0005, train epoch 0.:  70%|███████   | 375/535 [07:22<03:10,  1.19s/it]Validating lr=0.0005, train epoch 0.:  70%|███████   | 376/535 [07:23<03:08,  1.18s/it]Validating lr=0.0005, train epoch 0.:  70%|███████   | 377/535 [07:24<03:06,  1.18s/it]Validating lr=0.0005, train epoch 0.:  71%|███████   | 378/535 [07:26<03:05,  1.18s/it]Validating lr=0.0005, train epoch 0.:  71%|███████   | 379/535 [07:27<03:03,  1.18s/it]Validating lr=0.0005, train epoch 0.:  71%|███████   | 380/535 [07:28<03:03,  1.18s/it]Validating lr=0.0005, train epoch 0.:  71%|███████   | 381/535 [07:29<03:00,  1.17s/it]Validating lr=0.0005, train epoch 0.:  71%|███████▏  | 382/535 [07:30<03:00,  1.18s/it]Validating lr=0.0005, train epoch 0.:  72%|███████▏  | 383/535 [07:31<02:58,  1.18s/it]Validating lr=0.0005, train epoch 0.:  72%|███████▏  | 384/535 [07:33<02:57,  1.18s/it]Validating lr=0.0005, train epoch 0.:  72%|███████▏  | 385/535 [07:34<02:57,  1.18s/it]Validating lr=0.0005, train epoch 0.:  72%|███████▏  | 386/535 [07:35<02:56,  1.18s/it]Validating lr=0.0005, train epoch 0.:  72%|███████▏  | 387/535 [07:36<02:55,  1.19s/it]Validating lr=0.0005, train epoch 0.:  73%|███████▎  | 388/535 [07:37<02:54,  1.19s/it]Validating lr=0.0005, train epoch 0.:  73%|███████▎  | 389/535 [07:39<02:53,  1.19s/it]Validating lr=0.0005, train epoch 0.:  73%|███████▎  | 390/535 [07:40<02:51,  1.18s/it]Validating lr=0.0005, train epoch 0.:  73%|███████▎  | 391/535 [07:41<02:49,  1.18s/it]Validating lr=0.0005, train epoch 0.:  73%|███████▎  | 392/535 [07:42<02:47,  1.17s/it]Validating lr=0.0005, train epoch 0.:  73%|███████▎  | 393/535 [07:43<02:47,  1.18s/it]Validating lr=0.0005, train epoch 0.:  74%|███████▎  | 394/535 [07:44<02:46,  1.18s/it]Validating lr=0.0005, train epoch 0.:  74%|███████▍  | 395/535 [07:46<02:45,  1.19s/it]Validating lr=0.0005, train epoch 0.:  74%|███████▍  | 396/535 [07:47<02:45,  1.19s/it]Validating lr=0.0005, train epoch 0.:  74%|███████▍  | 397/535 [07:48<02:43,  1.19s/it]Validating lr=0.0005, train epoch 0.:  74%|███████▍  | 398/535 [07:49<02:41,  1.18s/it]Validating lr=0.0005, train epoch 0.:  75%|███████▍  | 399/535 [07:50<02:40,  1.18s/it]Validating lr=0.0005, train epoch 0.:  75%|███████▍  | 400/535 [07:52<02:39,  1.18s/it]Validating lr=0.0005, train epoch 0.:  75%|███████▍  | 401/535 [07:53<02:38,  1.19s/it]Validating lr=0.0005, train epoch 0.:  75%|███████▌  | 402/535 [07:54<02:37,  1.18s/it]Validating lr=0.0005, train epoch 0.:  75%|███████▌  | 403/535 [07:55<02:36,  1.19s/it]Validating lr=0.0005, train epoch 0.:  76%|███████▌  | 404/535 [07:56<02:34,  1.18s/it]Validating lr=0.0005, train epoch 0.:  76%|███████▌  | 405/535 [07:57<02:32,  1.17s/it]Validating lr=0.0005, train epoch 0.:  76%|███████▌  | 406/535 [07:59<02:31,  1.17s/it]Validating lr=0.0005, train epoch 0.:  76%|███████▌  | 407/535 [08:00<02:30,  1.18s/it]Validating lr=0.0005, train epoch 0.:  76%|███████▋  | 408/535 [08:01<02:29,  1.18s/it]Validating lr=0.0005, train epoch 0.:  76%|███████▋  | 409/535 [08:02<02:28,  1.18s/it]Validating lr=0.0005, train epoch 0.:  77%|███████▋  | 410/535 [08:03<02:26,  1.18s/it]Validating lr=0.0005, train epoch 0.:  77%|███████▋  | 411/535 [08:05<02:25,  1.17s/it]Validating lr=0.0005, train epoch 0.:  77%|███████▋  | 412/535 [08:06<02:23,  1.17s/it]Validating lr=0.0005, train epoch 0.:  77%|███████▋  | 413/535 [08:07<02:22,  1.17s/it]Validating lr=0.0005, train epoch 0.:  77%|███████▋  | 414/535 [08:08<02:21,  1.17s/it]Validating lr=0.0005, train epoch 0.:  78%|███████▊  | 415/535 [08:09<02:20,  1.17s/it]Validating lr=0.0005, train epoch 0.:  78%|███████▊  | 416/535 [08:10<02:19,  1.17s/it]Validating lr=0.0005, train epoch 0.:  78%|███████▊  | 417/535 [08:12<02:18,  1.18s/it]Validating lr=0.0005, train epoch 0.:  78%|███████▊  | 418/535 [08:13<02:18,  1.18s/it]Validating lr=0.0005, train epoch 0.:  78%|███████▊  | 419/535 [08:14<02:17,  1.18s/it]Validating lr=0.0005, train epoch 0.:  79%|███████▊  | 420/535 [08:15<02:15,  1.18s/it]Validating lr=0.0005, train epoch 0.:  79%|███████▊  | 421/535 [08:16<02:14,  1.18s/it]Validating lr=0.0005, train epoch 0.:  79%|███████▉  | 422/535 [08:17<02:13,  1.18s/it]Validating lr=0.0005, train epoch 0.:  79%|███████▉  | 423/535 [08:19<02:12,  1.18s/it]Validating lr=0.0005, train epoch 0.:  79%|███████▉  | 424/535 [08:20<02:10,  1.18s/it]Validating lr=0.0005, train epoch 0.:  79%|███████▉  | 425/535 [08:21<02:09,  1.18s/it]Validating lr=0.0005, train epoch 0.:  80%|███████▉  | 426/535 [08:22<02:08,  1.18s/it]Validating lr=0.0005, train epoch 0.:  80%|███████▉  | 427/535 [08:23<02:07,  1.18s/it]Validating lr=0.0005, train epoch 0.:  80%|████████  | 428/535 [08:25<02:06,  1.18s/it]Validating lr=0.0005, train epoch 0.:  80%|████████  | 429/535 [08:26<02:05,  1.18s/it]Validating lr=0.0005, train epoch 0.:  80%|████████  | 430/535 [08:27<02:04,  1.18s/it]Validating lr=0.0005, train epoch 0.:  81%|████████  | 431/535 [08:28<02:02,  1.18s/it]Validating lr=0.0005, train epoch 0.:  81%|████████  | 432/535 [08:29<02:01,  1.18s/it]Validating lr=0.0005, train epoch 0.:  81%|████████  | 433/535 [08:30<02:00,  1.18s/it]Validating lr=0.0005, train epoch 0.:  81%|████████  | 434/535 [08:32<01:59,  1.18s/it]Validating lr=0.0005, train epoch 0.:  81%|████████▏ | 435/535 [08:33<01:58,  1.18s/it]Validating lr=0.0005, train epoch 0.:  81%|████████▏ | 436/535 [08:34<01:56,  1.18s/it]Validating lr=0.0005, train epoch 0.:  82%|████████▏ | 437/535 [08:35<01:55,  1.18s/it]Validating lr=0.0005, train epoch 0.:  82%|████████▏ | 438/535 [08:36<01:54,  1.18s/it]Validating lr=0.0005, train epoch 0.:  82%|████████▏ | 439/535 [08:38<01:53,  1.18s/it]Validating lr=0.0005, train epoch 0.:  82%|████████▏ | 440/535 [08:39<01:51,  1.18s/it]Validating lr=0.0005, train epoch 0.:  82%|████████▏ | 441/535 [08:40<01:50,  1.18s/it]Validating lr=0.0005, train epoch 0.:  83%|████████▎ | 442/535 [08:41<01:49,  1.18s/it]Validating lr=0.0005, train epoch 0.:  83%|████████▎ | 443/535 [08:42<01:48,  1.18s/it]Validating lr=0.0005, train epoch 0.:  83%|████████▎ | 444/535 [08:43<01:47,  1.18s/it]Validating lr=0.0005, train epoch 0.:  83%|████████▎ | 445/535 [08:45<01:46,  1.18s/it]Validating lr=0.0005, train epoch 0.:  83%|████████▎ | 446/535 [08:46<01:45,  1.18s/it]Validating lr=0.0005, train epoch 0.:  84%|████████▎ | 447/535 [08:47<01:44,  1.18s/it]Validating lr=0.0005, train epoch 0.:  84%|████████▎ | 448/535 [08:48<01:43,  1.19s/it]Validating lr=0.0005, train epoch 0.:  84%|████████▍ | 449/535 [08:49<01:42,  1.19s/it]Validating lr=0.0005, train epoch 0.:  84%|████████▍ | 450/535 [08:51<01:40,  1.19s/it]Validating lr=0.0005, train epoch 0.:  84%|████████▍ | 451/535 [08:52<01:39,  1.18s/it]Validating lr=0.0005, train epoch 0.:  84%|████████▍ | 452/535 [08:53<01:38,  1.18s/it]Validating lr=0.0005, train epoch 0.:  85%|████████▍ | 453/535 [08:54<01:37,  1.19s/it]Validating lr=0.0005, train epoch 0.:  85%|████████▍ | 454/535 [08:55<01:35,  1.18s/it]Validating lr=0.0005, train epoch 0.:  85%|████████▌ | 455/535 [08:56<01:34,  1.18s/it]Validating lr=0.0005, train epoch 0.:  85%|████████▌ | 456/535 [08:58<01:33,  1.18s/it]Validating lr=0.0005, train epoch 0.:  85%|████████▌ | 457/535 [08:59<01:31,  1.18s/it]Validating lr=0.0005, train epoch 0.:  86%|████████▌ | 458/535 [09:00<01:31,  1.18s/it]Validating lr=0.0005, train epoch 0.:  86%|████████▌ | 459/535 [09:01<01:30,  1.19s/it]Validating lr=0.0005, train epoch 0.:  86%|████████▌ | 460/535 [09:02<01:28,  1.19s/it]Validating lr=0.0005, train epoch 0.:  86%|████████▌ | 461/535 [09:04<01:27,  1.18s/it]Validating lr=0.0005, train epoch 0.:  86%|████████▋ | 462/535 [09:05<01:26,  1.18s/it]Validating lr=0.0005, train epoch 0.:  87%|████████▋ | 463/535 [09:06<01:24,  1.18s/it]Validating lr=0.0005, train epoch 0.:  87%|████████▋ | 464/535 [09:07<01:23,  1.18s/it]Validating lr=0.0005, train epoch 0.:  87%|████████▋ | 465/535 [09:08<01:22,  1.18s/it]Validating lr=0.0005, train epoch 0.:  87%|████████▋ | 466/535 [09:09<01:21,  1.18s/it]Validating lr=0.0005, train epoch 0.:  87%|████████▋ | 467/535 [09:11<01:20,  1.19s/it]Validating lr=0.0005, train epoch 0.:  87%|████████▋ | 468/535 [09:12<01:19,  1.19s/it]Validating lr=0.0005, train epoch 0.:  88%|████████▊ | 469/535 [09:13<01:18,  1.19s/it]Validating lr=0.0005, train epoch 0.:  88%|████████▊ | 470/535 [09:14<01:17,  1.19s/it]Validating lr=0.0005, train epoch 0.:  88%|████████▊ | 471/535 [09:15<01:16,  1.19s/it]Validating lr=0.0005, train epoch 0.:  88%|████████▊ | 472/535 [09:17<01:14,  1.18s/it]Validating lr=0.0005, train epoch 0.:  88%|████████▊ | 473/535 [09:18<01:13,  1.18s/it]Validating lr=0.0005, train epoch 0.:  89%|████████▊ | 474/535 [09:19<01:11,  1.18s/it]Validating lr=0.0005, train epoch 0.:  89%|████████▉ | 475/535 [09:20<01:10,  1.18s/it]Validating lr=0.0005, train epoch 0.:  89%|████████▉ | 476/535 [09:21<01:09,  1.18s/it]Validating lr=0.0005, train epoch 0.:  89%|████████▉ | 477/535 [09:22<01:08,  1.18s/it]Validating lr=0.0005, train epoch 0.:  89%|████████▉ | 478/535 [09:24<01:07,  1.18s/it]Validating lr=0.0005, train epoch 0.:  90%|████████▉ | 479/535 [09:25<01:06,  1.18s/it]Validating lr=0.0005, train epoch 0.:  90%|████████▉ | 480/535 [09:26<01:04,  1.18s/it]Validating lr=0.0005, train epoch 0.:  90%|████████▉ | 481/535 [09:27<01:03,  1.18s/it]Validating lr=0.0005, train epoch 0.:  90%|█████████ | 482/535 [09:28<01:02,  1.18s/it]Validating lr=0.0005, train epoch 0.:  90%|█████████ | 483/535 [09:30<01:01,  1.18s/it]Validating lr=0.0005, train epoch 0.:  90%|█████████ | 484/535 [09:31<01:00,  1.18s/it]Validating lr=0.0005, train epoch 0.:  91%|█████████ | 485/535 [09:32<00:59,  1.18s/it]Validating lr=0.0005, train epoch 0.:  91%|█████████ | 486/535 [09:33<00:57,  1.18s/it]Validating lr=0.0005, train epoch 0.:  91%|█████████ | 487/535 [09:34<00:56,  1.18s/it]Validating lr=0.0005, train epoch 0.:  91%|█████████ | 488/535 [09:35<00:55,  1.19s/it]Validating lr=0.0005, train epoch 0.:  91%|█████████▏| 489/535 [09:37<00:54,  1.18s/it]Validating lr=0.0005, train epoch 0.:  92%|█████████▏| 490/535 [09:38<00:53,  1.18s/it]Validating lr=0.0005, train epoch 0.:  92%|█████████▏| 491/535 [09:39<00:52,  1.18s/it]Validating lr=0.0005, train epoch 0.:  92%|█████████▏| 492/535 [09:40<00:50,  1.19s/it]Validating lr=0.0005, train epoch 0.:  92%|█████████▏| 493/535 [09:41<00:49,  1.18s/it]Validating lr=0.0005, train epoch 0.:  92%|█████████▏| 494/535 [09:43<00:48,  1.19s/it]Validating lr=0.0005, train epoch 0.:  93%|█████████▎| 495/535 [09:44<00:47,  1.19s/it]Validating lr=0.0005, train epoch 0.:  93%|█████████▎| 496/535 [09:45<00:46,  1.18s/it]Validating lr=0.0005, train epoch 0.:  93%|█████████▎| 497/535 [09:46<00:44,  1.18s/it]Validating lr=0.0005, train epoch 0.:  93%|█████████▎| 498/535 [09:47<00:43,  1.18s/it]Validating lr=0.0005, train epoch 0.:  93%|█████████▎| 499/535 [09:48<00:42,  1.18s/it]Validating lr=0.0005, train epoch 0.:  93%|█████████▎| 500/535 [09:50<00:41,  1.19s/it]Validating lr=0.0005, train epoch 0.:  94%|█████████▎| 501/535 [09:51<00:39,  1.18s/it]Validating lr=0.0005, train epoch 0.:  94%|█████████▍| 502/535 [09:52<00:38,  1.17s/it]Validating lr=0.0005, train epoch 0.:  94%|█████████▍| 503/535 [09:53<00:37,  1.17s/it]Validating lr=0.0005, train epoch 0.:  94%|█████████▍| 504/535 [09:54<00:36,  1.18s/it]Validating lr=0.0005, train epoch 0.:  94%|█████████▍| 505/535 [09:56<00:35,  1.18s/it]Validating lr=0.0005, train epoch 0.:  95%|█████████▍| 506/535 [09:57<00:34,  1.18s/it]Validating lr=0.0005, train epoch 0.:  95%|█████████▍| 507/535 [09:58<00:32,  1.17s/it]Validating lr=0.0005, train epoch 0.:  95%|█████████▍| 508/535 [09:59<00:31,  1.18s/it]Validating lr=0.0005, train epoch 0.:  95%|█████████▌| 509/535 [10:00<00:30,  1.18s/it]Validating lr=0.0005, train epoch 0.:  95%|█████████▌| 510/535 [10:01<00:29,  1.18s/it]Validating lr=0.0005, train epoch 0.:  96%|█████████▌| 511/535 [10:03<00:28,  1.18s/it]Validating lr=0.0005, train epoch 0.:  96%|█████████▌| 512/535 [10:04<00:27,  1.18s/it]Validating lr=0.0005, train epoch 0.:  96%|█████████▌| 513/535 [10:05<00:25,  1.18s/it]Validating lr=0.0005, train epoch 0.:  96%|█████████▌| 514/535 [10:06<00:24,  1.18s/it]Validating lr=0.0005, train epoch 0.:  96%|█████████▋| 515/535 [10:07<00:23,  1.18s/it]Validating lr=0.0005, train epoch 0.:  96%|█████████▋| 516/535 [10:09<00:22,  1.18s/it]Validating lr=0.0005, train epoch 0.:  97%|█████████▋| 517/535 [10:10<00:21,  1.18s/it]Validating lr=0.0005, train epoch 0.:  97%|█████████▋| 518/535 [10:11<00:19,  1.18s/it]Validating lr=0.0005, train epoch 0.:  97%|█████████▋| 519/535 [10:12<00:18,  1.17s/it]Validating lr=0.0005, train epoch 0.:  97%|█████████▋| 520/535 [10:13<00:17,  1.18s/it]Validating lr=0.0005, train epoch 0.:  97%|█████████▋| 521/535 [10:14<00:16,  1.18s/it]Validating lr=0.0005, train epoch 0.:  98%|█████████▊| 522/535 [10:16<00:15,  1.17s/it]Validating lr=0.0005, train epoch 0.:  98%|█████████▊| 523/535 [10:17<00:14,  1.17s/it]Validating lr=0.0005, train epoch 0.:  98%|█████████▊| 524/535 [10:18<00:12,  1.18s/it]Validating lr=0.0005, train epoch 0.:  98%|█████████▊| 525/535 [10:19<00:11,  1.18s/it]Validating lr=0.0005, train epoch 0.:  98%|█████████▊| 526/535 [10:20<00:10,  1.18s/it]Validating lr=0.0005, train epoch 0.:  99%|█████████▊| 527/535 [10:21<00:09,  1.18s/it]Validating lr=0.0005, train epoch 0.:  99%|█████████▊| 528/535 [10:23<00:08,  1.18s/it]Validating lr=0.0005, train epoch 0.:  99%|█████████▉| 529/535 [10:24<00:07,  1.19s/it]Validating lr=0.0005, train epoch 0.:  99%|█████████▉| 530/535 [10:25<00:05,  1.19s/it]Validating lr=0.0005, train epoch 0.:  99%|█████████▉| 531/535 [10:26<00:04,  1.19s/it]Validating lr=0.0005, train epoch 0.:  99%|█████████▉| 532/535 [10:27<00:03,  1.19s/it]Validating lr=0.0005, train epoch 0.: 100%|█████████▉| 533/535 [10:29<00:02,  1.18s/it]Validating lr=0.0005, train epoch 0.: 100%|█████████▉| 534/535 [10:30<00:01,  1.19s/it]Validating lr=0.0005, train epoch 0.: 100%|██████████| 535/535 [10:31<00:00,  1.18s/it]Validating lr=0.0005, train epoch 0.: 100%|██████████| 535/535 [10:31<00:00,  1.18s/it]
Validating lr=0.0005, train epoch 1.:   0%|          | 0/535 [00:00<?, ?it/s]Validating lr=0.0005, train epoch 1.:   0%|          | 1/535 [00:01<10:44,  1.21s/it]Validating lr=0.0005, train epoch 1.:   0%|          | 2/535 [00:02<10:37,  1.20s/it]Validating lr=0.0005, train epoch 1.:   1%|          | 3/535 [00:03<10:30,  1.19s/it]Validating lr=0.0005, train epoch 1.:   1%|          | 4/535 [00:04<10:28,  1.18s/it]Validating lr=0.0005, train epoch 1.:   1%|          | 5/535 [00:05<10:22,  1.17s/it]Validating lr=0.0005, train epoch 1.:   1%|          | 6/535 [00:07<10:23,  1.18s/it]Validating lr=0.0005, train epoch 1.:   1%|▏         | 7/535 [00:08<10:20,  1.18s/it]Validating lr=0.0005, train epoch 1.:   1%|▏         | 8/535 [00:09<10:21,  1.18s/it]Validating lr=0.0005, train epoch 1.:   2%|▏         | 9/535 [00:10<10:19,  1.18s/it]Validating lr=0.0005, train epoch 1.:   2%|▏         | 10/535 [00:11<10:19,  1.18s/it]Validating lr=0.0005, train epoch 1.:   2%|▏         | 11/535 [00:12<10:17,  1.18s/it]Validating lr=0.0005, train epoch 1.:   2%|▏         | 12/535 [00:14<10:16,  1.18s/it]Validating lr=0.0005, train epoch 1.:   2%|▏         | 13/535 [00:15<10:15,  1.18s/it]Validating lr=0.0005, train epoch 1.:   3%|▎         | 14/535 [00:16<10:15,  1.18s/it]Validating lr=0.0005, train epoch 1.:   3%|▎         | 15/535 [00:17<10:16,  1.19s/it]Validating lr=0.0005, train epoch 1.:   3%|▎         | 16/535 [00:18<10:16,  1.19s/it]Validating lr=0.0005, train epoch 1.:   3%|▎         | 17/535 [00:20<10:15,  1.19s/it]Validating lr=0.0005, train epoch 1.:   3%|▎         | 18/535 [00:21<10:14,  1.19s/it]Validating lr=0.0005, train epoch 1.:   4%|▎         | 19/535 [00:22<10:09,  1.18s/it]Validating lr=0.0005, train epoch 1.:   4%|▎         | 20/535 [00:23<10:09,  1.18s/it]Validating lr=0.0005, train epoch 1.:   4%|▍         | 21/535 [00:24<10:06,  1.18s/it]Validating lr=0.0005, train epoch 1.:   4%|▍         | 22/535 [00:25<10:04,  1.18s/it]Validating lr=0.0005, train epoch 1.:   4%|▍         | 23/535 [00:27<10:04,  1.18s/it]Validating lr=0.0005, train epoch 1.:   4%|▍         | 24/535 [00:28<10:04,  1.18s/it]Validating lr=0.0005, train epoch 1.:   5%|▍         | 25/535 [00:29<10:02,  1.18s/it]Validating lr=0.0005, train epoch 1.:   5%|▍         | 26/535 [00:30<10:00,  1.18s/it]Validating lr=0.0005, train epoch 1.:   5%|▌         | 27/535 [00:31<10:02,  1.19s/it]Validating lr=0.0005, train epoch 1.:   5%|▌         | 28/535 [00:33<09:59,  1.18s/it]Validating lr=0.0005, train epoch 1.:   5%|▌         | 29/535 [00:34<10:00,  1.19s/it]Validating lr=0.0005, train epoch 1.:   6%|▌         | 30/535 [00:35<10:00,  1.19s/it]Validating lr=0.0005, train epoch 1.:   6%|▌         | 31/535 [00:36<09:56,  1.18s/it]Validating lr=0.0005, train epoch 1.:   6%|▌         | 32/535 [00:37<09:54,  1.18s/it]Validating lr=0.0005, train epoch 1.:   6%|▌         | 33/535 [00:39<09:55,  1.19s/it]Validating lr=0.0005, train epoch 1.:   6%|▋         | 34/535 [00:40<09:54,  1.19s/it]Validating lr=0.0005, train epoch 1.:   7%|▋         | 35/535 [00:41<09:54,  1.19s/it]Validating lr=0.0005, train epoch 1.:   7%|▋         | 36/535 [00:42<09:53,  1.19s/it]Validating lr=0.0005, train epoch 1.:   7%|▋         | 37/535 [00:43<09:51,  1.19s/it]Validating lr=0.0005, train epoch 1.:   7%|▋         | 38/535 [00:44<09:47,  1.18s/it]Validating lr=0.0005, train epoch 1.:   7%|▋         | 39/535 [00:46<09:46,  1.18s/it]Validating lr=0.0005, train epoch 1.:   7%|▋         | 40/535 [00:47<09:45,  1.18s/it]Validating lr=0.0005, train epoch 1.:   8%|▊         | 41/535 [00:48<09:45,  1.18s/it]Validating lr=0.0005, train epoch 1.:   8%|▊         | 42/535 [00:49<09:42,  1.18s/it]Validating lr=0.0005, train epoch 1.:   8%|▊         | 43/535 [00:50<09:40,  1.18s/it]Validating lr=0.0005, train epoch 1.:   8%|▊         | 44/535 [00:52<09:39,  1.18s/it]Validating lr=0.0005, train epoch 1.:   8%|▊         | 45/535 [00:53<09:39,  1.18s/it]Validating lr=0.0005, train epoch 1.:   9%|▊         | 46/535 [00:54<09:37,  1.18s/it]Validating lr=0.0005, train epoch 1.:   9%|▉         | 47/535 [00:55<09:37,  1.18s/it]Validating lr=0.0005, train epoch 1.:   9%|▉         | 48/535 [00:56<09:37,  1.19s/it]Validating lr=0.0005, train epoch 1.:   9%|▉         | 49/535 [00:57<09:32,  1.18s/it]Validating lr=0.0005, train epoch 1.:   9%|▉         | 50/535 [00:59<09:31,  1.18s/it]Validating lr=0.0005, train epoch 1.:  10%|▉         | 51/535 [01:00<09:32,  1.18s/it]Validating lr=0.0005, train epoch 1.:  10%|▉         | 52/535 [01:01<09:30,  1.18s/it]Validating lr=0.0005, train epoch 1.:  10%|▉         | 53/535 [01:02<09:30,  1.18s/it]Validating lr=0.0005, train epoch 1.:  10%|█         | 54/535 [01:03<09:29,  1.18s/it]Validating lr=0.0005, train epoch 1.:  10%|█         | 55/535 [01:05<09:28,  1.18s/it]Validating lr=0.0005, train epoch 1.:  10%|█         | 56/535 [01:06<09:27,  1.19s/it]Validating lr=0.0005, train epoch 1.:  11%|█         | 57/535 [01:07<09:25,  1.18s/it]Validating lr=0.0005, train epoch 1.:  11%|█         | 58/535 [01:08<09:25,  1.19s/it]Validating lr=0.0005, train epoch 1.:  11%|█         | 59/535 [01:09<09:22,  1.18s/it]Validating lr=0.0005, train epoch 1.:  11%|█         | 60/535 [01:10<09:22,  1.18s/it]Validating lr=0.0005, train epoch 1.:  11%|█▏        | 61/535 [01:12<09:19,  1.18s/it]Validating lr=0.0005, train epoch 1.:  12%|█▏        | 62/535 [01:13<09:19,  1.18s/it]Validating lr=0.0005, train epoch 1.:  12%|█▏        | 63/535 [01:14<09:19,  1.19s/it]Validating lr=0.0005, train epoch 1.:  12%|█▏        | 64/535 [01:15<09:18,  1.18s/it]Validating lr=0.0005, train epoch 1.:  12%|█▏        | 65/535 [01:16<09:14,  1.18s/it]Validating lr=0.0005, train epoch 1.:  12%|█▏        | 66/535 [01:18<09:12,  1.18s/it]Validating lr=0.0005, train epoch 1.:  13%|█▎        | 67/535 [01:19<09:12,  1.18s/it]Validating lr=0.0005, train epoch 1.:  13%|█▎        | 68/535 [01:20<09:12,  1.18s/it]Validating lr=0.0005, train epoch 1.:  13%|█▎        | 69/535 [01:21<09:12,  1.18s/it]Validating lr=0.0005, train epoch 1.:  13%|█▎        | 70/535 [01:22<09:08,  1.18s/it]Validating lr=0.0005, train epoch 1.:  13%|█▎        | 71/535 [01:23<09:06,  1.18s/it]Validating lr=0.0005, train epoch 1.:  13%|█▎        | 72/535 [01:25<09:05,  1.18s/it]Validating lr=0.0005, train epoch 1.:  14%|█▎        | 73/535 [01:26<09:02,  1.18s/it]Validating lr=0.0005, train epoch 1.:  14%|█▍        | 74/535 [01:27<09:02,  1.18s/it]read_signalfd: got signal 15 (Terminated)
Forwarding signal 15
x3003c0s13b1n0.hsn.cm.polaris.alcf.anl.gov: rank 0 died from signal 15
x3003c0s1b1n0.hsn.cm.polaris.alcf.anl.gov: rank 4 died from signal 15
x3003c0s1b1n0.hsn.cm.polaris.alcf.anl.gov: rank 6 died from signal 15
x3003c0s1b1n0.hsn.cm.polaris.alcf.anl.gov: rank 5 died from signal 15
x3003c0s13b1n0.hsn.cm.polaris.alcf.anl.gov: rank 1 died from signal 15
x3003c0s13b1n0.hsn.cm.polaris.alcf.anl.gov: rank 2 died from signal 15
x3003c0s1b1n0.hsn.cm.polaris.alcf.anl.gov: rank 7 died from signal 15
x3003c0s13b1n0.hsn.cm.polaris.alcf.anl.gov: rank 3 died from signal 15
Application 5dfb86c4 resources: utime=2602s stime=3300s maxrss=4989048KB inblock=2636418 oublock=520 minflt=3830449 majflt=13263 nvcsw=914984 nivcsw=466584
